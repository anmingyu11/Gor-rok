## 4. EXPERIMENS

The objective in Eq. 2 is independent of any downstream task and the flexibility in exploration offered by node2vec lends the learned feature representations to a wide variety of network analysis settings discussed below.

> Eq. 2中的目标独立于任何下游任务，node2vec提供的探索灵活性将学习到的特征表示用于下面讨论的各种网络分析设置。

## 4.1 Case Study: Les Misérables network

In Section 3.1 we observed that BFS and DFS strategies represent extreme ends on the spectrum of embedding nodes based on the principles of homophily (i.e., network communities) and structural equivalence (i.e., structural roles of nodes). We now aim to empirically demonstrate this fact and show that node2vec in fact can discover embeddings that obey both principles.

We use a network where nodes correspond to characters in the novel Les Misérables [13] and edges connect coappearing characters. The network has 77 nodes and 254 edges. We set $d = 16$ and run node2vec to learn feature representation for every node in the network. The feature representations are clustered using kmeans. We then visualize the original network in two dimensions with nodes now assigned colors based on their clusters.

Figure 3 (top) shows the example when we set $p = 1$, $q = 0.5$. Notice how regions of the network (i.e., network communities) are colored using the same color. In this setting node2vec discovers clusters/communities of characters that frequently interact with each other in the major sub-plots of the novel. Since the edges between characters are based on coappearances, we can conclude this characterization closely relates with homophily.

In order to discover which nodes have the same structural roles we use the same network but set p = 1, q = 2, use node2vec to get node features and then cluster the nodes based on the obtained features. Here node2vec obtains a complementary assignment of node to clusters such that the colors correspond to structural equivalence as illustrated in Figure 3(bottom). For instance, node2vec embeds blue-colored nodes close together. These nodes represent characters that act as bridges between different sub-plots of the novel. Similarly, the yellow nodes mostly represent characters that are at the periphery and have limited interactions. One could assign alternate semantic interpretations to these clusters of nodes, but the key takeaway is that node2vec is not tied to a particular notion of equivalence. As we show through our experiments, these equivalence notions are commonly exhibited in most real-world networks and have a significant impact on the performance of the learned representations for prediction tasks.

![Figure3](/Users/helloword/Anmingyu/Gor-rok/Papers/Embedding/Node2vecScalableFeatureLearningforNetworks/Fig3.png)

**Figure 3: Complementary visualizations of Les Misérables coappearance network generated by node2vec with label colors reflecting homophily (top) and structural equivalence (bottom).**

> 在 3.1 节中，我们观察到 BFS 和 DFS 策略代表了基于同质性(即，网络社区)和结构等价(即，节点的结构角色)的原理代表了极端情况。我们现在的目标是从经验上证明这一事实，并证明 node2vec 实际上可以发现同时符合这两个原则的 embedding。
>
> 我们使用节点对应于小说《悲惨世界》[13]中的人物的网络，其中，边连接了共同出现的人物。该网络有 77 个节点和 254 条边。运行 node2vec来学习网络中每个节点的特征表示。使用kmeans对特征表示进行聚类($d=16$)。然后我们将原始网络在二维中可视化，节点现在根据它们的簇分配颜色。
>
> 图3(顶部)显示了设置 $p=1$，$q=0.5$ 时的示例。请注意网络区域(即网络社区)是如何使用相同的颜色着色的。在这个背景下，node2vec 发现了小说主要次要情节中频繁互动的人物群/社区。由于角色与角色之间的边是以共现为基础的，因此可以得出结论，这种性质与同质性密切相关。

## 4.2 Experimental setup

Our experiments evaluate the feature representations obtained through node2vec on standard supervised learning tasks: multilabel classification for nodes and link prediction for edges. For both tasks, we evaluate the performance of node2vec against the following feature learning algorithms：

- Spectral clustering [29]: This is a matrix factorization approach in which we take the top $d$ eigenvectors of the normalized Laplacian matrix of graph $G$ as the feature vector representations for nodes.
- DeepWalk [24]: This approach learns $d$-dimensional feature representations by simulating uniform random walks. The sampling strategy in DeepWalk can be seen as a special case of node2vec with $p = 1$ and $q = 1$.
- LINE [28]: This approach learns $d$-dimensional feature representations in two separate phases. In the first phase, it learns $d/2$ dimensions by BFS-style simulations over immediate neighbors of nodes. In the second phase, it learns the next $d/2$ dimensions by sampling nodes strictly at a 2-hop distance from the source nodes.

We exclude other matrix factorization approaches which have already been shown to be inferior to DeepWalk [24]. We also exclude a recent approach, GraRep [6], that generalizes LINE to incorporate information from network neighborhoods beyond 2-hops, but is unable to efficiently scale to large networks.

> 我们的实验评估了通过 node2vec 获得的特征表示在标准监督学习任务中的表现：节点的多标签分类和边的链接预测。对于这两个任务，我们针对以下特征学习算法评估了 node2vec 的性能：
>
> - Spectral clustering [29]：这是一种矩阵分解方法，我们将图 $G$ 的归一化拉普拉斯矩阵的 top $d$ 特征向量作为节点的特征向量表示。
> - DeepWalk [24]: 该方法通过模拟均匀随机行走来学习 $d$ 维特征表示。DeepWalk中的采样策略可以看作是 $p=1$ 和 $q=1$ 的 node2vec 的特例。(注：DeepWalk是有向无权图)
> - LINE [28]: 该方法分两个阶段学习 $d$ 维特征表示。在第一阶段，它通过节点的直接邻域上的 BFS-style 的模拟来学习 $d/2$ 维。在第二阶段，它通过严格采样距离源节点 2-hop 的节点来学习下一个 $d/2$ 维。

We exclude other matrix factorization approaches which have already been shown to be inferior to DeepWalk [24]. We also exclude a recent approach, GraRep [6], that generalizes LINE to incorporate information from network neighborhoods beyond 2-hops, but is unable to efficiently scale to large networks.

> 我们排除了其他已被证明不如 DeepWalk[24] 的矩阵分解方法。我们还排除了最近的一种方法 GraRep[6]，该方法使 LINE 通用化以合并来自 2-hops 以外的网络邻居的信息，但无法有效地扩展到大型网络。

In contrast to the setup used in prior work for evaluating samplingbased feature learning algorithms, we generate an equal number of samples for each method and then evaluate the quality of the obtained features on the prediction task. In doing so, we discount for performance gain observed purely because of the implementation language (C/C++/Python) since it is secondary to the algorithm. Thus, in the sampling phase, the parameters for DeepWalk, LINE and node2vec are set such that they generate equal number of samples at runtime. As an example, if $\mathcal{K}$ is the overall sampling budget, then the node2vec parameters satisfy $\mathcal{K} = r \cdot l \cdot |V|$. In the optimization phase, all these benchmarks optimize using SGD with two key differences that we correct for. 

First, DeepWalk uses hierarchical sampling to approximate the softmax probabilities with an objective similar to the one use by node2vec. However, hierarchical softmax is inefficient when compared with negative sampling [22]. Hence, keeping everything else the same, we switch to negative sampling in DeepWalk which is also the de facto approximation in node2vec and LINE. Second, both node2vec and DeepWalk have a parameter for the number of context neighborhood nodes to optimize for and the greater the number, the more rounds of optimization are required. This parameter is set to unity for LINE , but since LINE completes a single epoch quicker than other approaches, we let it run for $k$ epochs.

The parameter settings used for node2vec are in line with typical values used for DeepWalk and LINE. Specifically, we set $d = 128 , r = 10 , l = 80 , k = 10$ and the optimization is run for a single epoch. We repeat our experiments for $10$ random seed initializations, and our results are statistically significant with a $p$-value of less than $0.01$.The best in-out and return hyperparameters were learned using $10$-fold cross-validation on 10% labeled data with a grid search over $p, q \in \{0.25, 0.50, 1, 2, 4\}$.

> 与先前工作中用于评估基于采样的特征学习算法的设置相比，我们为每种方法生成相等数量的样本，然后在预测任务上评估获得的特征的质量。 这样做时，我们会折衷由于实现语言（C / C ++ / Python）导致的表现提升，因为它是算法的次要部分。 因此，在采样阶段，对 DeepWalk，LINE 和node2vec 的参数进行设置，以使它们在运行时生成相等数量的样本。 例如，如果 $ \mathcal{K} $ 是整体采样预算，则 node2vec 参数满足 $ \mathcal{K} = r \cdot l \cdot | V | $ 。 在优化阶段，所有这些基准测试均使用 SGD 进行优化，但我们会修正两个主要差异。
>
> 首先，DeepWalk 使用 HS来近似 Softmax 概率，其目标类似于 node2vec 使用的目标。然而，与负采样相比，hs的效果一般[22]。因此，在保持其他一切不变的情况下，我们使用负采样，这也是 node2vec 和 LINE 中的实际上的近似。其次，node2vec 和 DeepWalk 都有一个用于优化的上下文邻域节点数量的参数，该数量越大，需要的优化次数就越多。对于 LINE，该参数被设置为 unity，但是由于 LINE 比其他方法更快地完成一个 epoch，所以我们让它运行 $k$ epochs。
>
> Node2vec使用的参数设置与 DeepWalk 和 LINE 使用的典型值一致。具体地说，我们设置 $d=128，r=10，l=80，k=10$，并且针对单个 epoch 运行优化。我们使用 $10$个随机种子初始化重复我们的实验，当 $p$-值小于 $0.01$ 时，我们的结果在统计上是显著的。使用 $10$-fold 交叉验证 10% 标记数据，并对 $p, q \in \{0.25, 0.50, 1,2,4 \}$ 进行网格搜索，学习最佳的输入-输出和返回超参数。

## 4.3 Multi-label classification

In the multi-label classification setting, every node is assigned one or more labels from a finite set $\mathcal{L}$. During the training phase, we observe a certain fraction of nodes and all their labels. The task is to predict the labels for the remaining nodes. This is a challenging task especially if $\mathcal{L}$ is large. We utilize the following datasets:

- BlogCatalog [38]: This is a network of social relationships of the bloggers listed on the BlogCatalog website. The labels represent blogger interests inferred through the metadata provided by the bloggers. The network has 10,312 nodes, 333,983 edges, and 39 different labels.
- Protein-Protein Interactions (PPI) [5]: We use a subgraph of the PPI network for Homo Sapiens. The subgraph corresponds to the graph induced by nodes for which we could obtain labels from the hallmark gene sets [19] and represent biological states. The network has 3,890 nodes, 76,584 edges, and 50 different labels.
- Wikipedia [20]: This is a cooccurrence network of words appearing in the first million bytes of the Wikipedia dump. The labels represent the Part-of-Speech (POS) tags inferred using the Stanford POS-Tagger [32]. The network has 4,777 nodes, 184,812 edges, and 40 different labels.

All these networks exhibit a fair mix of homophilic and structural equivalences. For example, we expect the social network of bloggers to exhibit strong homophily-based relationships; however, there might also be some “familiar strangers”, i.e., bloggers that do not interact but share interests and hence are structurally equivalent nodes. 

The biological states of proteins in a protein-protein interaction network also exhibit both types of equivalences. For example, they exhibit structural equivalence when proteins perform functions complementary to those of neighboring proteins, and at other times, they organize based on homophily in assisting neighboring proteins in performing similar functions.

The word cooccurence network is fairly dense, since edges exist between words cooccuring in a 2- length window in the Wikipedia corpus. Hence, words having the same POS tags are not hard to find, lending a high degree of homophily. At the same time, we expect some structural equivalence in the POS tags due to syntactic grammar patterns such as nouns following determiners, punctuations succeeding nouns etc.

> 在多标签分类设置中，为每个节点分配有限集合 $\mathcal{L}$ 中的一个或多个标签。在训练阶段，我们观察到一定比例的节点及其所有标签。任务是预测剩余节点的标签。这是一项具有挑战性的任务，尤其是在 $\mathcal{L}$ 很大的情况下。我们使用以下数据集：
>
> - BlogCatalog [38]：这是 BlogCatalog 网站上列出的博客作者的社交关系网络。 标签表示通过博客作者提供的元数据推断出的博客作者兴趣。 该网络具有10,312个节点，333,983个边和39个不同的标签。
> - Protein-Protein Interactions (PPI) [5]：我们使用 PPI 网络的子图来描述智人(Homo Sapiens)。该子图对应于从已标记的基因集[19]中获取标签并表示生物状态的节点诱导图。该网络有 3890 个节点、76584 条边和 50 个不同的标签。
> - Wikipedia [20]：这是在 Wikipedia 转储的前一百万个字节中出现的单词的共现网络。 标签代表使用斯坦福 POS-Tagger [32] 推断出的词性（POS）标签。 该网络具有 4,777 个节点，184,812 个边和 40 个不同的标签。
>
> 所有这些网络都表现出同质和结构等价物的合理混合。 例如，我们期望博客作者的社交网络表现出强大的基于同质性的关系； 但是，可能还会有一些“熟悉的陌生人”，即博客作者不互动但共享兴趣，因此在结构上是等效的节点。
>
> 蛋白质相互作用网络中蛋白质的生物状态也表现出这两种类型的等价性。例如，当蛋白质执行与邻近蛋白质互补的功能时，它们表现出结构上的等价性，而在其他时候，它们基于同质性组织起来，帮助邻近蛋白质执行相似的功能。
>
> 单词共现网络是相当密集的，因为在维基百科语料库中，在长度为二的窗口中共现的单词之间存在边。因此，不难发现具有相同 POS 标签的单词具有很高的同质性。同时，由于限定词后面的名词、名词后面的标点符号等句法语法模式的影响，我们期望 POS 标签中具有结构等价性。

#### Experimental results

The node feature representations are input to a one-vs-rest logistic regression classifier with L2 regularization. The train and test data is split equally over $10$ random instances. We use the Macro-F1 scores for comparing performance in Table 2 and the relative performance gain is over the closest benchmark. The trends are similar for Micro-F1 and accuracy and are not shown.

From the results, it is evident we can see how the added flexibility in exploring neighborhoods allows node2vec to outperform the other benchmark algorithms. In BlogCatalog, we can discover the right mix of homophily and structural equivalence by setting parameters $p$ and $q$ to low values, giving us 22.3% gain over DeepWalk and 229.2% gain over LINE in Macro-F1 scores. LINE showed worse performance than expected, which can be explained by its inability to reuse samples, a feat that can be easily done using the random walk methods.

Even in our other two networks, where we have a mix of equivalences present, the semi-supervised nature of node2vec can help us infer the appropriate degree of exploration necessary for feature learning. In the case of PPI network, the best exploration strategy ($p = 4, q = 1$) turns out to be virtually indistinguishable from DeepWalk’s uniform ($p = 1, q = 1$) exploration giving us only a slight edge over DeepWalk by avoiding redudancy in already visited nodes through a high $p$ value, but a convincing 23.8% gain over LINE in Macro-F1 scores. However, in general, the uniform random walks can be much worse than the exploration strategy learned by node2vec. As we can see in the Wikipedia word cooccurrence network, uniform walks cannot guide the search procedure towards the best samples and hence, we achieve a gain of 21.8% over DeepWalk and 33.2% over LINE.

![Table2](/Users/helloword/Anmingyu/Gor-rok/Papers/Embedding/Node2vecScalableFeatureLearningforNetworks/Table2.png)

**Table 2: Macro-F1 scores for multilabel classification on BlogCatalog, PPI (Homo sapiens) and Wikipedia word cooccurrence networks with 50% of the nodes labeled for training.**

> 将节点特征表示输入到具有 L2 正则化的 ovr 逻辑回归分类器中。 训练和测试数据通过 $ 10 $ 个随机实例平均分配。 我们使用 Macro-F1 分数来比较表2中的表现，并且相对性能增益超过了最接近的基准。 Micro-F1和准确性的趋势相似，但未在表中显示。
>
> 从测试结果可以明显看出，在探索社区时增加的灵活性使得node2vec的性能优于其他基准算法。在BlogCatalog中，通过将参数 $p$ 和 $q$ 设置为较低的值，我们可以发现同源和结构等价性的正确组合，在Macro-F1得分上比DeepWalk提高了 22.3%，比LINE提高了 229.2%。LINE的表现比预期的要差，这可以解释为它不能重用样本，而使用随机游走方法可以很容易地完成。
>
> 即使在其他两个网络中，我们存在同样的混合情况，node2vec 的半监督性质也可以帮助我们推断特征学习所需的适当探索程度。 在PPI网络的情况下，最好的探索策略( $p=4，q=1$ )实际上与 DeepWalk 的 uniform ($p=1，q=1$) 探索几乎没有区别，因为它通过较高的 $p$ 值避免了已经访问的节点中的冗余，但在 Macro-F1 分数中比 LINE 令人信服地提高了23.8％。但是，一般而言，uniform 的随机游走可能比 node2vec 学习的探索策略差得多。 正如我们在 Wikipedia 单词共现网络中看到的那样，uniform 游走无法指导搜索过程寻找最佳样本，因此，与 DeepWalk 相比，我们获得了 21.8％ 的收益，与 LINE 相比，我们获得了 33.2％ 的收益。

For a more fine-grained analysis, we also compare performance while varying the train-test split from 10% to 90%, while learning parameters $p$ and $q$ on 10% of the data as before. For brevity, we summarize the results for the Micro-F1 and Macro-F1 scores graphically in Figure 4. Here we make similar observations. All methods significantly outperform Spectral clustering, DeepWalk outperforms LINE, node2vec consistently outperforms LINE and achieves large improvement over DeepWalk across domains. For example, we achieve the biggest improvement over DeepWalk of 26.7% on BlogCatalog at 70% labeled data. In the worst case, the search phase has little bearing on learned representations in which case node2vec is equivalent to DeepWalk. Similarly, the improvements are even more striking when compared to LINE, where in addition to drastic gain (over 200%) on BlogCatalog, we observe high magnitude improvements upto 41.1% on other datasets such as PPI while training on just 10% labeled data.

![Figure4](/Users/helloword/Anmingyu/Gor-rok/Papers/Embedding/Node2vecScalableFeatureLearningforNetworks/Fig4.png)

**Figure 4: Performance evaluation of different benchmarks on varying the amount of labeled data used for training. The $x$ axis denotes the fraction of labeled data, whereas the $y$ axis in the top and bottom rows denote the Micro-F1 and Macro-F1 scores respectively. DeepWalk and node2vec give comparable performance on PPI. In all other networks, across all fractions of labeled data node2vec performs best.**

> 为了进行更细粒度的分析，我们还比较了在将训练测试分割从 10% 改为 90% 的情况下的表现，同时像之前一样在 10% 的数据上学习参数 $p$ 和 $q$ 。为了简洁起见，我们用图形化的方式总结了 Micro-F1 score 和 Macro-F1 score 的结果，见图4。这里我们做了类似的观察。所有方法的表现都明显优于谱聚类，DeepWalk 的表现优于 LINE，node2vec 的表现一直优于LINE，并且比 DeepWalk 跨领域域的表现有了很大的提高。例如，我们在 70% 标记数据的 BlogCatalog上实现了比 DeepWalk 更佳的 26.7% 的最大改进。在最坏的情况下，搜索阶段对学习的表示几乎没有影响，在这种情况下，node2vec 近似相当于 DeepWalk。
>
> 同样，与 LINE 相比，这一改进更加显著，除了在 BlogCatalog 上的显著提高(超过200%)，我们观察到在仅训练 10% 标记数据的情况下，在其他数据集(如PPI)上的显著提高达到了41.1%。

## 4.4 Parameter sensitivity

The node2vec algorithm involves a number of parameters and in Figure 5a, we examine how the different choices of parameters affect the performance of node2vec on the BlogCatalog dataset using a 50-50 split between labeled and unlabeled data. Except for the parameter being tested, all other parameters assume default values. The default values for $p$ and $q$ are set to unity.

We measure the Macro-F1 score as a function of parameters $p$ and $q$. The performance of node2vec improves as the in-out parameter $p$ and the return parameter $q$ decrease. This increase in performance can be based on the homophilic and structural equivalences we expect to see in BlogCatalog. While a low $q$ encourages outward exploration, it is balanced by a low $p$ which ensures that the walk does not go too far from the start node.

We also examine how the number of features $d$ and the node’s neighborhood parameters (number of walks $r$ , walk length $l$ , and neighborhood size $k$ ) affect the performance. We observe that performance tends to saturate once the dimensions of the representations reaches around $100$. Similarly, we observe that increasing the number and length of walks per source improves performance, which is not surprising since we have a greater overall sampling budget $K$ to learn representations. Both these parameters have a relatively high impact on the performance of the method. Interestingly, the context size, $k$ also improves performance at the cost of increased optimization time. However, the performance differences are not that large in this case.

![Figure5](/Users/helloword/Anmingyu/Gor-rok/Papers/Embedding/Node2vecScalableFeatureLearningforNetworks/Fig5.png)

**Figure 5: (a). Parameter sensitivity (b). Perturbation analysis for multilabel classification on the BlogCatalog network.**

> Node2vec 算法涉及多个参数，在图5a中，我们使用已标记数据和未标记数据各占一半的比例，检查参数的不同选择如何影响BlogCatalog数据集上的node2vec 的表现。除要测试的参数外，所有其他参数均采用默认值。 $p$ 和 $q$ 的默认值设置为 $1$。
>
> 我们根据参数 $ p $ 和 $ q $ 测量 Macro-F1 得分。 随着 in-out 参数 $ p $ 和返回参数$ q $的减小，node2vec 的表现将变好。 表现的变好可以基于我们希望在 BlogCatalog 中看到的 同质性 和 结构等价性。 $ q $ 较小会鼓励向外探索，但 $ p $ 较小则可以确保游走距离起始节点不会太远。
>
> 我们还研究了特征数 $d$ 和节点的邻域参数(游走数 $r$、游走长度 $l$ 和邻域大小 $k$)对表现的影响。我们观察到，一旦表示的维度达到 $100$ 左右，表示就趋于饱和。同样，我们观察到增加每个源节点的游走次数和长度可以提高表现效果，这并不令人惊讶，因为我们有更大的总体采样 $K$ 来学习表示。这两个参数对方法的表现都有相对较高的影响。有趣的是，上下文大小 $k$ 也提高了表现，但代价是增加了优化时间。然而，在这种情况下，表现的差异并不是很大。

## 4.5 Perturbation Analysis

For many real-world networks, we do not have access to accurate information about the network structure. We performed a perturbation study where we analyzed the performance of node2vec for two imperfect information scenarios related to the edge structure in the BlogCatalog network. In the first scenario, we measure performace as a function of the fraction of missing edges (relative to the full network). The missing edges are chosen randomly, subject to the constraint that the number of connected components in the network remains fixed. As we can see in Figure 5b(top), the decrease in Macro-F1 score as the fraction of missing edges increases is roughly linear with a small slope. Robustness to missing edges in the network is especially important in cases where the graphs are evolving over time (e.g., citation networks), or where network construction is expensive (e.g., biological networks).

In the second perturbation setting, we have noisy edges between randomly selected pairs of nodes in the network. As shown in Figure 5b(bottom), the performance of node2vec declines slightly faster initially when compared with the setting of missing edges, however, the rate of decrease in Macro-F1 score gradually slows down over time. Again, the robustness of node2vec to false edges is useful in several situations such as sensor networks where the measurements used for constructing the network are noisy.

> 对于许多现实世界的网络，我们无法获得有关网络结构的准确信息。我们进行了一个扰动研究，分析了两种与 BlogCatalog 网络中的边结构相关的不完全信息场景下 node2vec 的性能。
>
> 在第一个场景中，我们根据丢失边的比例(相对于整个网络)来衡量性能。缺失的边是随机选择的，受网络中连接组件的数量保持固定的约束。如图5b(上图)所示，Macro-F1分数随缺失边比例的增加而减小，大致呈线性关系，斜率较小。在网络随时间推移而演化的情况下(例如，引用网络)，或者在网络构建代价高昂的情况下(例如，生物网络)，对网络中缺失边的鲁棒性尤其重要。
>
> 在第二个扰动设置中，我们在网络中随机选择的节点对之间有噪声边。如图5b(底部)所示，与缺失边设置相比，node2vec 的性能在一开始下降的速度略快，但随着时间的推移，Macro-f1 分数的下降速度逐渐放缓。同样，node2vec 对假边的鲁棒性在几种情况下很有用，例如传感器网络，其中用于构建网络的测量值比较嘈杂。

## 4.6 Scalability

To test for scalability, we learn node representations using node2vec with default parameter values for Erdos-Renyi graphs with increasing sizes from $100$ to $1,000,000$ nodes and constant average degree of $10$. In Figure 6, we empirically observe that node2vec scales linearly with increase in number of nodes generating representations for one million nodes in less than four hours. The sampling procedure comprises of preprocessing for computing transition probabilities for our walk (negligibly small) and simulation of random walks. The optimization phase is made efficient using negative sampling [22] and asynchronous SGD [26].

Many ideas from prior work serve as useful pointers in making the sampling procedure computationally efficient. We showed how random walks, also used in DeepWalk [24], allow the sampled nodes to be reused as neighborhoods for different source nodes appearing in the walk. Alias sampling allows our walks to generalize to weighted networks, with little preprocessing [28]. Though we are free to set the search parameters based on the underlying task and domain at no additional cost, learning the best settings of our search parameters adds an overhead. However, as our experiments , this overhead is minimal since node2vec is semisupervised and hence, can learn these parameters efficiently with very little labeled data.

![Figure6](/Users/helloword/Anmingyu/Gor-rok/Papers/Embedding/Node2vecScalableFeatureLearningforNetworks/Fig6.png)

**Figure 6: Scalability of node2vec on Erdos-Renyi graphs with an average degree of 10.**

> 为了测试可扩展，我们使用默认参数值的 node2vec 来学习 Erdos-Renyi 图的节点表示，其大小从 $100$ 增加到 $1,000,000$ 个节点，平均度为 $10$。在图6中，我们根据经验观察到，在不到 4个小时内，node2vec的训练时间与节点个数成线性关系。采样过程包括计算我们预处理游走(小到可以忽略)的转移概率和随机游走的模拟。利用负采样[22]和异步SGD[26]使优化阶段变得高效。
>
> 先前工作中的许多想法可作为指导使采样过程在计算上更高效。我们展示了 DeepWalk[24] 中也使用的随机游走如何允许采样节点被重用为游走中出现的不同源节点的邻域。Alias sampling允许我们将游走推广到加权网络，只需很少的预处理[28]。虽然我们可以免费根据下游任务和域设置搜索参数，但学习最佳的搜索参数会增加开销。然而，作为我们的实验，这个开销是最小的，因为 node2vec 是半监督的，因此可以用很少的标签数据有效地学习这些参数。

## 4.7 Link prediction

In link prediction, we are given a network with a certain fraction of edges removed, and we would like to predict these missing edges. We generate the labeled dataset of edges as follows: To obtain positive examples, we remove 50% of edges chosen randomly from the network while ensuring that the residual network obtained after the edge removals is connected, and to generate negative examples, we randomly sample an equal number of node pairs from the network which have no edge connecting them.

Since none of feature learning algorithms have been previously used for link prediction, we additionally evaluate node2vec against some popular heuristic scores that achieve good performance in link prediction. The scores we consider are defined in terms of the neighborhood sets of the nodes constituting the pair (see Table 3). We test our benchmarks on the following datasets: 

- Facebook [14]: In the Facebook network, nodes represent users, and edges represent a friendship relation between any two users. The network has 4,039 nodes and 88,234 edges. 
- Protein-Protein Interactions (PPI) [5]: In the PPI network for Homo Sapiens, nodes represent proteins, and an edge indicates a biological interaction between a pair of proteins. The network has 19,706 nodes and 390,633 edges.
- arXiv ASTRO-PH [14]: This is a collaboration network generated from papers submitted to the e-print arXiv where nodes represent scientists, and an edge is present between two scientists if they have collaborated in a paper. The network has 18,722 nodes and 198,110 edges.

![Table3](/Users/helloword/Anmingyu/Gor-rok/Papers/Embedding/Node2vecScalableFeatureLearningforNetworks/Table3.png)

> 在链接预测中，我们有一个网络，其中删除了一定比例的边，我们希望预测这些缺失的边。我们生成带标签的边数据集如下：为了获得正例，我们从网络中随机选取 50% 的边，同时确保边移除后得到的剩余网络是连通的；为了生成反例，我们从网络中随机采样等量的没有边连接的节点对。
>
> 由于以前没有任何特征学习算法被用于链接预测，我们额外地根据一些流行的启发式得分来评估 node2vec，这些得分在链接预测中取得了良好的性能。我们考虑的分数是根据构成节点对的节点的邻域集定义的(见表3)。我们在以下数据集上测试我们的基准:
>
> - Facebook [14]:节点代表用户，边代表任意两个用户之间的朋友关系。该网络有 4039 个节点和 88234 条边。
> - Protein-Protein Interactions (PPI) [5]: 节点代表蛋白质，边表示一对蛋白质之间的生物相互作用。该网络有 19,706 个节点和 390,633 条边。
> - arXiv ASTRO-PH [14]：这是一个由提交给 e-print arXiv 的论文生成的合作网络，其中节点代表科学家，如果两位科学家在一篇论文中进行了合作，则他们之间存在一条边。网络有 18,722 个节点和 198,110 条边。

#### Experimental results.

We summarize our results for link prediction in Table 4. The best $p$ and $q$ parameter settings for each node2vec entry are omitted for ease of presentation. A general observation we can draw from the results is that the learned feature representations for node pairs significantly outperform the heuristic benchmark scores with node2vec achieving the best AUC improvement on 12.6% on the arXiv dataset over the best performing baseline (Adamic-Adar [1]).

Amongst the feature learning algorithms, node2vec outperforms both DeepWalk and LINE in all networks with gain up to 3.8% and 6.5% respectively in the AUC scores for the best possible choices of the binary operator for each algorithm. When we look at operators individually (Table 1), node2vec outperforms DeepWalk and LINE barring a couple of cases involving the Weighted-L1 and Weighted-L2 operators in which LINE performs better. Overall, the Hadamard operator when used with node2vec is highly stable and gives the best performance on average across all networks.

![Table4](/Users/helloword/Anmingyu/Gor-rok/Papers/Embedding/Node2vecScalableFeatureLearningforNetworks/Table4.png)

**Table 4: Area Under Curve (AUC) scores for link prediction. Comparison with popular baselines and embedding based methods bootstapped using binary operators: (a) Average, (b) Hadamard, (c) Weighted-L1, and (d) Weighted-L2 (See Table 1 for definitions).**

> 我们将链接预测的结果总结在表4中。为了便于表示，省略了每个 node2vec 的最佳 $p$ 和 $q$ 参数设置。我们可以从结果中得出的一个普遍的观察结果是，学习到的节点对的特征表示显著优于启发式基准测试分数，node2vec 在 arXiv 数据集上比最佳性能基线(Adamic-Adar[1])的最佳 AUC 提高 12.6%。
>
> 在特征学习算法中，node2vec 在所有网络上的表现都优于 DeepWalk 和 LINE，在为每种算法选择最佳二元运算的AUC得分上分别提高了 3.8% 和 6.5%。当我们单独查看运算(表1)时，node2vec 的表现优于 DeepWalk 和 LINE，除了几种涉及 weighted-L1 和 weighted-L2 运算符的情况外，在这两种情况下，LINE 的性能更好。总体而言，Hadamard 运算符与 node2vec 配合使用时高度稳定，在所有的网络上的平均性能最佳。

## 5.DISCUSSION AND CONCLUSION

In this paper, we studied feature learning in networks as a searchbased optimization problem. This perspective gives us multiple advantages. It can explain classic search strategies on the basis of the exploration-exploitation trade-off. Additionally, it provides a degree of interpretability to the learned representations when applied for a prediction task. For instance, we observed that BFS can explore only limited neighborhoods. This makes BFS suitable for characterizing structural equivalences in network that rely on the immediate local structure of nodes. On the other hand, DFS can freely explore network neighborhoods which is important in discovering homophilous communities at the cost of high variance. 

Both DeepWalk and LINE can be seen as rigid search strategies over networks. DeepWalk [24] proposes search using uniform random walks. The obvious limitation with such a strategy is that it gives us no control over the explored neighborhoods. LINE [28] proposes primarily a breadth-first strategy, sampling nodes and optimizing the likelihood independently over only 1-hop and 2-hop neighbors. The effect of such an exploration is easier to characterize, but it is restrictive and provides no flexibility in exploring nodes at further depths. In contrast, the search strategy in node2vec is both flexible and controllable exploring network neighborhoods through parameters $p$ and $q$. While these search parameters have intuitive interpretations, we obtain best results on complex networks when we can learn them directly from data. From a practical standpoint, node2vec is scalable and robust to perturbations.

We showed how extensions of node embeddings to link prediction outperform popular heuristic scores designed specifically for this task. Our method permits additional binary operators beyond those listed in Table 1. As a future work, we would like to explore the reasons behind the success of Hadamard operator over others, as well as establish interpretable equivalence notions for edges based on the search parameters. Future extensions of node2vec could involve networks with special structure such as heterogeneous information networks, networks with explicit domain features for nodes and edges and signed-edge networks. Continuous feature representations are the backbone of many deep learning algorithms, and it would be interesting to use node2vec representations as building blocks for end-to-end deep learning on graphs.

> 本文将网络中的特征学习作为一个基于搜索的优化问题进行研究。这个视角给了我们很多好处。它可以在 exploration-exploitation 权衡的基础上解释经典的搜索策略。此外，当应用于预测任务时，它为所学习的表示提供了一定程度的可解释性。例如，我们观察到 BFS 只能探索有限的社区。这使得 BFS 适合于描述依赖于节点直接局部结构的网络结构等价。另一方面，DFS 可以自由地探索网络邻域，这对于发现同质社区非常重要，但代价是高方差。
>
> DeepWalk 和 LINE 都可以视为网络上的严格搜索策略。 DeepWalk [24]提出使用 uniform 的随机游走进行搜索。 这种策略的明显局限性在于，它使我们无法控制所探索的社区。 LINE [28]主要提出了一种广度优先的策略，对节点进行采样并仅在 1跳 和 2跳 邻域上独立地优化似然性。 这种探索的效果更容易表征，但它是有限制的，并且在探索更深的节点时没有灵活性。 相反，node2vec中的搜索策略既灵活又可控，可以通过参数 $ p $ 和 $ q $ 探索网络邻域。 尽管这些搜索参数具有直观的解释，但是当我们可以直接从数据中学习它们时，我们可以在复杂的网络上获得最佳结果。 从实际的角度来看，node2vec 具有可扩展性并且对扰动具有鲁棒性。
>
> 我们展示了节点 embdding 到链接预测的扩展如何优于专门为这项任务设计的流行的启发式算法得分。我们的方法除了表1中列出的那些运算之外，还允许额外的二元运算符。作为未来的工作，我们将探索 Hadamard 算子比其他算子成功的原因，并建立基于搜索参数的边的可解释等价概念。 node2vec的未来扩展可能涉及具有特殊结构的网络，例如异构信息网络，节点和边有显式的领域特征的网络以及带符号的网络。连续特征表示是许多深度学习算法的支柱，使用 node2vec 表示作为图上的端到端深度学习的构建基块将是很有趣的。

## 6. REFERENCES

[1] L. A. Adamic and E. Adar. Friends and neighbors on the web. Social networks, 25(3):211–230, 2003. 

[2] L. Backstrom and J. Leskovec. Supervised random walks: predicting and recommending links in social networks. In WSDM, 2011. 

[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, 2001. 

[4] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE TPAMI, 35(8):1798–1828, 2013. 

[5] B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. Bähler, V. Wood, et al. The BioGRID interaction database. Nucleic acids research, 36:D637–D640, 2008. 

[6] S. Cao, W. Lu, and Q. Xu. GraRep: Learning Graph Representations with global structural information. In CIKM, 2015. 

[7] S. Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75 – 174, 2010. 

[8] B. Gallagher and T. Eliassi-Rad. Leveraging label-independent features for classification in sparsely labeled networks: An empirical study. In Lecture Notes in Computer Science: Advances in Social Network Mining and Analysis. Springer, 2009. 

[9] Z. S. Harris. Word. Distributional Structure, 10(23):146–162, 1954. 

[10] K. Henderson, B. Gallagher, T. Eliassi-Rad, H. Tong, S. Basu, L. Akoglu, D. Koutra, C. Faloutsos, and L. Li. RolX: structural role extraction & mining in large graphs. In KDD, 2012. 

[11] K. Henderson, B. Gallagher, L. Li, L. Akoglu, T. Eliassi-Rad, H. Tong, and C. Faloutsos. It’s who you know: graph mining using recursive structural features. In KDD, 2011. 

[12] P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis. J. of the American Statistical Association, 2002. 

[13] D. E. Knuth. The Stanford GraphBase: a platform for combinatorial computing, volume 37. Addison-Wesley Reading, 1993. 

[14] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. 

[15] K. Li, J. Gao, S. Guo, N. Du, X. Li, and A. Zhang. LRBM: A restricted boltzmann machine based approach for representation learning on linked data. In ICDM, 2014. 

[16] X. Li, N. Du, H. Li, K. Li, J. Gao, and A. Zhang. A deep learning approach to link prediction in dynamic networks. In ICDM, 2014. 

[17] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In ICLR, 2016. 

[18] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. J. of the American society for information science and technology, 58(7):1019–1031, 2007.

[19] A. Liberzon, A. Subramanian, R. Pinchback, H. Thorvaldsdóttir, P. Tamayo, and J. P. Mesirov. Molecular signatures database (MSigDB) 3.0. Bioinformatics, 27(12):1739–1740, 2011. 

[20] M. Mahoney. Large text compression benchmark. www.mattmahoney.net/dc/textdata, 2011. 

[21] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In ICLR, 2013. 

[22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. 

[23] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global vectors for word representation. In EMNLP, 2014. 

[24] B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. 

[25] P. Radivojac, W. T. Clark, T. R. Oron, A. M. Schnoes, T. Wittkop, A. Sokolov, K. Graim, C. Funk, Verspoor, et al. A large-scale evaluation of computational protein function prediction. Nature methods, 10(3):221–227, 2013. 

[26] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, 2011. 

[27] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. 

[28] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. 

[29] L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. 

[30] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. 

[31] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In AAAI, 2014. 

[32] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL, 2003. 

[33] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006. 

[34] A. Vazquez, A. Flammini, A. Maritan, and A. Vespignani. Global protein function prediction from protein-protein interaction networks. Nature biotechnology, 21(6):697–700, 2003. 

[35] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction. IEEE TPAMI, 29(1):40–51, 2007. 

[36] J. Yang and J. Leskovec. Overlapping communities explain core-periphery organization of networks. Proceedings of the IEEE, 102(12):1892–1902, 2014. 

[37] S.-H. Yang, B. Long, A. Smola, N. Sadagopan, Z. Zheng, and H. Zha. Like like alike: joint friendship and interest propagation in social networks. In WWW, 2011. 

[38] R. Zafarani and H. Liu. Social computing data repository at ASU, 2009. [39] S. Zhai and Z. Zhang. Dropout training of matrix factorization and autoencoder for link prediction in sparse graphs. In SDM, 2015.