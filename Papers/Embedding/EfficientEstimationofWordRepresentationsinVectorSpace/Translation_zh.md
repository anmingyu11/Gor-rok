# Efficient Estimation of Word Representations in Vector Space

## Abstract

我们提出了两种新的模型结构来计算来自大数据的词的连续向量表示。这些表示的质量在一个单词相似度任务中进行评估，并将结果与先前基于不同类型神经网络的最先进的技术进行比较。我们观察到不但准确性的大幅提高，而且计算成本低得多，即，从16亿个单词的数据集中学习高质量的单词向量只需不到一天的时间。 此外，我们证明了这些向量在评估语法和语义的相似性的测试集上提供了 state-of-the-art 的表现。

## 1 Introduction

当前的许多NLP系统和技术都将单词视为原子单位-单词之间没有相似性的概念，因为它们在词汇表中表示为索引。这种选择有几个很好的理由-简单性，鲁棒性以及观察到用大量数据训练的简单模型优于用较少数据训练的复杂系统。一个例子是用于统计语言建模的流行 N-gram 模型-如今，可以在几乎所有可用数据（万亿个单词[3]）上训练 N-gram。

但是，简单的技术在许多任务中都有其天花板。 例如，用于自动语音识别的**相关领域内**数据量是有限的，其性能通常取决于高质量转录语音数据的大小(通常只有数百万词)。 在机器翻译中，许多语言的现有语料库仅包含数十亿个单词或更少。因此，在某些情况下，简单扩展基础技术不会有任何重大进展，我们必须专注于更高级的技术。

随着近年来机器学习技术的进步，在更大的数据集上训练更复杂的模型成为可能，它们通常比简单的模型表现更好。可能最成功的概念是使用词[10]的分布式表示(distributed representation)。例如，基于神经网络的语言模型明显优于 N-gram 模型[1,27,17]。

#### 1.1 Goals of the Paper

本文的主要目标是介绍一些可用于从拥有数十亿词，且词典大小达百万的庞大语料中学习高质量的词向量的技术。据我们所知，之前提出的模型结构都没有成功地训练过超过数亿个单词(词向量的维度在50 - 100之间)。

我们使用最近提出的技术来衡量产生的向量表示的质量，期望不仅相似的单词会趋向于彼此接近，而且单词可以有多个相似程度[20]。

早在屈折(inflectional )语言的上下文中已经观察到这一点——例如，名词可以有多个结尾，并且如果我们在原始向量空间的子空间中搜索相似的单词，就有可能找到具有相似结尾的单词[13,14]。

令人惊讶地发现，词表示的相似性超出了简单的语法规律性。 使用对词向量执行简单代数运算的 word offset 技术，例如， 
$$
vector("King") - vector("Man") + vector("Woman")
$$
产生的向量最接近 Queen[20]的词向量表示。

**在本文中，我们试图通过开发新的模型架构来最大限度地提高这些向量运算的准确性，以保持词之间的线性规律。** 

我们设计了一种新的综合测试集来测量语法和语义规则1，并且表明许多这样的规则可以被高精度地学习。此外，我们讨论词向量的维数和训练数据的数量对词向量表示的精度和训练时间的影响。

#### 1.2 Previous Work

将词表示为连续向量已经有很长的历史了[10,26,8]。 在[1]中提出了一种非常流行的用于 estimating 神经网络语言模型（NNLM）的模型架构，其中使用具有 线性投影层 和 非线性隐藏层 的 前馈神经网络 来共同学习单词向量表示和统计语言模型。这项工作已被其他许多人关注。

NNLM的另一种有趣的架构在[13，14]中提出，其中首先使用具有单个隐藏层的神经网络来学习词向量。 然后，将词向量用于训练 NNLM。 因此，即使不构建完整的 NNLM，也可以学习词向量。在这项工作中，我们直接扩展了这个模型架构，并只关注第一步，即使用一个简单的模型学习词向量。

后来的研究表明，词向量可以用来显著改善和简化许多自然语言处理应用[4,5,29]。使用不同的模型架构对词向量本身进行评估，并在各种语料库上进行训练[4,29,23,19,9]，得到的部分词向量可供未来研究和比较2使用。然而，据我们所知，除了使用 diagonal weight matrices [23] 的 log-bilinear 模型的某些版本外，这些架构在训练方面的计算成本明显高于 [13] 中提出的架构。

#### 2 Model Architectures

许多不同类型的模型来评估词的连续表示，包括众所周知的 潜在语义分析(LSA) 和潜在 狄利克雷分配(LDA)。在本文中，我们关注的是通过神经网络学习的词的分布式表示，因为之前的研究表明，在保持单词之间的线性规律方面，神经网络的表现明显优于 LSA [20,31];此外，LDA在大型数据集上的计算成本非常高。

与[18]类似，为了比较不同的模型架构，我们首先将模型的计算复杂度定义为完全训练模型需要访问的参数的数量。接下来，我们将尝试使准确性最大化，同时使计算复杂度最小化。

对于以下所有模型，训练复杂度都成正比：
$$
O = E \times T \times Q \ , \qquad (1)
$$
其中$E$为训练 epoch 数，$T$为训练集中单词数，$Q$ 为每种模型体系结构的进一步定义(译者注：这应该是每个模型训练一个词需要的参数量？)。一般情况下 $E \in [3,50]$  T 多达十亿 。所有模型都使用 随机梯度下降 和反向传播[26]进行训练。

-----------------



- 大数据训练
- 向量之间的线性规律
- 语法语义评估
- NNLM