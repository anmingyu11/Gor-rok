# Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs

**Abstract** — We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.

> 我们提出了一种基于可控层次的可导航小世界图（Hierarchical NSW，HNSW）的 knn 最近邻搜索方法。所提出的解决方案完全基于 graph，无需额外的搜索结构，这些搜索结构通常在大多数接近性图技术的粗糙搜索阶段使用。Hierarchical NSW逐步构建一个多层结构，由存储元素的嵌套子集的分层近邻图（层）组成。随机选择元素所在的最高层，其选择概率服从指数衰减分布。这样可以生成类似于先前研究的可导航小世界（NSW）结构的图形，同时使链接根据它们的特征距离尺度分离。从上层开始搜索，并利用尺度分离提高了性能，相比NSW具有对数复杂度缩放。此外，使用启发式方法选择近邻图邻居显著提高了高召回率和高度聚类数据的性能。性能评估证明，所提出的通用度量空间搜索索引能够明显优于先前的开源向量方法。算法与跳表结构的相似性使得平衡分布式实现变得简单直观。

**Index Terms** — Graph and tree search strategies, Artificial Intelligence, Information Search and Retrieval, Information Storage and Retrieval, Information Technology and Systems, Search process, Graphs and networks, Data Structures, Nearest neighbor search, Big data, Approximate search, Similarity search

## 1 INTRODUCTION

Constantly growing amount of the available information resources has led to high demand in scalable and efficient similarity search data structures. One of the generally used approaches for information search is the K-Nearest Neighbor Search (K-NNS). The K-NNS assumes you have a defined distance function between the data elements and aims at finding the K elements from the dataset which minimize the distance to a given query. Such algorithms are used in many applications, such as non-parametric machine learning algorithms, image features matching in large scale databases [1] and semantic document retrieval [2]. A naïve approach to K-NNS is to compute the distances between the query and every element in the dataset and select the elements with minimal distance. Unfortunately, the complexity of the naïve approach scales linearly with the number of stored elements making it infeasible for large-scale datasets. This has led to a high interest in development of fast and scalable K-NNS algorithms.

> 不断增长的可用信息资源量导致了对可扩展和高效的相似性搜索数据结构的高需求。信息搜索的一种常用方法是K最近邻搜索（K-NNS）。K-NNS假定您具有数据元素之间定义的距离函数，并旨在找到数据集中与给定查询的距离最小的K个元素。这样的算法被广泛应用于许多应用程序，例如非参数机器学习算法、大规模数据库中的图像特征匹配[1]和语义文档检索[2]。一个朴素的K-NNS方法是计算查询与数据集中每个元素之间的距离，并选择最小距离的元素。然而，朴素方法的复杂度随着存储元素数量的线性增长，使得在大规模数据集上变得不可行。这导致了对快速和可扩展的KNNS算法的高度兴趣。

Exact solutions for K-NNS [3-5] may offer a substantial search speedup only in case of relatively low dimensional data due to “curse of dimensionality”. To overcome this problem a concept of Approximate Nearest Neighbors Search (K-ANNS) was proposed, which relaxes the condition of the exact search by allowing a small number of errors. The quality of an inexact search (the recall) is defined as the ratio between the number of found true nearest neighbors and K. The most popular K-ANNS solutions are based on approximated versions of tree algorithms [6, 7], locality-sensitive hashing (LSH) [8, 9] and product quantization (PQ) [10-17]. Proximity graph KANNS algorithms [10, 18-26] have recently gained popularity offering a better performance on high dimensional datasets. However, the power-law scaling of the proximity graph routing causes extreme performance degradation in case of low dimensional or clustered data. 

> 精确的K最近邻搜索（K-NNS）[3-5]仅在相对低维数据的情况下能够提供显著的搜索加速，这是由于“维度灾难”的存在。为了克服这个问题，提出了近似最近邻搜索（K-ANNS）的概念，它通过允许少量错误来放宽对准确搜索的要求。不准确搜索的质量（召回率）定义为找到的真实最近邻数与K之间的比值。最流行的K-ANNS解决方案基于树算法的近似版本[6, 7]、局部敏感哈希（LSH）[8, 9]和产品量化（PQ）[10-17]。近邻图KANNS算法[10, 18-26]最近变得越来越受欢迎，因为它们在高维数据集上表现出更好的性能。然而，近邻图路由的幂律缩放导致在低维或聚类数据的情况下出现极端性能降低。



## 4 ALGORITHM DESCRIPTION

Network construction algorithm (alg. 1) is organized via consecutive insertions of the stored elements into the graph structure. For every inserted element an integer maximum layer l is randomly selected with an exponentially decaying probability distribution (normalized by the mL parameter, see line 4 in alg. 1). 

> 网络构建算法（alg. 1）通过将存储的元素连续插入图结构中来进行组织。对于每个被插入的元素，都会随机选择一个整数最大层级l，选择过程遵循指数衰减概率分布（由mL参数进行归一化，在alg. 1的第4行可以看到）。

The first phase of the insertion process starts from the top layer by greedily traversing the graph in order to find the $ef$ closest neighbors to the inserted element $q$ in the layer. After that, the algorithm continues the search from the next layer using the found closest neighbors from the previous layer as enter points, and the process repeats. Closest neighbors at each layer are found by a variant of the greedy search algorithm described in alg. 2, which is an updated version of the algorithm from [26]. To obtain the approximate ef nearest neighbors in some layer $l_с$, a dynamic list $W$ of $ef$ closest found elements (initially filled with enter points) is kept during the search. The list is updated at each step by evaluating the neighborhood of the closest previously non-evaluated element in the list until the neighborhood of every element from the list is evaluated. Compared to limiting the number of distance calculations, Hierarchical NSW stop condition has an advantage - it allows discarding candidates for evalution that are further from the query than the furthest element in the list, thus avoiding bloating of search structures. As in NSW, the list is emulated via two priority queues for better performance. The distinctions from NSW (along with some queue optimizations) are: 1) the enter point is a fixed parameter; 2) instead of changing the number of multi-searches, the quality of the search is controlled by a different parameter ef (which was set to K in NSW [26]).

> 插入过程的第一阶段从顶层开始，通过贪婪地遍历图来找到在该层中与插入元素q最接近的ef个邻居。之后，算法从下一层继续搜索，使用上一层找到的最近邻作为起始点，并且重复这个过程。在每个层级中，使用一种改进版的贪婪搜索算法（alg. 2）来寻找最接近的邻居，这是对[26]中算法的更新版本。为了获得在某一层级 $l_с$ 中的近似 $ef$ 个最近邻居，搜索过程中会保持一个动态列表 $W$，其中存放着 $ef$ 个最接近的元素（初始时填充为起始点）。列表会在每一步中根据先前未评估的最接近元素的邻域进行更新，直到对列表中的每个元素的邻域都进行了评估。与限制距离计算次数相比，分层NSW的停止条件有一个优势 - 它可以舍弃那些离查询点比列表中最远的元素更远的候选项，从而避免了搜索结构的膨胀。与NSW类似，为了提高性能，列表通过两个优先队列进行模拟。与NSW的区别（以及一些队列优化）包括：1）起始点是一个固定参数；2）不再改变多次搜索的数量，而是通过不同的参数 $ef$ 来控制搜索的质量（在NSW [26]中设为K）。

![Alg2](/Users/anmingyu/Github/Gor-rok/Papers/TDM/Efficient and robust approximate nearest neighbor search using HNSWG/Alg2.png)

![Alg3](/Users/anmingyu/Github/Gor-rok/Papers/TDM/Efficient and robust approximate nearest neighbor search using HNSWG/Alg3.png)

![Alg4](/Users/anmingyu/Github/Gor-rok/Papers/TDM/Efficient and robust approximate nearest neighbor search using HNSWG/Alg4.png)

![Alg5](/Users/anmingyu/Github/Gor-rok/Papers/TDM/Efficient and robust approximate nearest neighbor search using HNSWG/Alg5.png)

During the first phase of the search the $ef$ parameter is set to 1 (simple greedy search) to avoid introduction of additional parameters.

> 在搜索的第一阶段，将 $ef$ 参数设置为1（简单贪婪搜索），以避免引入额外的参数。

When the search reaches the layer that is equal or less than $l$, the second phase of the construction algorithm is initiated. The second phase differs in two points: 1) the $ef$ parameter is increased from 1 to $efConstruction$ in order to control the recall of the greedy search procedure; 2) the found closest neighbors on each layer are also used as candidates for the connections of the inserted element.

> 当搜索达到等于或小于$l$的层级时，构建算法的第二阶段开始。第二阶段在两个方面有所不同：1）将$ef$参数从1增加到$efConstruction$，以控制贪婪搜索过程的召回率；2）在每个层级上找到的最接近邻居也被用作插入元素的连接候选项。

Two methods for the selection of $M$ neighbors from the candidates were considered: simple connection to the closest elements (alg. 3) and the heuristic that accounts for the distances between the candidate elements to create connections in diverse directions (alg. 4), described in the Section 3. The heuristic has two additional parameters: extendCandidates (set to false by default) which extends the candidate set and useful only for extremely clustered data, and keepPrunedConnections which allows getting fixed number of connection per element. The maximum number of connections that an element can have per layer is defined by the parameter $M_{max}$ for every layer higher than zero (a special parameter $M_{max0}$ is used for the ground layer separately). If a node is already full at the moment of making of a new connection, then its extended connection list gets shrunk by the same algorithm that used for the neighbors selection (algs. 3 or 4).

> 在考虑选择$M$个邻居的候选项时，有两种可选的方法：简单连接到最近的元素（alg. 3）和启发式方法（alg. 4），该方法考虑了候选元素之间的距离以创建多样化方向上的连接，具体描述在第3节中。启发式方法有两个额外的参数：extendCandidates（默认设置为false），用于扩展候选集，仅在数据极度聚集的情况下才有用；keepPrunedConnections 允许每个元素获得固定数量的连接。对于每个大于零的层级，一个元素能够拥有的最大连接数由参数$M_{max}$（特殊参数$M_{max0}$用于底层）定义。如果节点在进行新连接时已经满了，则其扩展连接列表会按照与邻居选择相同的算法（alg. 3或alg. 4）进行收缩。 

The insertion procedure terminates when the connections of the inserted elements are established on the zero layer

> 插入过程在插入元素的连接在零层建立完成时终止。也就是说，当插入元素的连接在最底层（零层）上建立完成时，插入过程终止。这意味着插入元素已经成功地加入到网络中，并与其他元素建立了连接。

The K-ANNS search algorithm used in Hierarchical NSW is presented in alg. 5. It is roughly equivalent to the insertion algorithm for an item with layer $l=0$. The difference is that the closest neighbors found at the ground layer which are used as candidates for the connections are now returned as the search result. The quality of the search is controlled by the $ef$ parameter (corresponding to $efConstruction$ in the construction algorithm).

> 分层NSW中使用的K-ANNS搜索算法如算法5所示。它与层级 $l=0$ 的元素插入算法大致相同。不同之处在于，作为连接候选项的最接近邻居现在作为搜索结果返回。搜索的质量由参数 $ef$ 控制（对应于构建算法中的$efConstruction$）。

#### 4.1 Influence of the construction parameters

Algorithm construction parameters $m_L$ and $M_{max0}$ are responsible for maintaining the small world navigability in the constructed graphs. Setting $m_L$ to zero (this corresponds to a single layer in the graph) and $M_{max0}$ to $M$ leads to production of directed k-NN graphs with a power-law search complexity well studied before [21, 29] (assuming using the alg. 3 for neighbor selection). Setting $m_L$ to zero and $M_{max0}$ to infinity leads to production of NSW graphs with polylogarithmic complexity [25, 26]. Finally, setting $m_L$ to some non-zero value leads to emergence of controllable hierarchy graphs which allow logarithmic search complexity by introduction of layers (see the Section 3). 

> 算法的构建参数$m_L$和$M_{max0}$负责维持构建图中的小世界可导航性。将$m_L$设置为零（这对应于图中的单个层级）、将$M_{max0}$设置为$M$将导致生成具有幂律搜索复杂度的有向k-NN图，这在之前已经得到了深入研究[21, 29]（假设使用alg. 3进行邻居选择）。将$m_L$设置为零并将$M_{max0}$设置为无穷大将导致生成具有多项对数复杂度的NSW图[25, 26]。最后，将$m_L$设置为某个非零值将导致可控层级图的出现，通过引入层级允许对数级的搜索复杂度（详见第3节）。

To achieve the optimum performance advantage of the controllable hierarchy, the overlap between neighbors on different layers (i.e. percent of element neighbors that are also belong to other layers) has to be small. In order to decrease the overlap we need to decrease the $m_L$ . However, at the same time, decreasing $m_L$ leads to an increase of average hop number during a greedy search on each layer, which negatively affects the performance. This leads to existence of the optimal value for the $m_L$ parameter. 

> 为了实现可控层级的最佳性能优势，不同层级上邻居之间的重叠（即同时属于其他层级的元素邻居的百分比）应该较小。为了减少重叠，我们需要减小$m_L$的值。然而，同时减小$m_L$会导致在每个层级上进行贪婪搜索时平均跳数的增加，这对性能产生负面影响。因此，存在着$m_L$参数的最佳值。

A simple choice for the optimal $m_L$ is $1/ln(M)$, this corresponds to the skip list parameter $p=1/M$ with an average single element overlap between the layers. Simulations done on an Intel Core i7 5930K CPU show that the proposed selection of $m_L$ is a reasonable choice (see Fig. 3 for data on 10M random $d=4$ vectors). In addition, the plot demonstrates a massive speedup on low dimensional data when increasing the $m_L$ from zero and the effect of using the heuristic for selection of the graph connections. It is hard to expect the same behavior for high dimensional data since in this case the k-NN graph already has very short greedy algorithm paths [28]. Surprisingly, increasing the $m_L$ from zero leads to a measurable increase in speed on very high dimensional data (100k dense random $d=1024$ vectors, see plot in Fig. 4), and does not introduce any penalty for the Hierarchical NSW approach. For real data such as SIFT vectors [1] (which have complex mixed structure), the performance improvement by increasing the $m_L$ is higher, but less prominent at current settings compared to improvement from the heuristic (see Fig. 5 for 1-NN search performance on 5 million 128- dimensional SIFT vectors from the learning set of BIGANN [13]). 

> 一个简单选择 $m_L$ 的最佳值是 $1/\ln(M)$，这对应于跳表参数 $p=1/M$，在层级之间具有平均单个元素重叠。在Intel Core i7 5930K CPU上进行的模拟显示，建议选择的 $m_L$ 是一个合理的选择（参见图3，用于10M个随机 $d=4$ 向量的数据）。此外，该图显示了在低维数据上增加 $m_L$ 与使用启发式方法选择图连接的显著加速效果。但是很难期望在高维数据上出现相同的行为，因为在这种情况下，k-NN图已经有非常短的贪婪算法路径[28]。令人惊讶的是，在从零开始增加 $m_L$ 时，速度在非常高维数据上也有明显的增加（例如100k个密集随机 $d=1024$ 向量，请参见图4中的曲线），并且不会对Hierarchical NSW方法产生任何负面影响。对于诸如SIFT向量[1]（具有复杂的混合结构）等实际数据，通过增加 $m_L$ 来提高性能的效果更好，但与启发式方法相比，当前设置下改进的效果不太明显（请参见图5，用于BIGANN[13]学习集中的500万个128维SIFT向量的1-NN搜索性能）。

Selection of the $M_{max0}$ (the maximum number of connections that an element can have in the zero layer) also has a strong influence on the search performance, especially in case of high quality (high recall) search. Simulations show that setting $M_{max0}$ to $M$ (this corresponds to kNN graphs on each layer if the neighbors selection heuristic is not used) leads to a very strong performance penalty at high recall. Simulations also suggest that 2∙$M$ is a good choice for $M_{max0}$ : setting the parameter higher leads to performance degradation and excessive memory usage. In Fig. 6 there are presented results of search performance for the 5M SIFT learn dataset depending on the $M_{max0}$ parameter (done on an Intel Core i5 2400 CPU). The suggested value gives performance close to optimal at different recalls.

> $M_{max0}$ 的选择（零层中一个元素可以拥有的最大连接数量）对搜索性能也有很大影响，特别是在高质量（高召回率）搜索的情况下。模拟结果显示，将 $M_{max0}$ 设置为 $M$（如果不使用邻居选择启发式方法，则对应于每个层级上的kNN图）会导致高召回率时性能严重下降。模拟还表明，2∙$M$ 是 $M_{max0}$ 的一个好选择：将该参数设置得更高会导致性能下降和过多的内存使用。图6显示了基于 $M_{max0}$ 参数的5M SIFT学习数据集的搜索性能结果（在Intel Core i5 2400 CPU上进行）。建议的值在不同的召回率下获得了接近最优的性能。

In all of the considered cases, use of the heuristic for proximity graph neighbors selection (alg. 4) leads to a higher or similar search performance compared to the naïve connection to the nearest neighbors (alg. 3). The effect is the most prominent for low dimensional data, at high recall for mid-dimensional data and for the case of highly clustered data (ideologically discontinuity can be regarded as a local low dimensional feature), see the comparison in Fig. 7 (Core i5 2400 CPU). When using the closest neighbors as connections for the proximity graph, the Hierarchical NSW algorithm fails to achieve a high recall for clustered data because the search stucks at the clusters boundaries. Contrary, when the heuristic is used (together with candidates’ extension, line 3 in Alg. 4), clustering leads to even higher performance. For uniform and very high dimensional data there is a little difference between the neighbors selecting methods (see Fig. 4), pos- sibly due to the fact that in this case almost all of the nearest neighbors are selected by the heuristic. 

> 在所有考虑的情况下，使用近邻选择启发式方法（算法4）相对于简单连接到最近邻居（算法3），可以获得更高或类似的搜索性能。这种效果在低维数据、中等维度数据的高召回率以及高度聚类数据的情况下最为显著（从意识形态上，不连续可以被视为局部低维特征），请参见图7中的比较结果（Core i5 2400 CPU）。当使用最近邻作为近邻图的连接时，Hierarchical NSW算法无法在聚类数据中实现高召回率，因为搜索会停留在聚类边界上。相反，在使用启发式方法时（与候选者扩展一起，第3行在算法4中），聚类会导致更高的性能。对于均匀和非常高维的数据，邻居选择方法之间的差异很小（请参见图4），可能是因为在这种情况下，几乎所有最近邻都是由启发式方法选择的。

The only meaningful construction parameter left for the user is $M$. A reasonable range of $M$ is from 5 to 48. Simulations show that smaller $M$ generally produces better results for lower recalls and/or lower dimensional data, while bigger $M$ is better for high recall and/or high dimensional data (see Fig. 8 for illustration, Core i5 2400 CPU). The parameter also defines the memory consumption of the algorithm (which is proportional to $M$), so it should be selected with care.

> 用户可以调整的唯一有意义的构建参数是$M$。合理的$M$范围是从5到48。模拟结果显示，较小的$M$通常在较低的召回率和/或较低维度的数据上产生更好的结果，而较大的$M$在高召回率和/或高维度的数据上效果更好（请参见图8以进行说明，Core i5 2400 CPU）。该参数还定义了算法的内存消耗（与$M$成正比），因此应谨慎选择。

Selection of the efConstruction parameter is straightforward. As it was suggested in [26] it has to be large enough to produce K-ANNS recall close to unity during the construction process (0.95 is enough for the most usecases). And just like in [26], this parameter can possibly be auto-configured by using sample data.

>选择 $efConstruction$ 参数是直接的。正如在[26]中建议的那样，它必须足够大，在构建过程中产生接近于1的K-ANNS召回率（对于大多数用例来说，0.95就足够了）。并且就像在[26]中一样，可以使用示例数据自动配置此参数。

The construction process can be easily and efficiently parallelized with only few synchronization points (as demonstrated in Fig. 9) and no measurable effect on index quality. Construction speed/index quality tradeoff is controlled via the $efConstruction$ parameter. The tradeoff between the search time and the index construction time is presented in Fig. 10 for a 10M SIFT dataset and shows that a reasonable quality index can be constructed for $efConstruction=100$ on a 4X 2.4 GHz 10-core Xeon E5- 4650 v2 CPU server in just 3 minutes. Further increase of the $efConstruction$ leads to little extra performance but in exchange of significantly longer construction time. 

> 构建过程可以通过少量的同步点轻松高效地并行化（如图9所示），且对索引质量没有可观测的影响。构建速度和索引质量之间的权衡通过 $efConstruction$ 参数控制。图10展示了一个包含10M个SIFT数据集的搜索时间与索引构建时间之间的权衡，结果显示在一个4X 2.4 GHz 10核Xeon E5- 4650 v2 CPU服务器上，可以在仅3分钟内使用 $efConstruction=100$ 构建出一个合理质量的索引。进一步增加 $efConstruction$ 会带来略微更好的性能，但代价是显著更长的构建时间。

