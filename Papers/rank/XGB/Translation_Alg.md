> åç››é¡¿å¤§å­¦ï¼Œé™ˆå¤©å¥‡

# XGBoost: A Scalable Tree Boosting System

## ABSTRACT

Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.

> æå‡æ ‘æ˜¯ä¸€ç§é«˜æ•ˆã€åº”ç”¨å¹¿æ³›çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ä¸ªå¯æ‰©å±•çš„ end-to-end æå‡æ ‘ç³»ç»Ÿ XGBoostï¼Œå®ƒè¢«æ•°æ®ç§‘å­¦å®¶å¹¿æ³›ä½¿ç”¨ï¼Œä»¥åœ¨è®¸å¤šæœºå™¨å­¦ä¹ æŒ‘æˆ˜ä¸­è·å¾—æœ€å…ˆè¿›çš„ç»“æœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–æ„ŸçŸ¥ç®—æ³•ï¼Œç”¨äºç¨€ç–æ•°æ®å’Œç”¨äºè¿‘ä¼¼æ ‘å­¦ä¹ çš„åŠ æƒåˆ†ä½æ•°è‰å›¾ã€‚
>
> æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æä¾›äº†æœ‰å…³ç¼“å­˜è®¿é—®æ¨¡å¼ã€æ•°æ®å‹ç¼©å’Œåˆ†ç‰‡çš„è§è§£ï¼Œä»¥æ„å»ºå¯æ‰©å±•çš„æå‡æ ‘ç³»ç»Ÿã€‚é€šè¿‡ç»“åˆè¿™äº›è§è§£ï¼ŒXGBoost ä½¿ç”¨æ¯”ç°æœ‰ç³»ç»Ÿå°‘å¾—å¤šçš„èµ„æºæ¥æ‰©å±•æ•°åäº¿ä¸ªç¤ºä¾‹ã€‚

## 1. INTRODUCTION

Machine learning and data-driven approaches are becoming very important in many areas. Smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event detection systems help experimental physicists to find events that lead to new physics. There are two important factors that drive these successful applications: usage of effective (statistical) models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets.

Among the machine learning methods used in practice, gradient tree boosting [10] 1 is one technique that shines in many applications. Tree boosting has been shown to give state-of-the-art results on many standard classification benchmarks [16]. LambdaMART [5], a variant of tree boosting for ranking, achieves state-of-the-art result for ranking problems. Besides being used as a stand-alone predictor, it is also incorporated into real-world production pipelines for ad click through rate prediction [15]. Finally, it is the defacto choice of ensemble method and is used in challenges such as the Netflix prize [3].

In this paper, we describe XGBoost, a scalable machine learning system for tree boosting. The system is available as an open source package2 . The impact of the system has been widely recognized in a number of machine learning and data mining challenges. Take the challenges hosted by the machine learning competition site Kaggle for example. Among the 29 challenge winning solutions 3 published at Kaggleâ€™s blog during 2015, 17 solutions used XGBoost. Among these solutions, eight solely used XGBoost to train the model, while most others combined XGBoost with neural nets in ensembles. For comparison, the second most popular method, deep neural nets, was used in 11 solutions. The success of the system was also witnessed in KDDCup 2015, where XGBoost was used by every winning team in the top-10. Moreover, the winning teams reported that ensemble methods outperform a well-configured XGBoost by only a small amount [1].

> æœºå™¨å­¦ä¹ å’Œæ•°æ®é©±åŠ¨çš„æ–¹æ³•åœ¨è®¸å¤šé¢†åŸŸå˜å¾—éå¸¸é‡è¦ã€‚æ™ºèƒ½åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨é€šè¿‡å­¦ä¹ æµ·é‡åƒåœ¾æ•°æ®å’Œç”¨æˆ·åé¦ˆæ¥ä¿æŠ¤æˆ‘ä»¬çš„ç”µå­é‚®ä»¶ï¼›å¹¿å‘Šç³»ç»Ÿå­¦ä¹ å°†æ­£ç¡®çš„å¹¿å‘Šä¸æ­£ç¡®çš„ä¸Šä¸‹æ–‡ç›¸åŒ¹é…ï¼›æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿä¿æŠ¤é“¶è¡Œå…å—æ¶æ„æ”»å‡»è€…çš„æ”»å‡»ï¼›
>
> é©±åŠ¨è¿™äº›æˆåŠŸåº”ç”¨ç¨‹åºçš„ä¸¤ä¸ªé‡è¦å› ç´ æ˜¯ï¼šä½¿ç”¨æœ‰æ•ˆçš„(ç»Ÿè®¡)æ¨¡å‹æ¥æ•è·å¤æ‚çš„æ•°æ®ä¾èµ–å…³ç³»ï¼Œä»¥åŠå¯æ‰©å±•çš„å­¦ä¹ ç³»ç»Ÿä»å¤§æ•°æ®é›†ä¸­å­¦ä¹ æ¨¡å‹ã€‚
>
> åœ¨å®é™…ä½¿ç”¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ä¸­ï¼Œæ¢¯åº¦æå‡æ ‘[10]1æ˜¯ä¸€ç§å…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯çš„æŠ€æœ¯ã€‚æå‡æ ‘å·²è¢«è¯æ˜åœ¨è®¸å¤šæ ‡å‡†åˆ†ç±»åŸºå‡†ä¸Šç»™å‡ºäº†æœ€å…ˆè¿›çš„ç»“æœ[16]ã€‚
> LambdaMART[5]ï¼Œæ˜¯ç”¨äºæ’åºçš„æå‡æ ‘çš„å˜ä½“ï¼Œåœ¨æ’åºé—®é¢˜ä¸Šè·å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚é™¤äº†è¢«ç”¨ä½œç‹¬ç«‹çš„é¢„æµ‹å™¨å¤–ï¼Œå®ƒè¿˜è¢«æ•´åˆåˆ°ç°å®ä¸–ç•Œçš„ pielines ä¸­ï¼Œç”¨äºå¹¿å‘Šç‚¹å‡»ç‡é¢„æµ‹[15]ã€‚æœ€åï¼Œå®ƒæ˜¯ é›†æˆæ–¹æ³•çš„ defacto é€‰æ‹©ï¼Œå¹¶è¢«ç”¨äºæŒ‘æˆ˜å¦‚Netflixå¥–çš„æŒ‘æˆ˜[3]ã€‚
>
> æœ¬æ–‡æè¿°äº† XGBoostï¼Œä¸€ä¸ªå¯æ‰©å±•çš„æå‡æ ‘æœºå™¨å­¦ä¹ ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä»¥å¼€æ”¾æºç è½¯ä»¶åŒ…2çš„å½¢å¼æä¾›ã€‚è¯¥ç³»ç»Ÿçš„å½±å“åœ¨è®¸å¤šæœºå™¨å­¦ä¹ å’Œæ•°æ®æŒ–æ˜æŒ‘æˆ˜ä¸­å¾—åˆ°äº†å¹¿æ³›è®¤å¯ã€‚ä»¥æœºå™¨å­¦ä¹ ç«èµ›ç½‘ç«™ Kaggle ä¸¾åŠçš„æŒ‘æˆ˜ä¸ºä¾‹ã€‚åœ¨ 2015 å¹´å‘å¸ƒåœ¨ Kaggle åšå®¢ä¸Šçš„ 29 ä¸ªæŒ‘æˆ˜åˆ¶èƒœè§£å†³æ–¹æ¡ˆ 3 ä¸­ï¼Œæœ‰ 17 ä¸ªè§£å†³æ–¹æ¡ˆä½¿ç”¨äº† XGBoostã€‚åœ¨è¿™äº›è§£å†³æ–¹æ¡ˆä¸­ï¼Œæœ‰ 8 ä¸ªåªä½¿ç”¨äº† XGBoost æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œå…¶ä»–å¤§å¤šæ•°è§£å†³æ–¹æ¡ˆåˆ™å°† XGBoost ä¸ç¥ç»ç½‘ç»œç»“åˆåœ¨ä¸€èµ·è¿›è¡Œé›†æˆã€‚ä½œä¸ºæ¯”è¾ƒï¼Œç¬¬äºŒç§æµè¡Œçš„æ–¹æ³•ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå…±æœ‰ 11 ç§è§£å†³æ–¹æ¡ˆä½¿ç”¨ã€‚è¯¥ç³»ç»Ÿçš„æˆåŠŸä¹Ÿåœ¨ 2015 å¹´çš„ KDDCup æ¯”èµ›ä¸­å¾—åˆ°äº†è§è¯ï¼Œå‰ 10 åä¸­çš„æ¯ä¸€æ”¯è·èƒœé˜Ÿä¼éƒ½ä½¿ç”¨äº† XGBoostã€‚æ­¤å¤–ï¼Œè·èƒœçš„å›¢é˜ŸæŠ¥å‘Šè¯´ï¼Œé›†æˆæ–¹æ³•çš„æ€§èƒ½ä»…æ¯”é…ç½®è‰¯å¥½çš„ XGBoost å¥½ä¸€ç‚¹[1]ã€‚

These results demonstrate that our system gives state-of-the-art results on a wide range of problems. Examples of the problems in these winning solutions include: store sales prediction; high energy physics event classification; web text classification; customer behavior prediction; motion detection; ad click through rate prediction; malware classification; product categorization; hazard risk prediction; massive online course dropout rate prediction. While domain dependent data analysis and feature engineering play an important role in these solutions, the fact that XGBoost is the consensus choice of learner shows the impact and importance of our system and tree boosting.

The most important factor behind the success of XGBoost is its scalability in all scenarios. The system runs more than ten times faster than existing popular solutions on a single machine and scales to billions of examples in distributed or memory-limited settings. The scalability of XGBoost is due to several important systems and algorithmic optimizations. These innovations include: a novel tree learning algorithm is for handling sparse data; a theoretically justified weighted quantile sketch procedure enables handling instance weights in approximate tree learning. Parallel and distributed computing makes learning faster which enables quicker model exploration. More importantly, XGBoost exploits out-of-core computation and enables data scientists to process hundred millions of examples on a desktop. Finally, it is even more exciting to combine these techniques to make an end-to-end system that scales to even larger data with the least amount of cluster resources. The major contributions of this paper is listed as follows:

- We design and build a highly scalable end-to-end tree boosting system.
- We propose a theoretically justified weighted quantile sketch for efficient proposal calculation.
- We introduce a novel sparsity-aware algorithm for parallel tree learning.
- We propose an effective cache-aware block structure for out-of-core tree learning.

> è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¹¿æ³›çš„é—®é¢˜ä¸Šæä¾›äº†æœ€å…ˆè¿›çš„ç»“æœã€‚è¿™äº›è·å¥–è§£å†³æ–¹æ¡ˆä¸­çš„è§£å†³çš„é—®é¢˜åŒ…æ‹¬ï¼šå•†åº—é”€é‡é¢„æµ‹ï¼›é«˜èƒ½ç‰©ç†äº‹ä»¶åˆ†ç±»ï¼›ç½‘é¡µæ–‡æœ¬åˆ†ç±»ï¼›å®¢æˆ·è¡Œä¸ºé¢„æµ‹ï¼›è¿åŠ¨æ£€æµ‹ï¼›å¹¿å‘Šç‚¹å‡»ç‡é¢„æµ‹ï¼›æ¶æ„è½¯ä»¶åˆ†ç±»ï¼›äº§å“åˆ†ç±»ï¼›å±é™©é£é™©é¢„æµ‹ï¼›æµ·é‡åœ¨çº¿è¯¾ç¨‹è¾å­¦ç‡é¢„æµ‹ã€‚è™½ç„¶é¢†åŸŸç›¸å…³çš„æ•°æ®åˆ†æå’Œç‰¹å¾å·¥ç¨‹åœ¨è¿™äº›è§£å†³æ–¹æ¡ˆä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ï¼Œä½† XGBoost æ˜¯å­¦ä¹ è€…çš„å…±è¯†é€‰æ‹©ï¼Œè¿™ä¸€äº‹å®è¡¨æ˜äº†æˆ‘ä»¬çš„ç³»ç»Ÿå’Œæå‡æ ‘çš„å½±å“å’Œé‡è¦æ€§ã€‚
>
> XGBoost æˆåŠŸçš„æœ€é‡è¦çš„å› ç´ æ˜¯å®ƒåœ¨æ‰€æœ‰åœºæ™¯ä¸­çš„å¯æ‰©å±•ã€‚è¯¥ç³»ç»Ÿåœ¨ä¸€å°æœºå™¨ä¸Šçš„è¿è¡Œé€Ÿåº¦æ¯”ç°æœ‰çš„æµè¡Œè§£å†³æ–¹æ¡ˆå¿«åå€ä»¥ä¸Šï¼Œåœ¨åˆ†å¸ƒå¼æˆ–å†…å­˜æœ‰é™çš„è®¾ç½®ä¸‹å¯ä»¥æ‰©å±•åˆ°æ•°åäº¿ä¸ªç¤ºä¾‹ã€‚XGBoost çš„å¯æ‰©å±•æ€§å¾—ç›Šäºå‡ ä¸ªé‡è¦çš„ç³»ç»Ÿå’Œç®—æ³•ä¼˜åŒ–ã€‚è¿™äº›åˆ›æ–°åŒ…æ‹¬ï¼šæ–°çš„æ ‘å­¦ä¹ ç®—æ³•ç”¨äºå¤„ç†ç¨€ç–æ•°æ®ï¼›ç†è®ºä¸Šåˆç†çš„ weighted quantile sketch è¿‡ç¨‹ä½¿å¾—èƒ½å¤Ÿåœ¨è¿‘ä¼¼æ ‘å­¦ä¹ ä¸­å¤„ç†å®ä¾‹æƒé‡ã€‚å¹¶è¡Œå’Œåˆ†å¸ƒå¼è®¡ç®—ä½¿å­¦ä¹ é€Ÿåº¦æ›´å¿«ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¿«åœ°æ¢ç´¢æ¨¡å‹ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒXGBoost åˆ©ç”¨äº† out-of-core è®¡ç®—ï¼Œä½¿æ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿåœ¨ç»ˆç«¯ä¸Šå¤„ç†æ•°äº¿ä¸ªç¤ºä¾‹ã€‚æœ€åï¼Œæ›´ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œå°†è¿™äº›æŠ€æœ¯ç»“åˆèµ·æ¥å½¢æˆä¸€ä¸ªend-to-end ç³»ç»Ÿï¼Œå¯ä»¥ç”¨æœ€å°‘çš„é›†ç¾¤èµ„æºæ‰©å±•åˆ°æ›´å¤§çš„æ•°æ®ã€‚
>
> æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼š
>
> - æˆ‘ä»¬è®¾è®¡å¹¶æ„å»ºäº†ä¸€ä¸ªé«˜åº¦å¯æ‰©å±•çš„ç«¯åˆ°ç«¯æå‡æ ‘ç³»ç»Ÿã€‚
> - æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è®ºä¸Šåˆç†çš„ weighted quantile sketchï¼Œç”¨äºé«˜æ•ˆè®¡ç®—ã€‚
> - æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–æ€§æ„ŸçŸ¥å¹¶è¡Œæ ‘å­¦ä¹ ç®—æ³•ã€‚
> - æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ç¼“å­˜æ„ŸçŸ¥å—ç»“æ„ï¼Œç”¨äº out-of-core å­¦ä¹ ã€‚

While there are some existing works on parallel tree boosting [22, 23, 19], the directions such as out-of-core computation, cache-aware and sparsity-aware learning have not been explored. More importantly, an end-to-end system that combines all of these aspects gives a novel solution for real-world use-cases. This enables data scientists as well as researchers to build powerful variants of tree boosting algorithms [7, 8]. Besides these major contributions, we also make additional improvements in proposing a regularized learning objective, which we will include for completeness. 

The remainder of the paper is organized as follows. We will first review tree boosting and introduce a regularized objective in Sec. 2. We then describe the split finding methods in Sec. 3 as well as the system design in Sec. 4, including experimental results when relevant to provide quantitative support for each optimization we describe. Related work is discussed in Sec. 5. Detailed end-to-end evaluations are included in Sec. 6. Finally we conclude the paper in Sec. 7.

> è™½ç„¶å·²æœ‰ä¸€äº›å…³äºå¹¶è¡Œæå‡æ ‘çš„å·¥ä½œ[22ï¼Œ23ï¼Œ19]ï¼Œä½†è¿˜æ²¡æœ‰æ¢ç´¢åˆ°è¯¸å¦‚ out-of-core è®¡ç®—ã€ç¼“å­˜æ„ŸçŸ¥å’Œç¨€ç–æ„ŸçŸ¥å­¦ä¹ ç­‰æ–¹å‘ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç»“åˆäº†æ‰€æœ‰è¿™äº›æ–¹é¢çš„ end-to-end ç³»ç»Ÿä¸ºç°å®ä¸–ç•Œçš„ç”¨ä¾‹æä¾›äº†ä¸€ç§æ–°é¢–çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ä½¿å¾—æ•°æ®ç§‘å­¦å®¶ä»¥åŠç ”ç©¶äººå‘˜èƒ½å¤Ÿæ„å»ºå¼ºå¤§çš„æå‡æ ‘ç®—æ³•å˜ç§[7ï¼Œ8]ã€‚é™¤äº†è¿™äº›ä¸»è¦è´¡çŒ®ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨æå‡ºæ­£åˆ™åŒ–å­¦ä¹ ç›®æ ‡æ–¹é¢åšå‡ºäº†é¢å¤–çš„æ”¹è¿›ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒåŒ…æ‹¬è¿›æ¥ï¼Œä»¥ç¡®ä¿å®Œæ•´æ€§ã€‚è®ºæ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ã€‚W
>
> è®ºæ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ã€‚æˆ‘ä»¬å°†é¦–å…ˆå›é¡¾æ ‘çš„æå‡ï¼Œå¹¶åœ¨ç¬¬äºŒèŠ‚å¼•å…¥ä¸€ä¸ªæ­£åˆ™åŒ–ç›®æ ‡ã€‚ç„¶åï¼Œç¬¬ä¸‰èŠ‚æè¿°åˆ†è£‚æŸ¥æ‰¾æ–¹æ³•ä»¥åŠç¬¬å››èŠ‚ä¸­çš„ç³»ç»Ÿè®¾è®¡ï¼ŒåŒ…æ‹¬ç›¸å…³çš„å®éªŒç»“æœï¼Œä»¥ä¾¿ä¸ºæˆ‘ä»¬æè¿°çš„æ¯ä¸€ç§ä¼˜åŒ–æä¾›å®šé‡æ”¯æŒã€‚ç¬¬äº”èŠ‚è®¨è®ºäº†ç›¸å…³å·¥ä½œã€‚ç¬¬å…­èŠ‚åŒ…æ‹¬äº†è¯¦ç»†çš„ end-to-end è¯„ä¼°ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ç¬¬ä¸ƒèŠ‚å¯¹è®ºæ–‡è¿›è¡Œäº†æ€»ç»“ã€‚

## 2. TREE BOOSTING IN A NUTSHELLï¼ˆæœå£³ä¸­çš„æå‡æ ‘ï¼‰

We review gradient tree boosting algorithms in this section. The derivation follows from the same idea in existing literatures in gradient boosting. Specicially the second order method is originated from Friedman et al. [12]. We make minor improvements in the reguralized objective, which were found helpful in practice.

> åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†æ¢¯åº¦æå‡æ ‘ç®—æ³•ã€‚è¿™ä¸€æ¨å¯¼æºäºç°æœ‰æ–‡çŒ®ä¸­å…³äºæ¢¯åº¦æå‡çš„ç›¸åŒæ€æƒ³ã€‚å…·ä½“åœ°è¯´ï¼ŒäºŒé˜¶æ–¹æ³•èµ·æºäº Friedman ç­‰äººã€‚
> [12]ã€‚æˆ‘ä»¬åœ¨é‡æ–°å®šä½çš„ç›®æ ‡ä¸Šåšäº†ä¸€äº›å°çš„æ”¹è¿›ï¼Œè¿™åœ¨å®è·µä¸­è¢«å‘ç°æ˜¯æœ‰å¸®åŠ©çš„ã€‚

#### 2.1 Regularized Learning Objective

For a given data set with $n$ examples and $m$ features $\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)$, a tree ensemble model (shown in Fig. 1) uses $K$ additive functions to predict the output.
$$
\hat{y}_{i}=\phi\left(\mathbf{x}_{i}\right)=\sum_{k=1}^{K} f_{k}\left(\mathbf{x}_{i}\right), \quad f_{k} \in \mathcal{F}
\qquad(1)
$$
where $Â \mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q: \mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weights $w$. Unlike decision trees, each regression tree contains a continuous score on each of the leaf, we use wi to represent score on $i$-th leaf. For a given example, we will use the decision rules in the trees (given by $q$) to classify it into the leaves and calculate the final prediction by summing up the score in the corresponding leaves (given by $w$). To learn the set of functions used in the model, we minimize the following regularized objective.

Here $l$ is a differentiable convex loss function that measures the difference between the prediction $\hat{y}_i$ and the target $y_i$. The second term $\Omega$ penalizes the complexity of the model (i.e., the regression tree functions). The additional regularization term helps to smooth the final learnt weights to avoid over-fitting. Intuitively, the regularized objective will tend to select a model employing simple and predictive functions. A similar regularization technique has been used in Regularized greedy forest (RGF) [25] model. Our objective and the corresponding learning algorithm is simpler than RGF and easier to parallelize. When the regularization parameter is set to zero, the objective falls back to the traditional gradient tree boosting.

![Figure1](/Users/helloword/Anmingyu/Gor-rok/Papers/GBDT/XGB/Fig1.png)

**Figure 1: Tree Ensemble Model. The final prediction for a given example is the sum of predictions from each tree.**

> å¯¹äºå…·æœ‰ $n$ ä¸ªç¤ºä¾‹å’Œ $m$ ä¸ªç‰¹å¾ $\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)$ çš„ç»™å®šæ•°æ®é›†ï¼Œé›†æˆçš„æ ‘æ¨¡å‹(å¦‚å›¾1æ‰€ç¤º)ä½¿ç”¨ $K$ ä¸ªå‡½æ•°åŠ æ³•æ¥é¢„æµ‹è¾“å‡ºã€‚
> $$
> \hat{y}_{i}=\phi\left(\mathbf{x}_{i}\right)=\sum_{k=1}^{K} f_{k}\left(\mathbf{x}_{i}\right), \quad f_{k} \in \mathcal{F}
> \qquad(1)
> $$
> å…¶ä¸­ $\mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q: \mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)$ æ˜¯å›å½’æ ‘çš„ç©ºé—´(ä¹Ÿç§°ä¸ºCART)ã€‚è¿™é‡Œ $q$ è¡¨ç¤ºå°†ç¤ºä¾‹æ˜ å°„åˆ°ç›¸åº”å¶ç´¢å¼•çš„æ¯æ£µæ ‘çš„ç»“æ„ã€‚$T$æ˜¯æ ‘ä¸­çš„å¶æ•°ã€‚æ¯ä¸ª $f_k$ å¯¹åº”äºç‹¬ç«‹çš„æ ‘ç»“æ„ $q$ å’Œå¶æƒé‡ $w$ã€‚ä¸å†³ç­–æ ‘ä¸åŒï¼Œæ¯ä¸ªå›å½’æ ‘éƒ½åŒ…å«æ¯ä¸ªå¶çš„è¿ç»­åˆ†æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ $w_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªå¶çš„åˆ†æ•°ã€‚å¯¹äºç»™å®šçš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ ‘ä¸­çš„å†³ç­–è§„åˆ™(ç”± $q$ ç»™å‡º)æ¥å°†å…¶åˆ†ç±»ä¸ºå¶å­ï¼Œå¹¶é€šè¿‡å°†ç›¸åº”å¶å­(ç”± $w$ ç»™å‡º)ä¸­çš„å¾—åˆ†ç›¸åŠ æ¥è®¡ç®—æœ€ç»ˆé¢„æµ‹ã€‚ä¸ºäº†å­¦ä¹ æ¨¡å‹ä¸­ä½¿ç”¨çš„å‡½æ•°é›†ï¼Œæˆ‘ä»¬æœ€å°åŒ–ä»¥ä¸‹å¸¦æ­£åˆ™åŒ–ç›®æ ‡ã€‚
> $$
> \begin{array}{l}
> \mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right) \\
> \text { where } \Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}
> \end{array}
> 
> \qquad(2)
> $$
> è¿™é‡Œï¼Œ$l$ æ˜¯ä¸€ä¸ªå¯å¾®çš„å‡¸å‡½æ•°ï¼Œå®ƒåº¦é‡é¢„æµ‹ $\hat{y}_i$ å’Œç›®æ ‡ $y_i$ ä¹‹é—´çš„å·®å¼‚ã€‚ç¬¬äºŒä¸ªæœ¯è¯­ $\Omega$ å¯¹åº”äºæ¨¡å‹çš„å¤æ‚åº¦çš„æƒ©ç½š(å³å›å½’æ ‘å‡½æ•°)ã€‚é™„å¸¦çš„æ­£åˆ™åŒ–é¡¹æœ‰åŠ©äºå¹³æ»‘æœ€ç»ˆå­¦ä¹ çš„æƒé‡ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆã€‚
>
> ç›´è§‚åœ°è¯´ï¼Œæ­£åˆ™åŒ–ç›®æ ‡å°†å€¾å‘äºé€‰æ‹©ä½¿ç”¨ç®€å•å’Œæ³›åŒ–æ€§å¥½çš„å‡½æ•°çš„æ¨¡å‹ã€‚åœ¨æ­£åˆ™åŒ–è´ªå©ªæ£®æ—(RGF)[25]æ¨¡å‹ä¸­ä¹Ÿä½¿ç”¨äº†ç±»ä¼¼çš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ç›®æ ‡å’Œç›¸åº”çš„å­¦ä¹ ç®—æ³•æ¯”RGFæ›´ç®€å•ï¼Œæ›´å®¹æ˜“å¹¶è¡ŒåŒ–ã€‚å½“æ­£åˆ™åŒ–å‚æ•°è®¾ç½®ä¸ºé›¶æ—¶ï¼Œç›®æ ‡é€€åŒ–ä¸ºä¼ ç»Ÿçš„æ¢¯åº¦æå‡æ ‘ã€‚

#### 2.2 Gradient Tree Boosting

The tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let $\hat{y}^{(t)}_i$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize the following objective.
$$
\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)
$$
This means we greedily add the $f_t$ that most improves our model according to Eq. (2). Second-order approximation can be used to quickly optimize the objective in the general setting [12].
$$
\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(t-1)}\right)+g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)
$$
where $g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right)$ and $h_{i}=\partial_{\hat{y}^{(t-1)}}^{2} l\left(y_{i}, \hat{y}^{(t-1)}\right)$ are first and second order gradient statistics on the loss function. We can remove the constant terms to obtain the following simplified objective at step $t$.
$$
\tilde{\mathcal{L}}^{(t)}=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right) \qquad(3)
$$
Define $I_{j}=\left\{i \mid q\left(\mathbf{x}_{i}\right)=j\right\}$ as the instance set of leaf $j$. We can rewrite Eq (3) by expanding $\Omega$ as follows
$$
\begin{aligned}
\tilde{\mathcal{L}}^{(t)} &=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\
&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
\end{aligned}
\qquad (4)
$$
For a fixed structure $q(\mathbb{x})$, we can compute the optimal weight $w^âˆ—_j$ of leaf $j$ by
$$
w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}
\qquad (5)
$$
and calculate the corresponding optimal value by
$$
\tilde{\mathcal{L}}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T
\qquad (6)
$$
Eq (6) can be used as a scoring function to measure the quality of a tree structure $q$. This score is like the impurity score for evaluating decision trees, except that it is derived for a wider range of objective functions. Fig. 2 illustrates how this score can be calculated.

Normally it is impossible to enumerate all the possible tree structures $q$. A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used instead. Assume that $I_L$ and $I_R$ are the instance sets of left and right nodes after the split. Lettting $I = I_L âˆª I_R$, then the loss reduction after the split is given by
$$
\mathcal{L}_{\text {split }}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma
\qquad(7)
$$
This formula is usually used in practice for evaluating the split candidates.

![Figure2](/Users/helloword/Anmingyu/Gor-rok/Papers/GBDT/XGB/Fig2.png)

**Figure 2: Structure Score Calculation. We only need to sum up the gradient and second order gradient statistics on each leaf, then apply the scoring formula to get the quality score.**

> Eq.(2)ä¸­çš„é›†æˆæ ‘æ¨¡å‹çš„å‚æ•°åŒ…æ‹¬å‡½æ•°ï¼Œä¸èƒ½åœ¨æ¬§å¼ç©ºé—´ç”¨ä¼ ç»Ÿçš„æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œä»¥åŠ æ³•æ¨¡å‹çš„æ–¹å¼è®­ç»ƒæ¨¡å‹ã€‚å½¢å¼ä¸Šï¼Œå‡è®¾ $\hat{y}^{(T)}_i$ æ˜¯ç¬¬ $t$ æ¬¡è¿­ä»£çš„ç¬¬ $i$ ä¸ªå®ä¾‹çš„é¢„æµ‹ï¼Œæˆ‘ä»¬å°†éœ€è¦æ·»åŠ  $f_t$ ä»¥æœ€å°åŒ–ä»¥ä¸‹ç›®æ ‡ã€‚
> $$
> \mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)
> $$
> è¿™æ„å‘³ç€æˆ‘ä»¬æ ¹æ® Eq.(2)ä»¥è´ªå©ªæ–¹å¼æ·»åŠ æœ€èƒ½æ”¹å–„æˆ‘ä»¬æ¨¡å‹çš„ $f_t$ã€‚åœ¨ä¸€èˆ¬è®¾ç½®[12]ä¸­ï¼ŒäºŒé˜¶è¿‘ä¼¼å¯ä»¥ç”¨æ¥å¿«é€Ÿä¼˜åŒ–ç›®æ ‡ã€‚
> $$
> \mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(t-1)}\right)+g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)
> $$
> å…¶ä¸­ $g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right)$ å’Œ $h_{i}=\partial_{\hat{y}^{(t-1)}}^{2} l\left(y_{i}, \hat{y}^{(t-1)}\right)$ æ˜¯æŸå¤±çš„ä¸€é˜¶å’ŒäºŒé˜¶æ¢¯åº¦ç»Ÿè®¡é‡ã€‚æˆ‘ä»¬å¯ä»¥åœ¨æ­¥éª¤ $t$ ä¸­å»æ‰å¸¸æ•°é¡¹ä»¥è·å¾—ä»¥ä¸‹ç®€åŒ–ç›®æ ‡ã€‚
> $$
> \tilde{\mathcal{L}}^{(t)}=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right) \qquad(3)
> $$
> å®šä¹‰ $I_{j}=\left\{i \mid q\left(\mathbf{x}_{i}\right)=j\right\}$ ä½œä¸ºå¶å­èŠ‚ç‚¹ $j$ çš„å®ä¾‹é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å±•å¼€ $\Omega$ æ¥é‡å†™å…¬å¼(3)ï¼Œå¦‚ä¸‹æ‰€ç¤º
> $$
> \begin{aligned}
> \tilde{\mathcal{L}}^{(t)} &=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\
> &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
> \end{aligned}
> \qquad (4)
> $$
> å¯¹äºå›ºå®šç»“æ„ $q(\mathbb{x})$ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å¶å­èŠ‚ç‚¹ $j$ çš„æœ€ä¼˜æƒé‡ $w^âˆ—_j$
> $$
> w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}
> \qquad (5)
> $$
> å¹¶é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®—ç›¸åº”çš„æœ€ä¼˜å€¼
> $$
> \tilde{\mathcal{L}}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T
> \qquad (6)
> $$
> Eq(6) å¯ä»¥ä½œä¸ºä¸€ä¸ªè¯„åˆ†å‡½æ•°æ¥è¡¡é‡æ ‘ç»“æ„ $q$ çš„è´¨é‡ã€‚è¿™ä¸ªåˆ†æ•°ç±»ä¼¼äºç”¨äºè¯„ä¼°å†³ç­–æ ‘çš„ impurity åˆ†æ•°ï¼Œåªä¸è¿‡å®ƒæ˜¯ä¸ºæ›´å¹¿æ³›çš„ç›®æ ‡å‡½æ•°æ´¾ç”Ÿçš„ã€‚å›¾2 è¯´æ˜äº†å¦‚ä½•è®¡ç®—è¿™ä¸ªåˆ†æ•°ã€‚
> $$
> \mathcal{L}_{\text {split }}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma
> \qquad(7)
> $$
> è¿™ä¸€å…¬å¼åœ¨å®è·µä¸­é€šå¸¸ç”¨äºè¯„ä¼°åˆ†è£‚çš„å€™é€‰ã€‚

#### 2.3 Shrinkage and Column Subsampling

Besides the regularized objective mentioned in Sec. 2.1, two additional techniques are used to further prevent overfitting. 

The first technique is shrinkage introduced by Friedman [11]. Shrinkage scales newly added weights by a factor $\eta$ after each step of tree boosting. Similar to a learning rate in tochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. 

The second technique is column (feature) subsampling. This technique is used in RandomForest [4,13], It is implemented in a commercial software TreeNet 4 for gradient boosting, but is not implemented in existing opensource packages. 

According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later.

> é™¤äº† Sec.2.1 æåˆ°çš„å¸¦æœ‰æ­£åˆ™åŒ–çš„ç›®æ ‡å‡½æ•°ä¹‹å¤–ã€‚è¿˜ä½¿ç”¨äº†ä¸¤ç§å…¶ä»–çš„æŠ€æœ¯æ¥è¿›ä¸€æ­¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
>
> ç¬¬ä¸€ç§æ˜¯ friedman ä»‹ç»çš„ shrinkage [11]ã€‚
>
> åœ¨æå‡æ ‘çš„æ¯ä¸€æ­¥ï¼Œä½¿ç”¨ $\eta$ å› å­ç¼©æ”¾æ–°æ·»åŠ çš„æƒé‡ã€‚ä¸éšæœºä¼˜åŒ–ä¸­çš„å­¦ä¹ ç‡ç±»ä¼¼ï¼Œshrinkage å‡å°‘äº†æ¯æ£µæ ‘çš„å½±å“ï¼Œå¹¶ä¸ºæœªæ¥çš„æ ‘å»æ”¹è¿›æ¨¡å‹ç•™å‡ºäº†ç©ºé—´ã€‚
>
> ç¬¬äºŒç§æ˜¯åˆ—(ç‰¹å¾)å­æŠ½æ ·ã€‚
>
> è¿™é¡¹æŠ€æœ¯åœ¨ RandomForest[4ï¼Œ13]ä¸­ä½¿ç”¨ï¼Œå®ƒæ˜¯åœ¨å•†ä¸šè½¯ä»¶ TreeNet4 ä¸­å®ç°çš„ï¼Œç”¨äºæ¢¯åº¦æå‡ï¼Œä½†æ²¡æœ‰åœ¨ç°æœ‰çš„å¼€æºè½¯ä»¶åŒ…ä¸­å®ç°ã€‚
>
> æ ¹æ®ä½¿ç”¨è€…åé¦ˆï¼Œä½¿ç”¨åˆ—æŠ½æ ·æ¯”ä½¿ç”¨ä¼ ç»Ÿçš„è¡ŒæŠ½æ ·(ä¹Ÿæ”¯æŒ)æ›´èƒ½é˜²æ­¢è¿‡æ‹Ÿåˆã€‚åˆ—æŠ½æ ·çš„ä½¿ç”¨è¿˜åŠ å¿«äº†ç¨åæè¿°çš„å¹¶è¡Œç®—æ³•çš„è®¡ç®—é€Ÿåº¦ã€‚

## 3. SPLIT FINDING ALGORITHMS

#### 3.1 Basic Exact Greedy Algorithm

One of the key problems in tree learning is to find the best split as indicated by Eq (7). In order to do so, a split finding algorithm enumerates over all the possible splits on all the features. We call this the exact greedy algorithm. Most existing single machine tree boosting implementations, such as scikit-learn [20], Râ€™s gbm [21] as well as the single machine version of XGBoost support the exact greedy algorithm. The exact greedy algorithm is shown in Alg. 1. 

It is computationally demanding to enumerate all the possible splits for continuous features. In order to do so efficiently, the algorithm must first sort the data according to feature values and visit the data in sorted order to accumulate the gradient statistics for the structure score in Eq (7).

![Algorithm1](/Users/helloword/Anmingyu/Gor-rok/Papers/GBDT/XGB/Alg1.png)

> å¦‚å…¬å¼ (7) æ‰€ç¤ºï¼Œæ ‘å­¦ä¹ çš„å…³é”®é—®é¢˜ä¹‹ä¸€æ˜¯æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œåˆ†è£‚ç‚¹æŸ¥æ‰¾ç®—æ³•æšä¸¾äº†æ‰€æœ‰ç‰¹å¾ä¸Šçš„æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºexact greedy ç®—æ³•ã€‚å¤§å¤šæ•°ç°æœ‰çš„ single machine tree boosting å®ç°ï¼Œå¦‚scikit-learn [20]ã€Rçš„gbm[21]ä»¥åŠ XGBoost çš„å•æœºç‰ˆæœ¬éƒ½æ”¯æŒ exact greedy ç®—æ³•ã€‚exact greedy å¦‚ ALG.1 æ‰€ç¤ºã€‚
>
> å¯¹äºè¿ç»­çš„ç‰¹å¾ï¼Œéœ€è¦è®¡ç®—æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œç®—æ³•å¿…é¡»é¦–å…ˆæ ¹æ®ç‰¹å¾å€¼å¯¹æ•°æ®è¿›è¡Œæ’åºï¼Œç„¶åæ ¹æ®å…¬å¼ Eq (7) è®¡ç®—å‡ºå½“å‰åˆ†å‰²ç‚¹çš„æ¢¯åº¦ç»Ÿè®¡é‡ã€‚

#### 3.2 Approximate Algorithm

The exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily. However, it is impossible to efficiently do so when the data does not fit entirely into memory. Same problem also arises in the distributed setting. To support effective gradient tree boosting in these two settings, an approximate algorithm is needed.

We summarize an approximate framework, which resembles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm first proposes candidate splitting points according to percentiles of feature distribution (a specific criteria will be given in Sec. 3.3). The algorithm then maps the continuous features into buckets split by these candidate points, aggregates the statistics and finds the best solution among proposals based on the aggregated statistics.

> The exact greedy ç®—æ³•æ˜¯éå¸¸å¼ºå¤§çš„ï¼Œå› ä¸ºå®ƒè´ªå©ªåœ°æšä¸¾äº†æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹ã€‚ç„¶è€Œï¼Œå½“æ•°æ®ä¸èƒ½å®Œå…¨è¯»å…¥å†…å­˜æ—¶ï¼Œè¿™æ ·åšå°±ä¸ä¼šå¾ˆæœ‰æ•ˆç‡ã€‚åŒæ ·çš„é—®é¢˜ä¹Ÿå‡ºç°åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ã€‚ä¸ºäº†åœ¨è¿™ä¸¤ç§è®¾ç½®ä¸‹æ”¯æŒæœ‰æ•ˆçš„æ¢¯åº¦æ ‘æå‡ï¼Œéœ€è¦ä¸€ç§è¿‘ä¼¼ç®—æ³•ã€‚
>
> æˆ‘ä»¬åœ¨ ALG.2 ä¸­æå‡ºäº†ä¸€ä¸ªè¿‘ä¼¼çš„æ¡†æ¶ï¼Œå®ƒç±»ä¼¼äºè¿‡å»æ–‡çŒ®[17ï¼Œ2ï¼Œ22]ä¸­æå‡ºçš„æ€æƒ³ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥ç®—æ³•é¦–å…ˆæ ¹æ®ç‰¹å¾åˆ†å¸ƒçš„ç™¾åˆ†ä½æ•°æå‡ºå€™é€‰åˆ†å‰²ç‚¹(å…·ä½“æ ‡å‡†å°†åœ¨3.3èŠ‚ç»™å‡º)ã€‚
>
> ç„¶åï¼Œè¯¥ç®—æ³•å°†è¿ç»­çš„ç‰¹å¾æ˜ å°„åˆ°ç”±è¿™äº›å€™é€‰åˆ†å‰²ç‚¹åˆ†å‡ºçš„æ¡¶ä¸­ï¼Œè®¡ç®—å‡ºæ¯ä¸ªç®±å­ä¸­æ•°æ®çš„ç»Ÿè®¡é‡ï¼ˆæ³¨ï¼šè¿™é‡Œçš„ç»Ÿè®¡é‡æŒ‡çš„æ˜¯å…¬å¼ï¼ˆ7ï¼‰ä¸­çš„ $g$ å’Œ $h $ï¼‰ï¼Œç„¶åæ ¹æ®ç»Ÿè®¡é‡æ‰¾åˆ°æœ€ä½³çš„åˆ†å‰²ç‚¹ã€‚

There are two variants of the algorithm, depending on when the proposal is given. The global variant proposes all the candidate splits during the initial phase of tree construction, and uses the same proposals for split finding at all levels. The local variant re-proposes after each split. The global method requires less proposal steps than the local method. However, usually more candidate points are needed for the global proposal because candidates are not refined after each split. The local proposal refines the candidates after splits, and can potentially be more appropriate for deeper trees. A comparison of different algorithms on a Higgs boson dataset is given by Fig. 3. We find that the local proposal indeed requires fewer candidates. The global proposal can be as accurate as the local one given enough candidates.

Most existing approximate algorithms for distributed tree learning also follow this framework. Notably, it is also possible to directly construct approximate histograms of gradient statistics [22]. It is also possible to use other variants of binning strategies instead of quantile [17]. Quantile strategy benefit from being distributable and recomputable, which we will detail in next subsection. From Fig. 3, we also find that the quantile strategy can get the same accuracy as exact greedy given reasonable approximation level.

Our system efficiently supports exact greedy for the single machine setting, as well as approximate algorithm with both local and global proposal methods for all settings. Users can freely choose between the methods according to their needs.

![Figure3](/Users/helloword/Anmingyu/Gor-rok/Papers/GBDT/XGB/Fig3.png)

**Figure 3: Comparison of test AUC convergence on Higgs 10M dataset. The eps parameter corresponds to the accuracy of the approximate sketch. This roughly translates to 1 / eps buckets in the proposal. We find that local proposals require fewer buckets, because it refine split candidates.**

![Alg2](/Users/helloword/Anmingyu/Gor-rok/Papers/GBDT/XGB/Alg2.png)

> è¯¥ç®—æ³•æœ‰ä¸¤ç§å˜ç§ï¼Œå–å†³äºåˆ†å‰²çš„æ—¶é—´ã€‚
>
> å…¨å±€é€‰æ‹©åœ¨æ ‘æ„é€ çš„åˆå§‹é˜¶æ®µç»™å‡ºæ‰€æœ‰å€™é€‰åˆ†è£‚ç‚¹ï¼Œå¹¶ä¸”åœ¨æ ‘çš„æ‰€æœ‰å±‚ä¸­ä½¿ç”¨ç›¸åŒçš„åˆ†è£‚èŠ‚ç‚¹ç”¨äºåˆ†è£‚ã€‚å±€éƒ¨é€‰æ‹©åœ¨åˆ†è£‚åé‡æ–°ç»™å‡ºåˆ†è£‚å€™é€‰èŠ‚ç‚¹ã€‚
>
> å…¨å±€æ–¹æ³•æ¯”å±€éƒ¨æ–¹æ³•éœ€è¦æ›´å°‘çš„æ­¥éª¤ã€‚ç„¶è€Œï¼Œé€šå¸¸åœ¨å…¨å±€é€‰æ‹©ä¸­éœ€è¦æ›´å¤šçš„å€™é€‰ç‚¹ï¼Œå› ä¸ºåœ¨æ¯æ¬¡åˆ†è£‚åå€™é€‰èŠ‚ç‚¹æ²¡æœ‰è¢«æ›´æ–°ã€‚å±€éƒ¨é€‰æ‹©åœ¨åˆ†è£‚åæ›´æ–°å€™é€‰èŠ‚ç‚¹ï¼Œå¹¶ä¸”å¯èƒ½æ›´é€‚åˆäºæ·±åº¦æ›´æ·±çš„æ ‘ã€‚å›¾3 ç»™å‡ºäº†åŸºäºå¸Œæ ¼æ–¯ç»è‰²å­æ•°æ®é›†çš„ä¸åŒç®—æ³•çš„æ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°ï¼Œæœ¬åœ°å˜ç§ç¡®å®éœ€è¦æ›´å°‘çš„å€™é€‰èŠ‚ç‚¹ã€‚å½“ç»™å‡ºè¶³å¤Ÿçš„å€™é€‰èŠ‚ç‚¹ï¼Œå…¨å±€å˜ç§å¯ä»¥è¾¾åˆ°ä¸æœ¬åœ°å˜ç§ä¸€æ ·çš„å‡†ç¡®ç‡ã€‚
>
> ç°æœ‰çš„å¤§å¤šæ•°åˆ†å¸ƒå¼æ ‘æ¨¡å‹å­¦ä¹ çš„è¿‘ä¼¼ç®—æ³•ä¹Ÿéµå¾ªè¿™ä¸€æ¡†æ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿˜å¯ä»¥ç›´æ¥æ„å»ºæ¢¯åº¦ç»Ÿè®¡é‡çš„è¿‘ä¼¼ç›´æ–¹å›¾[22]ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–ä¸åŒçš„åˆ†æ¡¶ç­–ç•¥æ¥ä»£æ›¿åˆ†ä½æ•°[17]ã€‚åˆ†ä½æ•°ç­–ç•¥å—ç›Šäºå¯åˆ†å‘å’Œå¯é‡æ–°è®¡ç®—ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€å°èŠ‚ä¸­è¯¦ç»†è¯´æ˜è¿™ä¸€ç‚¹ã€‚ä» å›¾3 æˆ‘ä»¬è¿˜å‘ç°ï¼Œå½“è®¾ç½®åˆç†çš„è¿‘ä¼¼æ°´å¹³ï¼Œåˆ†ä½æ•°ç­–ç•¥å¯ä»¥è·å¾—ä¸ exact greedy ç›¸åŒçš„ç²¾åº¦ã€‚
>
> æˆ‘ä»¬çš„ç³»ç»Ÿæœ‰æ•ˆåœ°æ”¯æŒå•æœºç¯å¢ƒä¸‹çš„ exact greedyï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒè¿‘ä¼¼ç®—æ³•çš„local å˜ç§å’Œ global å˜ç§çš„æ‰€æœ‰è®¾ç½®ã€‚ä½¿ç”¨è€…å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦è‡ªç”±é€‰æ‹©ä¸åŒçš„æ–¹æ³•ã€‚

#### 3.3 Weighted Quantile Sketch

One important step in the approximate algorithm is to propose candidate split points. Usually percentiles of a feature are used to make candidates distribute evenly on the data. Formally, let multi-set $\mathcal{D}_{k}=\left\{\left(x_{1 k}, h_{1}\right),\left(x_{2 k}, h_{2}\right) \cdots\left(x_{n k}, h_{n}\right)\right\}$ represent the $k$-th feature values and second order gradient statistics of each training instances. We can define a rank functions $r_{k}: \mathbb{R} \rightarrow[0,+\infty)$ as
$$
r_{k}(z)=\frac{1}{\sum_{(x, h) \in \mathcal{D}_{k}} h} \sum_{(x, h) \in \mathcal{D}_{k}, x<z} h \qquad(8)
$$
which represents the proportion of instances whose feature value $k$ is smaller than $z$. The goal is to find candidate split points $\{s_{k1}, s_{k2},\cdots,s_{kl}\}$, such that
$$
\left|r_{k}\left(s_{k, j}\right)-r_{k}\left(s_{k, j+1}\right)\right|<\epsilon, \quad s_{k 1}=\min _{i} \mathbf{x}_{i k}, s_{k l}=\max _{i} \mathbf{x}_{i k}
$$
Here $\epsilon$ is an approximation factor. Intuitively, this means that there is roughly $1/\epsilon$ candidate points. Here each data point is weighted by $h_i$. To see why $h_i$ represents the weight, we can rewrite Eq (3) as
$$
\sum_{i=1}^{n} \frac{1}{2} h_{i}\left(f_{t}\left(\mathbf{x}_{i}\right)-g_{i} / h_{i}\right)^{2}+\Omega\left(f_{t}\right)+\text { constant }
$$
which is exactly weighted squared loss with labels $g_i/h_i$  and weights $h_i$. For large datasets, it is non-trivial to find candidate splits that satisfy the criteria. When every instance has equal weights, an existing algorithm called quantile sketch [14, 24] solves the problem. However, there is no existing quantile sketch for the weighted datasets. Therefore, most existing approximate algorithms either resorted to sorting on a random subset of data which have a chance of failure or heuristics that do not have theoretical guarantee.

To solve this problem, we introduced a novel distributed weighted quantile sketch algorithm that can handle weighted data with a provable theoretical guarantee. The general idea is to propose a data structure that supports merge and prune operations, with each operation proven to maintain a certain accuracy level. A detailed description of the algorithm as well as proofs are given in the appendix.

> è¿‘ä¼¼ç®—æ³•ä¸­å¾ˆé‡è¦çš„ä¸€æ­¥æ˜¯åˆ—å‡ºå€™é€‰çš„åˆ†å‰²ç‚¹ã€‚é€šå¸¸ç‰¹å¾çš„ç™¾åˆ†ä½æ•°ä½œä¸ºå€™é€‰åˆ†å‰²ç‚¹çš„åˆ†å¸ƒä¼šæ¯”è¾ƒå‡åŒ€ã€‚
>
> å½¢å¼åŒ–åœ°è®²ï¼Œè®¾å¤šå…ƒé›†åˆ $\mathcal{D}_{k}=\left\{\left(x_{1 k}, h_{1}\right),\left(x_{2 k}, h_{2}\right) \cdots\left(x_{n k}, h_{n}\right)\right\}$ è¡¨ç¤ºæ ·æœ¬çš„ç¬¬ $k$ ä¸ªç‰¹å¾çš„å–å€¼å’Œå…¶äºŒé˜¶æ¢¯åº¦ç»Ÿè®¡é‡ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªæ’åºå‡½æ•° $r_{k}: \mathbb{R} \rightarrow[0,+\infty)$ :
> $$
> r_{k}(z)=\frac{1}{\sum_{(x, h) \in \mathcal{D}_{k}} h} \sum_{(x, h) \in \mathcal{D}_{k}, x<z} h \qquad(8)
> $$
> ä¸Šå¼è¡¨ç¤ºæ ·æœ¬ä¸­ç¬¬ $k$ä¸ªç‰¹å¾çš„å–å€¼å°äº $z$ çš„æ¯”ä¾‹ (æ³¨ï¼šç‰¹å¾å€¼å°äº $z$ çš„äºŒé˜¶æ¢¯åº¦ç»Ÿè®¡é‡çš„æ¯”ä¾‹)ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°å€™é€‰çš„åˆ†å‰²èŠ‚ç‚¹ $\{s_{k1}, s_{k2},\cdots,s_{kl}\}$
> $$
> \left|r_{k}\left(s_{k, j}\right)-r_{k}\left(s_{k, j+1}\right)\right|<\epsilon, \quad s_{k 1}=\min _{i} \mathbf{x}_{i k}, s_{k l}=\max _{i} \mathbf{x}_{i k}
> \qquad(9)
> $$
> è¿™é‡Œ $\epsilon$ æ˜¯ä¸€ä¸ªè¿‘ä¼¼å› å­ã€‚ç›´è§‚åœ°è¯´ï¼Œè¿™æ„å‘³ç€å¤§çº¦æœ‰ $1/\epsilon$ å€™é€‰ç‚¹ã€‚è¿™é‡Œï¼Œæ¯ä¸ªæ•°æ®ç‚¹ç”± $h_i$ åŠ æƒã€‚è¦äº†è§£ä¸ºä»€ä¹ˆ $h_i$ è¡¨ç¤ºæƒé‡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¬å¼(3)é‡å†™ä¸º
> $$
> \sum_{i=1}^{n} \frac{1}{2} h_{i}\left(f_{t}\left(\mathbf{x}_{i}\right)-g_{i} / h_{i}\right)^{2}+\Omega\left(f_{t}\right)+\text { constant }
> $$
> è¿™å®é™…ä¸Šæ˜¯æƒå€¼ä¸º $h_i$ï¼Œæ ‡ç­¾ä¸º $g_i/h_i$ çš„åŠ æƒå¹³æ–¹æŸå¤±ã€‚å¯¹äºå¤§æ•°æ®é›†æ¥è¯´ï¼Œæ‰¾åˆ°æ»¡è¶³æ ‡å‡†çš„å€™é€‰åˆ†å‰²ç‚¹æ˜¯éå¸¸ä¸å®¹æ˜“çš„ã€‚å½“æ¯ä¸ªå®ä¾‹å…·æœ‰ç›¸ç­‰çš„æƒé‡æ—¶ï¼Œä¸€ä¸ªç°å­˜çš„å« quantile sketch çš„ç®—æ³•è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹äºåŠ æƒçš„æ•°æ®é›†æ²¡æœ‰ç°æˆçš„ quantile sketch ç®—æ³•ã€‚å› æ­¤ï¼Œå¤§éƒ¨åˆ†ç°å­˜çš„è¿‘ä¼¼ç®—æ³•è¦ä¹ˆå¯¹å¯èƒ½å¤±è´¥çš„æ•°æ®çš„éšæœºå­é›†è¿›è¡Œæ’åºï¼Œè¦ä¹ˆä½¿ç”¨æ²¡æœ‰ç†è®ºä¿è¯çš„å¯å‘å¼ç®—æ³•ã€‚
>
> ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¼åŠ æƒ quantile sketch ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥å¤„ç†åŠ æƒæ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥ä»ç†è®ºä¸Šè¯æ˜ã€‚é€šå¸¸çš„åšæ³•æ˜¯æå‡ºä¸€ç§æ”¯æŒ merge å’Œ prune æ“ä½œçš„æ•°æ®ç»“æ„ï¼Œæ¯ä¸ªæ“ä½œéƒ½æ˜¯å¯ä»¥è¢«è¯æ˜ä¿æŒä¸€å®šå‡†ç¡®åº¦çš„ã€‚é™„å½•ä¸­ç»™å‡ºäº†ç®—æ³•çš„è¯¦ç»†æè¿°ä»¥åŠè¯æ˜ã€‚

#### 3.4 Sparsity-aware Split Finding

In many real-world problems, it is quite common for the input $\mathbb{x}$ to be sparse. There are multiple possible causes for sparsity: 

1. presence of missing values in the data;
2. frequent zero entries in the statistics; 
3. artifacts of feature engineering such as one-hot encoding.

It is important to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. 4. When a value is missing in the sparse matrix $\mathbb{x}$, the instance is classified into the default direction. 

There are two choices of default direction in each branch. The optimal default directions are learnt from the data. The algorithm is shown in Alg. 3. The key improvement is to only visit the non-missing entries $I_k$. The presented algorithm treats the non-presence as a missing value and learns the best direction to handle missing values. 

The same algorithm can also be applied when the non-presence corresponds to a user specified value by limiting the enumeration only to consistent solutions.

To the best of our knowledge, most existing tree learning algorithms are either only optimized for dense data, or need specific procedures to handle limited cases such as categorical encoding. XGBoost handles all sparsity patterns in a unified way. More importantly, our method exploits the sparsity to make computation complexity linear to number of non-missing entries in the input. 

Fig. 5 shows the comparison of sparsity aware and a naive implementation on an Allstate-10K dataset (description of dataset given in Sec. 6). We find that the sparsity aware algorithm runs 50 times faster than the naive version. This confirms the importance of the sparsity aware algorithm.

![Figure5](/Users/helloword/Anmingyu/Gor-rok/Papers/GBDT/XGB/Fig5.png)

**Figure 5: Impact of the sparsity aware algorithm on Allstate-10K. The dataset is sparse mainly due to one-hot encoding. The sparsity aware algorithm is more than 50 times faster than the naive version that does not take sparsity into consideration.**

![Algorithm3](/Users/helloword/Anmingyu/Gor-rok/Papers/GBDT/XGB/Alg3.png)

> åœ¨è®¸å¤šå®é™…é—®é¢˜ä¸­ï¼Œè¾“å…¥ $\mathbb{x}$ ç¨€ç–æ˜¯å¾ˆå¸¸è§çš„ã€‚ç¨€ç–æœ‰å¤šç§å¯èƒ½çš„åŸå› ï¼š
>
> 1. æ•°æ®ä¸­å­˜åœ¨ç¼ºå¤±å€¼ï¼›
> 2. å¤§é‡é›¶å€¼ï¼›
> 3. ç‰¹å¾å·¥ç¨‹ï¼Œå¦‚ one-hotã€‚
>
> é‡è¦çš„æ˜¯è®©ç®—æ³•æ„ŸçŸ¥æ•°æ®ä¸­çš„ç¨€ç–æ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºè®®åœ¨æ¯ä¸ªæ ‘èŠ‚ç‚¹ä¸­æ·»åŠ ä¸€ä¸ªé»˜è®¤æ–¹å‘ï¼Œå¦‚ å›¾4 æ‰€ç¤ºã€‚å½“ç¨€ç–çŸ©é˜µ $\mathbb{x}$ ä¸­ç¼ºç¼ºå¤±æ—¶ï¼Œå®ä¾‹è¢«åˆ†ç±»ä¸ºé»˜è®¤æ–¹å‘ã€‚
>
> æ¯ä¸ªåˆ†æ”¯ä¸­æœ‰ä¸¤ä¸ªé»˜è®¤æ–¹å‘é€‰æ‹©ã€‚ä»æ•°æ®ä¸­å­¦ä¹ æœ€ä½³é»˜è®¤æ–¹å‘ã€‚è¯¥ç®—æ³•åœ¨Alg.3. ä¸­å®ç°ã€‚å…³é”®æ”¹è¿›æ˜¯åªè®¿é—®éç¼ºå¤±çš„ item $I_k$ã€‚è¯¥ç®—æ³•å°†ä¸å­˜åœ¨è§†ä¸ºç¼ºå¤±å€¼ï¼Œå¹¶å­¦ä¹ æœ€ä½³æ–¹å‘æ¥å¤„ç†ç¼ºå¤±å€¼ã€‚
>
> The same algorithm can also be applied when the non-presence corresponds to a user specified value by limiting the enumeration only to consistent solutions.
>
> æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå¤§å¤šæ•°ç°æœ‰çš„æ ‘å­¦ä¹ ç®—æ³•è¦ä¹ˆåªå¯¹è¿ç»­æ•°æ®è¿›è¡Œä¼˜åŒ–ï¼Œè¦ä¹ˆéœ€è¦ç‰¹å®šçš„è¿‡ç¨‹æ¥å¤„ç†éƒ¨åˆ†æƒ…å†µï¼Œä¾‹å¦‚ç±»åˆ«ç¼–ç ã€‚XGBoost ä»¥ç»Ÿä¸€çš„æ–¹å¼å¤„ç†æ‰€æœ‰ç¨€ç–æ¨¡å¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ç¨€ç–æ€§ï¼Œä½¿å¾—è®¡ç®—çš„å¤æ‚åº¦ä¸è¾“å…¥ä¸­çš„éç¼ºå¤±æ•°æ®çš„æ•°é‡æˆçº¿æ€§å…³ç³»ã€‚å›¾5 æ˜¾ç¤ºäº†ç¨€ç–æ„ŸçŸ¥ç®—æ³•å’Œä¸€ä¸ªå¸¸è§„ç®—æ³•åœ¨æ•°æ®é›† Allstate-10Kï¼ˆæ­¤æ•°æ®é›†åœ¨ç¬¬6éƒ¨åˆ†æè¿°ï¼‰ä¸Šçš„æ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°ç¨€ç–æ„ŸçŸ¥ç®—æ³•çš„è¿è¡Œé€Ÿåº¦æ¯”å¸¸è§„ç‰ˆæœ¬å¿« 50 å€ã€‚è¿™è¯å®äº†ç¨€ç–æ„ŸçŸ¥ç®—æ³•çš„é‡è¦æ€§ã€‚
>

