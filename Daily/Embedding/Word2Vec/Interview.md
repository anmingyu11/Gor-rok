[word2vec经验](https://zhuanlan.zhihu.com/p/29364112)

[史上最全词向量详解](https://zhuanlan.zhihu.com/p/75391062)

![](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/skipgram.jpg)

1. 
2. Word2Vec是如何得到词向量的？



##  word2vec 是word embedding 最好的工具吗？

word2vec并非是效果最好的word embedding 工具。最容易看出的就是word2vec没有考虑语序，这里会有训练效果损失。

由于 word2vec 训练速度快 ，易用，google出品 等，使得word2vec使用的人多。

训练快是因为 word2vec只有输入层和输出层，砍去了神经网络中，隐藏层的耗时计算（所以word2vec并不算是一个深度学习算法）。另外，阅读word2vec的google的源码，会发现里面有一些提速的trick。如 sigmod函数，采用一次计算，以后查表，减去了大量的重复计算。如词典hash存储， 层次softmax等。

易用是因为word2vec 公布了word2vec的代码。在tensorflow,gensim,spark mllib包中都有集成，使用方便。



word2vec 的原理就是 一个词预测 前后词 或者 前后词 预测 当前词，使得概率最大化。

这就导致如下两个结果：

**2.1.1 相似的句子，相同部位的词 会相似。**

比如 句子1 w1 w2 w3 w4 X w5 w6 w7.

句子2 w1 w2 w3 w5 Y w5 w6 w7.

因为 X 的向量 受 w1 w2 w3 w4 w5 w6 w7 向量影响决定， Y也是受这几个词影响决定。

所以 X Y 是相似的。





## 2.2 算法参数的影响。

算法参数对总体效果影响不大。相对来说，比较重要的参数有以下：

**2.2.1 降采样(subsampling)**

降采样越低，对高频词越不利，对低频词有利。可以这么理解，本来高频词 词被迭代50次，低频词迭代10次，如果采样频率降低一半，高频词失去了25次迭代，而低频词只失去了5次。一般设置成le-5。个人觉得，降采样有类似tf-idf的功能，降低高频词对上下文影响的权重。

**2.2. 2 语言模型**

skip-gram 和cbow,之前有对比，切词效果偏重各不相同。
从效果来看，感觉cbow对词频低的词更有利。这是因为 cbow是基于周围词来预测某个词，虽然这个词词频低，但是他是基于 周围词训练的基础上，通过算法来得到这个词的向量。通过周围词的影响，周围词训练的充分，这个词就会收益。

**2.2. 3 窗口大小**

窗口大小影响 词 和前后多少个词的关系，和语料中语句长度有关，建议可以统计一下语料中，句子长度的分布，再来设置window大小。一般设置成8。

**2.2. 4 min-count**

最小词频训练阀值，这个根据训练语料大小设置，只有词频超过这个阀值的词才能被训练。

根据经验，如果切词效果不好，会切错一些词，比如 “在深圳”，毕竟切错的是少数情况，使得这种错词词频不高，可以通过设置相对大一点的 min-count 过滤掉切错的词。

**2.2. 5 向量维度**

如果词量大，训练得到的词向量还要做语义层面的叠加，比如 句子 的向量表示 用 词的向量叠加，为了有区分度，语义空间应该要设置大一些，所以维度要偏大。一般 情况下200维够用。

**2.2. 6 其他参数**

比如学习率 可以根据需要调。



## 3 word2vec 影响速度的因素有哪些？

## 3.1 语言模型：cbow 比skip-gram 更快

为什么 cbow更快，很重要的一个原因，cbow是基于周围词来预测这个单词本身 。而skip-gram是基于本身词去预测周围词。 那么，cbow只要 把窗口内的其他词相加一次作为输入来预测 一个单词。不管窗口多大，只需要一次运算。而skip-gram直接受窗口影响，窗口越大，需要预测的周围词越多。在训练中，通过调整窗口大小明显感觉到训练速度受到很大影响。

## 3.2 迭代次数

影响训练次数，语料不够的情况下，可以调大迭代次数。spark 版本有bug，迭代次数超过1，训练得到的词向量维度值超大。

## 3.4 其他参数

采样频率 影响词的训练频率
min-count 最小词频 影响 训练词的数量
Window大小 影响 skip-gram 的 预测次数。
向量维度 维度决定了训练过程中计算的维度

## 4 怎样评估word2vec训练的好坏？

## 4.1 词聚类

可以采用 kmeans 聚类，看聚类簇的分布

## 4.2 词cos 相关性

查找cos相近的词

## 4.3 Analogy对比

a:b 与 c:d的cos距离 (man-king woman-queen )

## 4.4 使用tnse，pca等降维可视化展示

词的分布，推荐用google的[tensorboard](https://link.zhihu.com/?target=https%3A//www.tensorflow.org/get_started/summaries_and_tensorboard)，可以多视角查看，如果不想搭建服务，直接[访问这里](https://link.zhihu.com/?target=http%3A//projector.tensorflow.org/)。另外可以用python的matplotlib。



## 词向量从哪里来的？

首先我们把语料读进来，然后要把作为文本信息的语料，转换为可以进行数学计算的数字形式。我们首先要统计语料中的所有词语（如果是中文的话还要先进行分词处理），然后建立一个字典，让每一个词语都唯一对应一个数字ID，最后再把每一句话都变成跟词语一一对应的一串数字ID，这个过程叫做tokenization，一般翻译成“标记化”或者“令牌化”。

## HS

层次Softmax的方法最早由Bengio在05年引入到语言模型中。它的**基本思想是将复杂的归一化概率分解为一系列条件概率乘积的形式**：
$$
p(v|context)=\prod_{i=1}^m{p(b_i(v)|b_1(v), ..., b_{i-1}(v), context)}
$$
其中，每一层条件概率对应一个二分类问题，可以通过一个简单的逻辑回归函数去拟合。这样，我们将对V个词的概率归一化问题，转化成了对logV个词的概率拟合问题。

层次Softmax是一个很巧妙的模型。它通过构造一颗二叉树，将目标概率的计算复杂度**从最初的V降低到了logV** 的量级。不过付出的**代价是人为增强了词与词之间的耦合性**。例如，**一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响。**因此，构造一颗有意义的二叉树就显得十分重要。实践证明，在实际的应用中，**基于Huffman编码的二叉树可以满足大部分应用场景的需求。**

