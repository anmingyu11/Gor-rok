## 什么是Word2vec?



举个简单例子，判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。



- hierarchical softmax

- - 本质是把 N 分类问题变成 log(N)次二分类



- negative sampling

- - 本质是预测总体类别的一个子集

-------------------



## XinRong



本质上是把W的第k行复制到h。 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw_I%7D) 是输入单词 ![[公式]](https://www.zhihu.com/equation?tex=w_I) 的向量表示。这意味着隐含层单元的连接(激活)函数是简单的线性函数(即，直接将输入的加权和传递给下一层)。



从隐含层到输出层，有一个不同的权值矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W%27%3D%5C%7B%7Bw_%7Bij%7D%27%7D%5C%7D) ,这是一个N×V的矩阵。使用这个权值，我们可以计算词汇表中每个单词的分数 ![[公式]](https://www.zhihu.com/equation?tex=u_j) ,

-------------

## DouDou



## NNLM(Neural Network Language Model)

input : 
$$
w_1,w_2... w_{n-1}
$$
output:
$$
w_n
$$
word的one-hot编码

$w_i$的为($V\times1$)的列向量，one-hot编码
$$
w_1, w_2 ,..., w_{n-1}
$$
$w_i \times V \times P$

模型过于复杂

参数量过大

网络结构

## Word2Vec



### CBOW

Continuous Bag of Word

Context -> target



为解决模型中语料库过大，训练速度慢的问题。

Hierarchical Softmax

Negative Sampling

## CBOW和skip-gram的差异

- Skip-gram效果比CBOW好；
- Skip-gram训练时间长，但是对低频词(生僻词)效果好；CBOW训练时间短，对低频词效果比较差。





--------------------



作者：gggctgg
链接：https://www.zhihu.com/question/44832436/answer/864708693
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



参考斋藤康毅的书《深度学习入门2自然语言处理篇》，这本书还没翻译成中文版，但写的很详细，首先介绍了one-hot编码，假设你的文本总共有100个词汇，那就用一个100维的矢量来表示一个词，其中的一维数字为1，其他维是0（这也叫稀疏的表示，仅有一维有实际的信息），这样就得到了最简单的一种用矢量表示单词的方法，然而这种方法有缺陷，就是词汇越多，维数越多，计算量也越大。

我们希望用较低维的矢量来表示每个单词，这就是embedding的方法，embedding里表示每一个单词的维数比one-hot要小，而且每一维都表示了单词的某种内在特性，这种特性人类可能难以理解，但又是确实存在的。

学习embedding的表达应该怎么做呢？首先你要知道一个单词的意思是由它的上下文（context，你可以理解为在它前后的单词）决定的，为了简单起见，我们假设一个单词（target）的意思可由它前一个单词（input1）和后一个单词（input2）决定（这里的target，input1，input2都是单词对应的one-hot编码矢量，而且是行矢量，元素数等于词汇数n）。

我们来训练这么一个神经网络，输入input1和input2，input1和input2分别乘以同一个矩阵w1（**w1的行数等于n，列数是自己定的一个远小于词汇数的数m**），得到两个相同维数的行矢量，这两个矢量相加取平均得到中间层行矢量（这一过程是encoder），这个行矢量的维数是m，表示被压缩进m维空间里的input1和input2的综合信息（具体每一维表示什么信息人类并不理解），我们再把这个中间层行矢量乘以另一个矩阵w2（w2的行数为m，列数为n），得到一个输出output（n维行矢量，这一过程是decoder），这个output我们希望它跟target越像越好，因此通过不断训练更新w1和w2，最终我们可以得到这样一个网络。

此时w1和w2里就储存了我们想要的词编码信息，w1的第i行和w2的第i列都可用来表示one-hot里第i个单词，大多数用w1的第i行（m维行矢量）来表示，你也可以w1的i行与w2的i列的转置加起来取平均来表示。

-----------------------

首先必须明确的是，word2vec的词向量（也就是论文里的word embedding）是基于某个特定语料的，单独拿出任一个词向量都没有什么意义，它的作用在于众多词向量之间的相对差异所表现出来的语义表达。

**（1）关于向量的维数**。这是事先设定的，这个值至于是多少目前并没有很严格的理论依据，如果你自己运行word2vec源码你自己设100，200都是可以，当然得到的词向量在做具体的NLP任务（比如题主说的文本聚类任务）时是效果也是会有差异的，可以多设几次，哪次效果好你就选哪个维数，而300这个常用数值也是mikovo在论文里提到的。

**（2）关于向量各元素值。**正如上面某同学提到初始值是随机的，然后以语料中哪几个词离得近让这两个词对应的词向量的距离越近为目标不断地修改向量各元素值，从这个角度来看，实际上就是迭代过程。至于怎么迭代的，如何迭代这个修改过程就要提到神经语言模型了，如果想研究的话入门论文我建议yoshua bengio的 A Neural Probabilistic Language Model ，整篇论文娓娓道来，很详细也很容易懂，另外还有Mikovo的2013系列，再有时间甚至word2vec的源码也可以拿来研究一下。其他的辅助资上面的回答已经给出很好的材料了，我就不多赘述了。

作者：shirley
链接：https://www.zhihu.com/question/44832436/answer/179769034
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

--------------------------------

看到这句“而后者的输入就貌似已经是词向量了”我猜题主真正想问的应该是“最初的词向量是哪来的”，因为对于这个问题我曾经也纠结了很久。

所以现在我来从实际的代码出发解释一下，我用的pytorch，用Keras或者TF的老铁可以自行查阅各自的框架API。

首先我们把语料读进来，然后要把作为文本信息的语料，转换为可以进行数学计算的数字形式。我们首先要统计语料中的所有词语（如果是中文的话还要先进行分词处理），然后建立一个字典，让每一个词语都唯一对应一个数字ID，最后再把每一句话都变成跟词语一一对应的一串数字ID，这个过程叫做tokenization，一般翻译成“标记化”或者“令牌化”。

配合pytorch使用，有一个库叫torchtext，可以完成这个操作：

![](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/jiangyucheng_1.png)

输出结果差不多是这样：

![img](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/jiangyucheng_2.png)

第一行那个defaultdict输出的是torchtext工具生成的字典，字典的key是词，value是对应的数字ID。

下面是输出的原始语料和tokenization之后的语料，可以看到第一个语料“备胎是硬伤！”，这里面“是”的ID为7，“！”（中文感叹号）的ID是8，然后下面经过tokenization的第一句话，对应这两个词的数字ID恰好也是7和8。

字典里的<unk>表示字典中没有出现过的词，而<pad>表示padding，因为很多模型一般都要求输入的语料长度一致，所以一种常用的处理方式就是把每一句话都跟语料库中曾经出现过的最长的一句话对齐——实际不足的部分就用padding补齐。所以我们可以看到，第一句话的后面全都是表示padding的ID 1。

然后在搭建模型的过程中，我们可以直接使用已经定义好的常见神经网络层——其中有一种embedding层，就是我们所说的词嵌入层。

![img](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/jiangyucheng_3.png)

Embedding层的参数，是一个L * D大小的矩阵——L是上一步生成的词典的长度，d是词向量的维度。

Embedding层接受数字作为输入，然后返回数字所对应的矩阵列下标的向量。

比如我们输入[0,1,2]，Embedding层就会返回矩阵的第0列、第1列和第2列的列向量——实际返回的是这三个列向量拼成的一个3 * D大小的矩阵：

![](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/jiangyucheng_4.png)

于是让我们回到最初的问题——“最初的词向量是哪来的”，这个Embedding层里已有的“词向量”数字是怎么出现的呢？

其实答案很简单：

简单地说，包括Embedding层在内，torch.nn里提供的所有常用的神经网络层，刚建立的时候里面的参数都是**随机生成的一堆没什么意义的随机数**。

当然呢，我们也不能真的瞎JB填，具体怎么对模型参数进行初始化，也是有讲究的——不过这里我们先不讨论这些，继续往下看。

然后就是对模型进行训练了。有一点你一定要明确，**词向量参数是瞎JB填的，完全不影响模型按照Skip-gram或者CBOW的方法来进行相应的计算**。当然呢，计算出来的结果，就跟正确答案差得远了——所以我们才要用真实语料对模型进行训练。

具体的训练过程其他答案和网上的文章已经讲得非常清楚了，我就不再废话了——简而言之，通过真实语料的词与词之间的共现关系，计算出你现在的瞎JB填参数的模型错的有多离谱，也就是损失函数，然后对损失函数进行求导，通过反向传播求出你一开始瞎JB填的各个词向量参数的导数，然后更新这些参数的值。

在pytorch里，训练的过程差不多是这样的：首先用模型进行一次预测，得到result，然后用损失函数（可以是事先定义好的，也可以是你自己写的）计算loss的值，再对loss进行求导，最后让事先准备好的优化器（现在比较常用的是Adam）根据求出的导数更新模型参数。pytorch已经实现了自动求导和反向传播，你直接调loss的backward方法就可以了：

![](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/jiangyucheng_5.png)

这样一来，经过大量语料训练之后，你的Embedding层就已经是比较准确的词向量了。当然目前我们一般不会直接用自己的语料训练词向量，而是使用别人在超大量语料库上训好的词向量，直接用这些现成的词向量进行下游任务。除了w2v之外，比较常用的还有GloVe。至于ELMo和BERT，它的原理就跟w2v完全不同了，这个需要单独学习。现在你明白了吗？

作者：到处挖坑蒋玉成
链接：https://www.zhihu.com/question/44832436/answer/802442061
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

--------------------

-- 什么，输入也能优化？
-- 对。CBOW也好，Skip-gram也好，还是各种各样其他NNLM也好，简单来看，不过是**模型任务**的区别。通过建模，每个词与其**上下文**建立了联系；通过训练，参数和**输入**得到优化。最终，在实现损失函数minimization的同时，得到一份饱含semantic properties的词向量。很简单吧 :)

作者：斤木
链接：https://www.zhihu.com/question/44832436/answer/112678396
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



---------------------------------------

由于在训练过程中，我们是通过训练语料中的上下文—目标单词对来迭代地更新模型参数，每次迭代更新对向量的影响也是累积的。我们可以想象成单词w的输出向量被w所有周围词的输入向量的来回往复的拖拽。就好比有真实的琴弦在单词w和其他词之间。同样的，输入向量也可以被想象成被很多输出向量拖拽。这种解释可以提醒我们想象成一个重力，或者其他力量所指引的输出图。每根弦的均衡长度跟共同出现的关联单词对之间的力量权衡有关，也跟学习率有关。经过多次迭代，输入和输出向量的相对位置将趋于平稳。