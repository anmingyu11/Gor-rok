> https://www.cnblogs.com/guoyaohua/p/9240336.html

2013å¹´ï¼ŒGoogleå¼€æºäº†ä¸€æ¬¾ç”¨äºè¯å‘é‡è®¡ç®—çš„å·¥å…·â€”â€”word2vecï¼Œå¼•èµ·äº†å·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œçš„å…³æ³¨ã€‚é¦–å…ˆï¼Œword2vecå¯ä»¥åœ¨ç™¾ä¸‡æ•°é‡çº§çš„è¯å…¸å’Œä¸Šäº¿çš„æ•°æ®é›†ä¸Šè¿›è¡Œé«˜æ•ˆåœ°è®­ç»ƒï¼›å…¶æ¬¡ï¼Œè¯¥å·¥å…·å¾—åˆ°çš„è®­ç»ƒç»“æœâ€”â€”è¯å‘é‡ï¼ˆword embeddingï¼‰ï¼Œå¯ä»¥å¾ˆå¥½åœ°åº¦é‡è¯ä¸è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚éšç€æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­åº”ç”¨çš„æ™®åŠï¼Œå¾ˆå¤šäººè¯¯ä»¥ä¸ºword2vecæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ ç®—æ³•ã€‚å…¶å®word2vecç®—æ³•çš„èƒŒåæ˜¯ä¸€ä¸ªæµ…å±‚ç¥ç»ç½‘ç»œã€‚å¦å¤–éœ€è¦å¼ºè°ƒçš„ä¸€ç‚¹æ˜¯ï¼Œword2vecæ˜¯ä¸€ä¸ªè®¡ç®—word vectorçš„å¼€æºå·¥å…·ã€‚å½“æˆ‘ä»¬åœ¨è¯´word2vecç®—æ³•æˆ–æ¨¡å‹çš„æ—¶å€™ï¼Œå…¶å®æŒ‡çš„æ˜¯å…¶èƒŒåç”¨äºè®¡ç®—word vectorçš„ CBoW æ¨¡å‹å’Œ Skip-gram æ¨¡å‹ã€‚å¾ˆå¤šäººä»¥ä¸ºword2vecæŒ‡çš„æ˜¯ä¸€ä¸ªç®—æ³•æˆ–æ¨¡å‹ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§è°¬è¯¯ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬æ–‡å°†ä»ç»Ÿè®¡è¯­è¨€æ¨¡å‹å‡ºå‘ï¼Œå°½å¯èƒ½è¯¦ç»†åœ°ä»‹ç»word2vecå·¥å…·èƒŒåçš„ç®—æ³•æ¨¡å‹çš„æ¥é¾™å»è„‰ã€‚

## Statistical Language Model

åœ¨æ·±å…¥word2vecç®—æ³•çš„ç»†èŠ‚ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆå›é¡¾ä¸€ä¸‹è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼š**å¦‚ä½•è®¡ç®—ä¸€æ®µæ–‡æœ¬åºåˆ—åœ¨æŸç§è¯­è¨€ä¸‹å‡ºç°çš„æ¦‚ç‡ï¼Ÿä¹‹æ‰€ä¸ºç§°å…¶ä¸ºä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œæ˜¯å› ä¸ºå®ƒåœ¨å¾ˆå¤šNLPä»»åŠ¡ä¸­éƒ½æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚** ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘çš„é—®é¢˜ä¸­ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“äº†ç›®æ ‡è¯­è¨€ä¸­æ¯å¥è¯çš„æ¦‚ç‡ï¼Œå°±å¯ä»¥ä»å€™é€‰é›†åˆä¸­æŒ‘é€‰å‡ºæœ€åˆç†çš„å¥å­åšä¸ºç¿»è¯‘ç»“æœè¿”å›ã€‚

ç»Ÿè®¡è¯­è¨€æ¨¡å‹ç»™å‡ºäº†è¿™ä¸€ç±»é—®é¢˜çš„ä¸€ä¸ªåŸºæœ¬è§£å†³æ¡†æ¶ã€‚å¯¹äºä¸€æ®µæ–‡æœ¬åºåˆ—
$$
S=w_1, w_2, ... , w_T
$$
å®ƒçš„æ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$
P(S)=P(w_1, w_2, ..., w_T)=\prod_{t=1}^Tp(w_t|w_1, w_2, ..., w_{t-1})
$$
å³å°†åºåˆ—çš„è”åˆæ¦‚ç‡è½¬åŒ–ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ã€‚é—®é¢˜å˜æˆäº†å¦‚ä½•å»é¢„æµ‹è¿™äº›ç»™å®šprevious wordsä¸‹çš„æ¡ä»¶æ¦‚ç‡ï¼š
$$
p(w_t|w_1,w_2,...,w_{t-1})
$$
ç”±äºå…¶å·¨å¤§çš„å‚æ•°ç©ºé—´ï¼Œè¿™æ ·ä¸€ä¸ªåŸå§‹çš„æ¨¡å‹åœ¨å®é™…ä¸­å¹¶æ²¡æœ‰ä»€ä¹ˆç”¨ã€‚æˆ‘ä»¬æ›´å¤šçš„æ˜¯é‡‡ç”¨å…¶ç®€åŒ–ç‰ˆæœ¬â€”â€”Ngramæ¨¡å‹ï¼š
$$
p(w_t|w_1, w_2, ..., w_{t-1}) \approx p(w_t|w_{t-n+1}, ..., w_{t-1})
$$
å¸¸è§çš„å¦‚bigramæ¨¡å‹ï¼ˆN=2ï¼‰å’Œtrigramæ¨¡å‹ï¼ˆN=3ï¼‰ã€‚äº‹å®ä¸Šï¼Œç”±äºæ¨¡å‹å¤æ‚åº¦å’Œé¢„æµ‹ç²¾åº¦çš„é™åˆ¶ï¼Œæˆ‘ä»¬å¾ˆå°‘ä¼šè€ƒè™‘N>3çš„æ¨¡å‹ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨æœ€å¤§ä¼¼ç„¶æ³•å»æ±‚è§£Ngramæ¨¡å‹çš„å‚æ•°â€”â€”ç­‰ä»·äºå»ç»Ÿè®¡æ¯ä¸ªNgramçš„æ¡ä»¶è¯é¢‘ã€‚

ä¸ºäº†é¿å…ç»Ÿè®¡ä¸­å‡ºç°çš„é›¶æ¦‚ç‡é—®é¢˜ï¼ˆä¸€æ®µä»æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°è¿‡çš„Ngramç‰‡æ®µä¼šä½¿å¾—æ•´ä¸ªåºåˆ—çš„æ¦‚ç‡ä¸º0ï¼‰ï¼Œäººä»¬åŸºäºåŸå§‹çš„Ngramæ¨¡å‹è¿›ä¸€æ­¥å‘å±•å‡ºäº†back-off trigramæ¨¡å‹ï¼ˆç”¨ä½é˜¶çš„bigramå’Œunigramä»£æ›¿é›¶æ¦‚ç‡çš„trigramï¼‰å’Œinterpolated trigramæ¨¡å‹ï¼ˆå°†æ¡ä»¶æ¦‚ç‡è¡¨ç¤ºä¸ºunigramã€bigramã€trigramä¸‰è€…çš„çº¿æ€§å‡½æ•°ï¼‰ã€‚æ­¤å¤„ä¸å†èµ˜è¿°ã€‚æ„Ÿå…´è¶£è€…å¯è¿›ä¸€æ­¥é˜…è¯»ç›¸å…³çš„æ–‡çŒ®[3]ã€‚

## Distributed Representation

ä¸è¿‡ï¼ŒNgramæ¨¡å‹ä»æœ‰å…¶å±€é™æ€§ã€‚é¦–å…ˆï¼Œç”±äºå‚æ•°ç©ºé—´çš„çˆ†ç‚¸å¼å¢é•¿ï¼Œå®ƒæ— æ³•å¤„ç†æ›´é•¿ç¨‹çš„contextï¼ˆN>3ï¼‰ã€‚å…¶æ¬¡ï¼Œå®ƒæ²¡æœ‰è€ƒè™‘è¯ä¸è¯ä¹‹é—´å†…åœ¨çš„è”ç³»æ€§ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘"the cat is walking in the bedroom"è¿™å¥è¯ã€‚å¦‚æœæˆ‘ä»¬åœ¨è®­ç»ƒè¯­æ–™ä¸­çœ‹åˆ°äº†å¾ˆå¤šç±»ä¼¼â€œthe dog is walking in the bedroomâ€æˆ–æ˜¯â€œthe cat is running in the bedroomâ€è¿™æ ·çš„å¥å­ï¼Œé‚£ä¹ˆï¼Œå³ä½¿æˆ‘ä»¬æ²¡æœ‰è§è¿‡è¿™å¥è¯ï¼Œä¹Ÿå¯ä»¥ä»â€œcatâ€å’Œâ€œdogâ€ï¼ˆâ€œwalkingâ€å’Œâ€œrunningâ€ï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ¨æµ‹å‡ºè¿™å¥è¯çš„æ¦‚ç‡[3]ã€‚ç„¶è€Œï¼Œ Ngramæ¨¡å‹åšä¸åˆ°ã€‚

è¿™æ˜¯å› ä¸ºï¼ŒNgramæœ¬è´¨ä¸Šæ˜¯å°†è¯å½“åšä¸€ä¸ªä¸ªå­¤ç«‹çš„åŸå­å•å…ƒï¼ˆatomic unitï¼‰å»å¤„ç†çš„ã€‚è¿™ç§å¤„ç†æ–¹å¼å¯¹åº”åˆ°æ•°å­¦ä¸Šçš„å½¢å¼æ˜¯ä¸€ä¸ªä¸ªç¦»æ•£çš„one-hotå‘é‡ï¼ˆé™¤äº†ä¸€ä¸ªè¯å…¸ç´¢å¼•çš„ä¸‹æ ‡å¯¹åº”çš„æ–¹å‘ä¸Šæ˜¯1 ï¼Œå…¶ä½™æ–¹å‘ä¸Šéƒ½æ˜¯0ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªå¤§å°ä¸º5çš„è¯å…¸ï¼š{"I", "love", "nature", "luaguage", "processing"}ï¼Œâ€œnatureâ€å¯¹åº”çš„one-hotå‘é‡ä¸ºï¼š[0,0,1,0,0] ã€‚æ˜¾ç„¶ï¼Œone-hotå‘é‡çš„ç»´åº¦ç­‰äºè¯å…¸çš„å¤§å°ã€‚è¿™åœ¨åŠ¨è¾„ä¸Šä¸‡ç”šè‡³ç™¾ä¸‡è¯å…¸çš„å®é™…åº”ç”¨ä¸­ï¼Œé¢ä¸´ç€å·¨å¤§çš„ç»´åº¦ç¾éš¾é—®é¢˜ï¼ˆThe Curse of Dimensionalityï¼‰

äºæ˜¯ï¼Œäººä»¬å°±è‡ªç„¶è€Œç„¶åœ°æƒ³åˆ°ï¼Œèƒ½å¦ç”¨ä¸€ä¸ª**è¿ç»­çš„ç¨ å¯†å‘é‡å»åˆ»ç”»ä¸€ä¸ªwordçš„ç‰¹å¾**å‘¢ï¼Ÿè¿™æ ·ï¼Œæˆ‘ä»¬ä¸ä»…å¯ä»¥ç›´æ¥åˆ»ç”»è¯ä¸è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œè¿˜å¯ä»¥å»ºç«‹ä¸€ä¸ªä»å‘é‡åˆ°æ¦‚ç‡çš„å¹³æ»‘å‡½æ•°æ¨¡å‹ï¼Œä½¿å¾—ç›¸ä¼¼çš„è¯å‘é‡å¯ä»¥æ˜ å°„åˆ°ç›¸è¿‘çš„æ¦‚ç‡ç©ºé—´ä¸Šã€‚è¿™ä¸ªç¨ å¯†è¿ç»­å‘é‡ä¹Ÿè¢«ç§°ä¸ºwordçš„ **distributed representation**[3]ã€‚

äº‹å®ä¸Šï¼Œè¿™ä¸ªæ¦‚å¿µåœ¨**ä¿¡æ¯æ£€ç´¢ï¼ˆInformation Retrievalï¼‰**é¢†åŸŸæ—©å°±å·²ç»è¢«å¹¿æ³›åœ°ä½¿ç”¨äº†ã€‚åªä¸è¿‡ï¼Œåœ¨IRé¢†åŸŸé‡Œï¼Œè¿™ä¸ªæ¦‚å¿µè¢«ç§°ä¸º**å‘é‡ç©ºé—´æ¨¡å‹ï¼ˆVector Space Modelï¼Œä»¥ä¸‹ç®€ç§°VSMï¼‰**ã€‚

VSMæ˜¯åŸºäºä¸€ç§Statistical Semantics Hypothesis[4]ï¼š**è¯­è¨€çš„ç»Ÿè®¡ç‰¹å¾éšè—ç€è¯­ä¹‰çš„ä¿¡æ¯**ï¼ˆStatistical pattern of human word usage can be used to figure out what people meanï¼‰ã€‚ä¾‹å¦‚ï¼Œä¸¤ç¯‡å…·æœ‰ç›¸ä¼¼è¯åˆ†å¸ƒçš„æ–‡æ¡£å¯ä»¥è¢«è®¤ä¸ºæ˜¯æœ‰ç€ç›¸è¿‘çš„ä¸»é¢˜ã€‚è¿™ä¸ªHypothesisæœ‰å¾ˆå¤šè¡ç”Ÿç‰ˆæœ¬ã€‚å…¶ä¸­ï¼Œæ¯”è¾ƒå¹¿ä¸ºäººçŸ¥çš„ä¸¤ä¸ªç‰ˆæœ¬æ˜¯**Bag of Words Hypothesis**å’Œ**Distributional Hypothesis**ã€‚å‰è€…æ˜¯è¯´ï¼Œ**ä¸€ç¯‡æ–‡æ¡£çš„è¯é¢‘ï¼ˆè€Œä¸æ˜¯è¯åºï¼‰ä»£è¡¨äº†æ–‡æ¡£çš„ä¸»é¢˜**ï¼›åè€…æ˜¯è¯´ï¼Œ**ä¸Šä¸‹æ–‡ç¯å¢ƒç›¸ä¼¼çš„ä¸¤ä¸ªè¯æœ‰ç€ç›¸è¿‘çš„è¯­ä¹‰**ã€‚åé¢æˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œ**word2vecç®—æ³•ä¹Ÿæ˜¯åŸºäºDistributionalçš„å‡è®¾ã€‚**

é‚£ä¹ˆï¼ŒVSMæ˜¯å¦‚ä½•å°†ç¨€ç–ç¦»æ•£çš„ one-hot è¯å‘é‡æ˜ å°„ä¸ºç¨ å¯†è¿ç»­çš„Distributional Representationçš„å‘¢ï¼Ÿ

ç®€å•æ¥è¯´ï¼Œ**åŸºäºBag of Words Hypothesis**ï¼Œæˆ‘ä»¬å¯ä»¥æ„é€ ä¸€ä¸ª**term-documentçŸ©é˜µ**$ğ´$ï¼šçŸ©é˜µçš„è¡Œ$A_{i,:}$ $:$ å¯¹åº”ç€è¯å…¸é‡Œçš„ä¸€ä¸ªwordï¼›çŸ©é˜µçš„åˆ—$A_{:,j}$å¯¹åº”ç€è®­ç»ƒè¯­æ–™é‡Œçš„ä¸€ç¯‡æ–‡æ¡£ï¼›çŸ©é˜µé‡Œçš„å…ƒç´ $A_{i,j}$ä»£è¡¨ç€word$w_i$åœ¨æ–‡æ¡£$D_j$ä¸­**å‡ºç°çš„æ¬¡æ•°ï¼ˆæˆ–é¢‘ç‡ï¼‰**ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å°±å¯ä»¥æå–è¡Œå‘é‡åšä¸ºwordçš„è¯­ä¹‰å‘é‡ï¼ˆä¸è¿‡ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬æ›´å¤šçš„æ˜¯ç”¨åˆ—å‘é‡åšä¸ºæ–‡æ¡£çš„ä¸»é¢˜å‘é‡ï¼‰ã€‚

ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäº**Distributional Hypothesis**æ„é€ ä¸€ä¸ª**word-contextçš„çŸ©é˜µ**ã€‚æ­¤æ—¶ï¼Œ**çŸ©é˜µçš„åˆ—å˜æˆäº†contexté‡Œçš„wordï¼ŒçŸ©é˜µçš„å…ƒç´ ä¹Ÿå˜æˆäº†ä¸€ä¸ªcontextçª—å£é‡Œwordçš„å…±ç°æ¬¡æ•°ã€‚**

æ³¨æ„ï¼Œè¿™ä¸¤ç±»çŸ©é˜µçš„è¡Œå‘é‡æ‰€è®¡ç®—çš„ç›¸ä¼¼åº¦æœ‰ç€ç»†å¾®çš„å·®å¼‚ï¼šterm-documentçŸ©é˜µä¼šç»™ç»å¸¸å‡ºç°åœ¨åŒä¸€ç¯‡documenté‡Œçš„ä¸¤ä¸ªwordèµ‹äºˆæ›´é«˜çš„ç›¸ä¼¼åº¦ï¼›è€Œword-contextçŸ©é˜µä¼šç»™é‚£äº›æœ‰ç€ç›¸åŒcontextçš„ä¸¤ä¸ªwordèµ‹äºˆæ›´é«˜çš„ç›¸ä¼¼åº¦ã€‚åè€…ç›¸å¯¹äºå‰è€…æ˜¯ä¸€ç§æ›´é«˜é˜¶çš„ç›¸ä¼¼åº¦ï¼Œå› æ­¤åœ¨ä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢é¢†åŸŸä¸­å¾—åˆ°äº†æ›´åŠ å¹¿æ³›çš„åº”ç”¨ã€‚

ä¸è¿‡ï¼Œè¿™ç§co-occurrenceçŸ©é˜µä»ç„¶å­˜åœ¨ç€æ•°æ®ç¨€ç–æ€§å’Œç»´åº¦ç¾éš¾çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œäººä»¬æå‡ºäº†ä¸€ç³»åˆ—å¯¹çŸ©é˜µè¿›è¡Œé™ç»´çš„æ–¹æ³•ï¼ˆå¦‚LSIï¼LSAç­‰ï¼‰ã€‚è¿™äº›æ–¹æ³•å¤§éƒ½æ˜¯åŸºäºSVDçš„æ€æƒ³ï¼Œå°†åŸå§‹çš„ç¨€ç–çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µä¹˜ç§¯çš„å½¢å¼ã€‚

å…³äºVSMæ›´å¤šçš„ä»‹ç»ï¼Œå¯ä»¥è¿›ä¸€æ­¥é˜…è¯»æ–‡æœ«çš„å‚è€ƒæ–‡çŒ®[4]ã€‚

## Neural Network Language Model

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å›åˆ°å¯¹ç»Ÿè®¡è¯­è¨€æ¨¡å‹çš„è®¨è®ºã€‚é‰´äºNgramç­‰æ¨¡å‹çš„ä¸è¶³ï¼Œ2003å¹´ï¼ŒBengioç­‰äººå‘è¡¨äº†ä¸€ç¯‡å¼€åˆ›æ€§çš„æ–‡ç« ï¼šA neural probabilistic language model[3]ã€‚åœ¨è¿™ç¯‡æ–‡ç« é‡Œï¼Œä»–ä»¬æ€»ç»“å‡ºäº†ä¸€å¥—ç”¨ç¥ç»ç½‘ç»œå»ºç«‹ç»Ÿè®¡è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼ˆNeural Network Language Modelï¼Œä»¥ä¸‹ç®€ç§°NNLMï¼‰ï¼Œå¹¶é¦–æ¬¡æå‡ºäº†**word embedding**çš„æ¦‚å¿µï¼ˆè™½ç„¶æ²¡æœ‰å«è¿™ä¸ªåå­—ï¼‰ï¼Œä»è€Œå¥ å®šäº†åŒ…æ‹¬word2vecåœ¨å†…åç»­ç ”ç©¶word representation learningçš„åŸºç¡€ã€‚

NNLMæ¨¡å‹çš„åŸºæœ¬æ€æƒ³å¯ä»¥æ¦‚æ‹¬å¦‚ä¸‹ï¼š

1. å‡å®šè¯è¡¨ä¸­çš„æ¯ä¸€ä¸ªwordéƒ½å¯¹åº”ç€ä¸€ä¸ªè¿ç»­çš„ç‰¹å¾å‘é‡ï¼›
2. å‡å®šä¸€ä¸ªè¿ç»­å¹³æ»‘çš„æ¦‚ç‡æ¨¡å‹ï¼Œè¾“å…¥ä¸€æ®µè¯å‘é‡çš„åºåˆ—ï¼Œå¯ä»¥è¾“å‡ºè¿™æ®µåºåˆ—çš„è”åˆæ¦‚ç‡ï¼›
3. **åŒæ—¶å­¦ä¹ **è¯å‘é‡çš„æƒé‡å’Œæ¦‚ç‡æ¨¡å‹é‡Œçš„å‚æ•°ã€‚

å€¼å¾—æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œè¿™é‡Œçš„**è¯å‘é‡ä¹Ÿæ˜¯è¦å­¦ä¹ çš„å‚æ•°ã€‚**

åœ¨03å¹´çš„è®ºæ–‡é‡Œï¼ŒBengioç­‰äººé‡‡ç”¨äº†ä¸€ä¸ªç®€å•çš„å‰å‘åé¦ˆç¥ç»ç½‘ç»œ $f(w_{t-n+1},...,w_{t})$ æ¥æ‹Ÿåˆä¸€ä¸ªè¯åºåˆ—çš„æ¡ä»¶æ¦‚ç‡$p(w_t|w_1,w_2,...,w_{t-1})$ã€‚

æ•´ä¸ªæ¨¡å‹çš„ç½‘ç»œç»“æ„è§ä¸‹å›¾ï¼š

![](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/guoyaoyang/NNLM.png)

æˆ‘ä»¬å¯ä»¥å°†æ•´ä¸ªæ¨¡å‹æ‹†åˆ†æˆä¸¤éƒ¨åˆ†åŠ ä»¥ç†è§£ï¼š

1. é¦–å…ˆæ˜¯ä¸€ä¸ªçº¿æ€§çš„Embeddingå±‚ã€‚å®ƒå°†è¾“å…¥çš„Nâˆ’1ä¸ªone-hotè¯å‘é‡ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„DÃ—Vçš„çŸ©é˜µCï¼Œæ˜ å°„ä¸ºNâˆ’1ä¸ªåˆ†å¸ƒå¼çš„è¯å‘é‡ï¼ˆdistributed vectorï¼‰ã€‚å…¶ä¸­ï¼ŒVæ˜¯è¯å…¸çš„å¤§å°ï¼ŒDæ˜¯Embeddingå‘é‡çš„ç»´åº¦ï¼ˆä¸€ä¸ªå…ˆéªŒå‚æ•°ï¼‰ã€‚**CçŸ©é˜µé‡Œå­˜å‚¨äº†è¦å­¦ä¹ çš„word vectorã€‚**

2. å…¶æ¬¡æ˜¯ä¸€ä¸ªç®€å•çš„å‰å‘åé¦ˆç¥ç»ç½‘ç»œgã€‚å®ƒç”±ä¸€ä¸ªtanhéšå±‚å’Œä¸€ä¸ªsoftmaxè¾“å‡ºå±‚ç»„æˆã€‚é€šè¿‡å°†Embeddingå±‚è¾“å‡ºçš„Nâˆ’1ä¸ªè¯å‘é‡æ˜ å°„ä¸ºä¸€ä¸ªé•¿åº¦ä¸ºVçš„æ¦‚ç‡åˆ†å¸ƒå‘é‡ï¼Œä»è€Œå¯¹è¯å…¸ä¸­çš„wordåœ¨è¾“å…¥contextä¸‹çš„æ¡ä»¶æ¦‚ç‡åšå‡ºé¢„ä¼°ï¼š
   $$
   \begin{align}
   p(w_i|w_1,w_2,...,w_{t-1}) \approx \\ 
   f(w_i, w_{t-1}, ..., w_{t-n+1}) = \\
   g(w_i, C(w_{t-n+1}), ..., C(w_{t-1}))
   \end{align}
   $$

æˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€å°åŒ–ä¸€ä¸ªcross-entropyçš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°æ¥è°ƒæ•´æ¨¡å‹çš„å‚æ•°$Î¸$ï¼š
$$
L(\theta)=\frac{1}{T}\sum_t{\log{f(w_t, w_{t-1}, ..., w_{t-n+1})}}+R(\theta)
$$
å…¶ä¸­ï¼Œæ¨¡å‹çš„å‚æ•° $Î¸$ åŒ…æ‹¬äº†Embeddingå±‚çŸ©é˜µCçš„å…ƒç´ ï¼Œå’Œå‰å‘åé¦ˆç¥ç»ç½‘ç»œæ¨¡å‹gé‡Œçš„æƒé‡ã€‚è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„å‚æ•°ç©ºé—´ã€‚ä¸è¿‡ï¼Œåœ¨ç”¨SGDå­¦ä¹ æ›´æ–°æ¨¡å‹çš„å‚æ•°æ—¶ï¼Œ**å¹¶ä¸æ˜¯æ‰€æœ‰çš„å‚æ•°éƒ½éœ€è¦è°ƒæ•´ï¼ˆä¾‹å¦‚æœªåœ¨è¾“å…¥çš„contextä¸­å‡ºç°çš„è¯å¯¹åº”çš„è¯å‘é‡ï¼‰**ã€‚**è®¡ç®—çš„ç“¶é¢ˆä¸»è¦æ˜¯åœ¨softmaxå±‚çš„å½’ä¸€åŒ–å‡½æ•°ä¸Š**ï¼ˆéœ€è¦å¯¹è¯å…¸ä¸­æ‰€æœ‰çš„wordè®¡ç®—ä¸€éæ¡ä»¶æ¦‚ç‡ï¼‰ã€‚

ç„¶è€Œï¼ŒæŠ›å´å¤æ‚çš„å‚æ•°ç©ºé—´ï¼Œæˆ‘ä»¬ä¸ç¦è¦é—®ï¼Œä¸ºä»€ä¹ˆè¿™æ ·ä¸€ä¸ªç®€å•çš„æ¨¡å‹ä¼šå–å¾—å·¨å¤§çš„æˆåŠŸå‘¢ï¼Ÿ

#### >>> Important

ä»”ç»†è§‚å¯Ÿè¿™ä¸ªæ¨¡å‹å°±ä¼šå‘ç°ï¼Œå®ƒå…¶å®åœ¨åŒæ—¶è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š**ä¸€ä¸ªæ˜¯ç»Ÿè®¡è¯­è¨€æ¨¡å‹é‡Œå…³æ³¨çš„æ¡ä»¶æ¦‚ç‡$p(w_t|context)$çš„è®¡ç®—ï¼›ä¸€ä¸ªæ˜¯å‘é‡ç©ºé—´æ¨¡å‹é‡Œå…³æ³¨çš„è¯å‘é‡çš„è¡¨è¾¾**ã€‚è€Œè¿™ä¸¤ä¸ªé—®é¢˜æœ¬è´¨ä¸Šå¹¶ä¸ç‹¬ç«‹ã€‚é€šè¿‡å¼•å…¥è¿ç»­çš„è¯å‘é‡å’Œå¹³æ»‘çš„æ¦‚ç‡æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨ä¸€ä¸ªè¿ç»­ç©ºé—´é‡Œå¯¹åºåˆ—æ¦‚ç‡è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œä»æ ¹æœ¬ä¸Š**ç¼“è§£æ•°æ®ç¨€ç–æ€§å’Œç»´åº¦ç¾éš¾çš„é—®é¢˜**ã€‚å¦ä¸€æ–¹é¢ï¼Œä»¥æ¡ä»¶æ¦‚ç‡ $p(w_t|context)$ ä¸ºå­¦ä¹ ç›®æ ‡å»æ›´æ–°è¯å‘é‡çš„æƒé‡ï¼Œå…·æœ‰æ›´å¼ºçš„å¯¼å‘æ€§ï¼ŒåŒæ—¶ä¹Ÿä¸VSMé‡Œçš„Distributional Hypothesisä¸è°‹è€Œåˆã€‚

#### <<< Important

## CBoW & Skip-gram Model

é“ºå«äº†è¿™ä¹ˆå¤šï¼Œç»ˆäºè¦è½®åˆ°ä¸»è§’å‡ºåœºäº†ã€‚

ä¸è¿‡åœ¨ä¸»è§’æ­£å¼ç™»åœºå‰ï¼Œæˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹NNLMå­˜åœ¨çš„å‡ ä¸ªé—®é¢˜ã€‚

ä¸€ä¸ªé—®é¢˜æ˜¯ï¼ŒåŒNgramæ¨¡å‹ä¸€æ ·ï¼ŒNNLMæ¨¡å‹åªèƒ½å¤„ç†å®šé•¿çš„åºåˆ—ã€‚åœ¨03å¹´çš„è®ºæ–‡é‡Œï¼ŒBengioç­‰äººå°†æ¨¡å‹èƒ½å¤Ÿä¸€æ¬¡å¤„ç†çš„åºåˆ—é•¿åº¦Næé«˜åˆ°äº†5ï¼Œè™½ç„¶ç›¸æ¯”bigramå’Œtrigramå·²ç»æ˜¯å¾ˆå¤§çš„æå‡ï¼Œä½†ä¾ç„¶ç¼ºå°‘çµæ´»æ€§ã€‚

å› æ­¤ï¼ŒMikolovç­‰äººåœ¨2010å¹´æå‡ºäº†ä¸€ç§RNNLMæ¨¡å‹[7]ï¼Œç”¨é€’å½’ç¥ç»ç½‘ç»œä»£æ›¿åŸå§‹æ¨¡å‹é‡Œçš„å‰å‘åé¦ˆç¥ç»ç½‘ç»œï¼Œå¹¶å°†Embeddingå±‚ä¸RNNé‡Œçš„éšè—å±‚åˆå¹¶ï¼Œä»è€Œè§£å†³äº†å˜é•¿åºåˆ—çš„é—®é¢˜ã€‚

å¦ä¸€ä¸ªé—®é¢˜å°±æ¯”è¾ƒä¸¥é‡äº†ã€‚NNLMçš„è®­ç»ƒå¤ªæ…¢äº†ã€‚å³ä¾¿æ˜¯åœ¨ç™¾ä¸‡é‡çº§çš„æ•°æ®é›†ä¸Šï¼Œå³ä¾¿æ˜¯å€ŸåŠ©äº†40ä¸ªCPUè¿›è¡Œè®­ç»ƒï¼ŒNNLMä¹Ÿéœ€è¦è€—æ—¶æ•°å‘¨æ‰èƒ½ç»™å‡ºä¸€ä¸ªç¨å¾®é è°±çš„è§£æ¥ã€‚æ˜¾ç„¶ï¼Œå¯¹äºç°åœ¨åŠ¨è¾„ä¸Šåƒä¸‡ç”šè‡³ä¸Šäº¿çš„çœŸå®è¯­æ–™åº“ï¼Œè®­ç»ƒä¸€ä¸ªNNLMæ¨¡å‹å‡ ä¹æ˜¯ä¸€ä¸ªimpossible missionã€‚

è¿™æ—¶å€™ï¼Œè¿˜æ˜¯é‚£ä¸ªMikolovç«™äº†å‡ºæ¥ã€‚ä»–æ³¨æ„åˆ°ï¼ŒåŸå§‹çš„NNLMæ¨¡å‹çš„è®­ç»ƒå…¶å®å¯ä»¥æ‹†åˆ†æˆ**ä¸¤ä¸ªæ­¥éª¤**ï¼š

1. ç”¨ä¸€ä¸ªç®€å•æ¨¡å‹è®­ç»ƒå‡ºè¿ç»­çš„è¯å‘é‡ï¼›
2. åŸºäºè¯å‘é‡çš„è¡¨è¾¾ï¼Œè®­ç»ƒä¸€ä¸ªè¿ç»­çš„Ngramç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è€ŒNNLMæ¨¡å‹çš„è®¡ç®—ç“¶é¢ˆä¸»è¦æ˜¯åœ¨ç¬¬äºŒæ­¥ã€‚

å¦‚æœæˆ‘ä»¬åªæ˜¯æƒ³å¾—åˆ°wordçš„è¿ç»­ç‰¹å¾å‘é‡ï¼Œæ˜¯ä¸æ˜¯å¯ä»¥å¯¹ç¬¬äºŒæ­¥é‡Œçš„ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œç®€åŒ–å‘¢ï¼Ÿ

Mikolovæ˜¯è¿™ä¹ˆæƒ³çš„ï¼Œä¹Ÿæ˜¯è¿™ä¹ˆåšçš„ã€‚ä»–åœ¨2013å¹´ä¸€å£æ°”æ¨å‡ºäº†ä¸¤ç¯‡paperï¼Œå¹¶å¼€æºäº†ä¸€æ¬¾è®¡ç®—è¯å‘é‡çš„å·¥å…·â€”â€”è‡³æ­¤ï¼Œword2vecæ¨ªç©ºå‡ºä¸–ï¼Œä¸»è§’é—ªäº®ç™»åœºã€‚

ä¸‹é¢ï¼Œæˆ‘å°†å¸¦é¢†å¤§å®¶ç®€å•å‰–æä¸‹word2vecç®—æ³•çš„åŸç†ã€‚æœ‰äº†å‰æ–‡çš„åŸºç¡€ï¼Œç†è§£word2vecç®—æ³•å°±å˜å¾—å¾ˆç®€å•äº†ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹åŸå§‹çš„NNLMæ¨¡å‹åšå¦‚ä¸‹æ”¹é€ ï¼š

1. ç§»é™¤å‰å‘åé¦ˆç¥ç»ç½‘ç»œä¸­éçº¿æ€§çš„hidden layerï¼Œç›´æ¥å°†ä¸­é—´å±‚çš„Embedding layerä¸è¾“å‡ºå±‚çš„softmax layerè¿æ¥ï¼›
2. å¿½ç•¥ä¸Šä¸‹æ–‡ç¯å¢ƒçš„åºåˆ—ä¿¡æ¯ï¼šè¾“å…¥çš„æ‰€æœ‰è¯å‘é‡å‡æ±‡æ€»åˆ°åŒä¸€ä¸ªEmbedding layerï¼›
3. å°†Future wordsçº³å…¥ä¸Šä¸‹æ–‡ç¯å¢ƒ

å¾—åˆ°çš„æ¨¡å‹ç§°ä¹‹ä¸º**CBoWæ¨¡å‹ï¼ˆContinuous Bag-of-Words Modelï¼‰**ï¼Œä¹Ÿæ˜¯**word2vecç®—æ³•çš„ç¬¬ä¸€ä¸ªæ¨¡å‹**ï¼š

![](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/guoyaoyang/CBow.png)

ä»æ•°å­¦ä¸Šçœ‹ï¼ŒCBoWæ¨¡å‹ç­‰ä»·äºä¸€ä¸ªè¯è¢‹æ¨¡å‹çš„å‘é‡ä¹˜ä»¥ä¸€ä¸ªEmbeddingçŸ©é˜µï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªè¿ç»­çš„embeddingå‘é‡ã€‚è¿™ä¹Ÿæ˜¯CBoWæ¨¡å‹åç§°çš„ç”±æ¥ã€‚

**CBoWæ¨¡å‹ä¾ç„¶æ˜¯ä»contextå¯¹target wordçš„é¢„æµ‹ä¸­å­¦ä¹ åˆ°è¯å‘é‡çš„è¡¨è¾¾**ã€‚åè¿‡æ¥ï¼Œæˆ‘ä»¬èƒ½å¦**ä»target wordå¯¹contextçš„é¢„æµ‹ä¸­å­¦ä¹ åˆ°word vectorå‘¢**ï¼Ÿç­”æ¡ˆæ˜¾ç„¶æ˜¯å¯ä»¥çš„ï¼š

![](/Users/helloword/Anmingyu/Gor-rok/Daily/Word2Vec/guoyaoyang/Skip-gram.png)

è¿™ä¸ªæ¨¡å‹è¢«ç§°ä¸º**Skip-gramæ¨¡å‹**ï¼ˆåç§°æºäºè¯¥æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šå¯¹ä¸Šä¸‹æ–‡ç¯å¢ƒé‡Œçš„wordè¿›è¡Œé‡‡æ ·ï¼‰ã€‚

å¦‚æœå°†Skip-gramæ¨¡å‹çš„å‰å‘è®¡ç®—è¿‡ç¨‹å†™æˆæ•°å­¦å½¢å¼ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
$$
p(w_o|w_i)=\frac{e^{U_o \cdot V_i}}{\sum_j{e^{U_j \cdot V_i}}}
$$
å…¶ä¸­ï¼Œ$V_i$ æ˜¯Embeddingå±‚çŸ©é˜µé‡Œçš„åˆ—å‘é‡ï¼Œä¹Ÿè¢«ç§°ä¸º $w_i$ çš„ input vectorã€‚$U_j$ æ˜¯softmaxå±‚çŸ©é˜µé‡Œçš„è¡Œå‘é‡ï¼Œä¹Ÿè¢«ç§°ä¸º $w_i$ çš„output vector.
$$
p(w_o|w_i)=\frac{e^{U_o \cdot V_i}}{\sum_j{e^{U_j \cdot V_i}}}
$$
å› æ­¤ï¼ŒSkip-gramæ¨¡å‹çš„æœ¬è´¨æ˜¯ **è®¡ç®—è¾“å…¥wordçš„input vectorä¸ç›®æ ‡wordçš„output vectorä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶è¿›è¡Œsoftmaxå½’ä¸€åŒ–**ã€‚æˆ‘ä»¬è¦å­¦ä¹ çš„æ¨¡å‹å‚æ•°æ­£æ˜¯è¿™ä¸¤ç±»è¯å‘é‡ã€‚

ç„¶è€Œï¼Œç›´æ¥å¯¹è¯å…¸é‡Œçš„ $V$ ä¸ªè¯è®¡ç®—ç›¸ä¼¼åº¦å¹¶å½’ä¸€åŒ–ï¼Œæ˜¾ç„¶æ˜¯ä¸€ä»¶æå…¶è€—æ—¶çš„impossible missionã€‚ä¸ºæ­¤ï¼ŒMikolovå¼•å…¥äº†ä¸¤ç§ä¼˜åŒ–ç®—æ³•ï¼š**å±‚æ¬¡Softmaxï¼ˆHierarchical Softmaxï¼‰**å’Œ**è´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰**ã€‚

## Hierarchical Softmax[5]

å±‚æ¬¡Softmaxçš„æ–¹æ³•æœ€æ—©ç”±Bengioåœ¨05å¹´å¼•å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ã€‚å®ƒçš„**åŸºæœ¬æ€æƒ³æ˜¯å°†å¤æ‚çš„å½’ä¸€åŒ–æ¦‚ç‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡ä¹˜ç§¯çš„å½¢å¼**ï¼š
$$
p(v|context)=\prod_{i=1}^m{p(b_i(v)|b_1(v), ..., b_{i-1}(v), context)}
$$
å…¶ä¸­ï¼Œæ¯ä¸€å±‚æ¡ä»¶æ¦‚ç‡å¯¹åº”ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„é€»è¾‘å›å½’å‡½æ•°å»æ‹Ÿåˆã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°†å¯¹ $V$ ä¸ªè¯çš„æ¦‚ç‡å½’ä¸€åŒ–é—®é¢˜ï¼Œè½¬åŒ–æˆäº†å¯¹$logV$ä¸ªè¯çš„æ¦‚ç‡æ‹Ÿåˆé—®é¢˜ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ„é€ ä¸€é¢—åˆ†ç±»äºŒå‰æ ‘æ¥ç›´è§‚åœ°ç†è§£è¿™ä¸ªè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†åŸå§‹å­—å…¸Dåˆ’åˆ†ä¸ºä¸¤ä¸ªå­é›†D1ã€D2ï¼Œå¹¶å‡è®¾åœ¨ç»™å®šcontextä¸‹ï¼Œtarget wordå±äºå­é›†D1çš„æ¦‚ç‡$p(w_t \in D_1|context)$æœä»logistical functionçš„å½¢å¼ï¼š
$$
p(w_t \in D_1|context)=\frac{1}{1+e^{-U_{D_{root}} \cdot V_{w_t}}}
$$
å…¶ä¸­ï¼Œ $U_{D_{root}}$ å’Œ $V_{wt}$ éƒ½æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å­é›†D1å’ŒD2è¿›ä¸€æ­¥åˆ’åˆ†ã€‚**é‡å¤è¿™ä¸€è¿‡ç¨‹ï¼Œç›´åˆ°é›†åˆé‡Œåªå‰©ä¸‹ä¸€ä¸ªword**ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å°†åŸå§‹å¤§å°ä¸ºVçš„å­—å…¸Dè½¬æ¢æˆäº†ä¸€é¢—æ·±åº¦ä¸ºlogVçš„äºŒå‰æ ‘ã€‚**æ ‘çš„å¶å­èŠ‚ç‚¹ä¸åŸå§‹å­—å…¸é‡Œçš„wordä¸€ä¸€å¯¹åº”**ï¼›éå¶èŠ‚ç‚¹åˆ™å¯¹åº”ç€æŸä¸€ç±»wordçš„é›†åˆã€‚**æ˜¾ç„¶ï¼Œä»æ ¹èŠ‚ç‚¹å‡ºå‘åˆ°ä»»æ„ä¸€ä¸ªå¶å­èŠ‚ç‚¹éƒ½åªæœ‰ä¸€æ¡å”¯ä¸€è·¯å¾„â€”â€”è¿™æ¡è·¯å¾„ä¹Ÿç¼–ç äº†è¿™ä¸ªå¶å­èŠ‚ç‚¹æ‰€å±çš„ç±»åˆ«ã€‚**

åŒæ—¶ï¼Œä»æ ¹èŠ‚ç‚¹å‡ºå‘åˆ°å¶å­èŠ‚ç‚¹ä¹Ÿæ˜¯ä¸€ä¸ªéšæœºæ¸¸èµ°çš„è¿‡ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºè¿™é¢—äºŒå‰æ ‘å¯¹å¶å­èŠ‚ç‚¹å‡ºç°çš„ä¼¼ç„¶æ¦‚ç‡è¿›è¡Œè®¡ç®—ã€‚ä¾‹å¦‚ï¼Œå¯¹äºè®­ç»ƒæ ·æœ¬é‡Œçš„ä¸€ä¸ªtarget word $w_t$,å‡è®¾å…¶å¯¹åº”çš„äºŒå‰æ ‘ç¼–ç ä¸º$\{1, 0, 1, ..., 1\}$ï¼Œåˆ™æˆ‘ä»¬æ„é€ çš„ä¼¼ç„¶å‡½æ•°ä¸ºï¼š
$$
p(w_t|context)=p(D_1=1|context)p(D_2=0|D_1=1)\dots p(w_t|D_k=1)
$$
ä¹˜ç§¯ä¸­çš„æ¯ä¸€é¡¹éƒ½æ˜¯ä¸€ä¸ªé€»è¾‘å›å½’çš„å‡½æ•°ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€å¤§åŒ–è¿™ä¸ªä¼¼ç„¶å‡½æ•°æ¥æ±‚è§£äºŒå‰æ ‘ä¸Šçš„å‚æ•°â€”â€”éå¶èŠ‚ç‚¹ä¸Šçš„å‘é‡ï¼Œç”¨æ¥è®¡ç®—æ¸¸èµ°åˆ°æŸä¸€ä¸ªå­èŠ‚ç‚¹çš„æ¦‚ç‡ã€‚

#### >>> Important

å±‚æ¬¡Softmaxæ˜¯ä¸€ä¸ªå¾ˆå·§å¦™çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡æ„é€ ä¸€é¢—äºŒå‰æ ‘ï¼Œå°†ç›®æ ‡æ¦‚ç‡çš„è®¡ç®—å¤æ‚åº¦**ä»æœ€åˆçš„Vé™ä½åˆ°äº†logV** çš„é‡çº§ã€‚ä¸è¿‡ä»˜å‡ºçš„**ä»£ä»·æ˜¯äººä¸ºå¢å¼ºäº†è¯ä¸è¯ä¹‹é—´çš„è€¦åˆæ€§**ã€‚ä¾‹å¦‚ï¼Œ**ä¸€ä¸ªwordå‡ºç°çš„æ¡ä»¶æ¦‚ç‡çš„å˜åŒ–ï¼Œä¼šå½±å“åˆ°å…¶è·¯å¾„ä¸Šæ‰€æœ‰éå¶èŠ‚ç‚¹çš„æ¦‚ç‡å˜åŒ–ï¼Œé—´æ¥åœ°å¯¹å…¶ä»–wordå‡ºç°çš„æ¡ä»¶æ¦‚ç‡å¸¦æ¥ä¸åŒç¨‹åº¦çš„å½±å“ã€‚** å› æ­¤ï¼Œæ„é€ ä¸€é¢—æœ‰æ„ä¹‰çš„äºŒå‰æ ‘å°±æ˜¾å¾—ååˆ†é‡è¦ã€‚å®è·µè¯æ˜ï¼Œåœ¨å®é™…çš„åº”ç”¨ä¸­ï¼Œ**åŸºäºHuffmanç¼–ç çš„äºŒå‰æ ‘å¯ä»¥æ»¡è¶³å¤§éƒ¨åˆ†åº”ç”¨åœºæ™¯çš„éœ€æ±‚ã€‚**

#### <<< Important

## Negative Sampling[6]

**è´Ÿé‡‡æ ·**çš„æ€æƒ³æœ€åˆæ¥æºäºä¸€ç§å«åš**Noise-Contrastive Estimationçš„ç®—æ³•**[6]ï¼ŒåŸæœ¬æ˜¯ä¸ºäº†è§£å†³é‚£äº›æ— æ³•å½’ä¸€åŒ–çš„æ¦‚ç‡æ¨¡å‹çš„å‚æ•°é¢„ä¼°é—®é¢˜ã€‚ä¸æ”¹é€ æ¨¡å‹è¾“å‡ºæ¦‚ç‡çš„Hierarchical Softmaxç®—æ³•ä¸åŒï¼Œ**NCEç®—æ³•æ”¹é€ çš„æ˜¯æ¨¡å‹çš„ä¼¼ç„¶å‡½æ•°ã€‚**

ä»¥Skip-gramæ¨¡å‹ä¸ºä¾‹ï¼Œå…¶åŸå§‹çš„ä¼¼ç„¶å‡½æ•°å¯¹åº”ç€ä¸€ä¸ªMultinomialçš„åˆ†å¸ƒã€‚åœ¨ç”¨æœ€å¤§ä¼¼ç„¶æ³•æ±‚è§£è¿™ä¸ªä¼¼ç„¶å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªcross-entropyçš„æŸå¤±å‡½æ•°ï¼š
$$
J(\theta)=-\frac{1}{T}\sum_{t=1}^T{\sum_{-c \leq j \leq c, j \neq 0}{\log p(w_{t+j}|w_t)}}
$$
å¼ä¸­çš„$p(w_{t+j}|w_t)$æ˜¯ä¸€ä¸ªåœ¨æ•´ä¸ªå­—å…¸ä¸Šå½’ä¸€åŒ–äº†çš„æ¦‚ç‡

è€Œåœ¨NCEç®—æ³•ä¸­ï¼Œæˆ‘ä»¬æ„é€ äº†è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼š**å¯¹äºä¸€ç»„è®­ç»ƒæ ·æœ¬ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“ï¼Œtarget wordçš„å‡ºç°ï¼Œæ˜¯æ¥è‡ªäºcontextçš„é©±åŠ¨ï¼Œè¿˜æ˜¯ä¸€ä¸ªäº‹å…ˆå‡å®šçš„èƒŒæ™¯å™ªå£°çš„é©±åŠ¨ï¼Ÿæ˜¾ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªé€»è¾‘å›å½’çš„å‡½æ•°æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š**
$$
p(D=1|w, context)=\frac{p(w|context)}{p(w|context)+kp_n(w)}=\sigma (\log p(w|context) - \log kp_n(w))
$$
è¿™ä¸ªå¼å­ç»™å‡ºäº†ä¸€ä¸ª target word $w$ æ¥è‡ªäº context é©±åŠ¨çš„æ¦‚ç‡ã€‚å…¶ä¸­ï¼Œ$k$ æ˜¯ä¸€ä¸ªå…ˆéªŒå‚æ•°ï¼Œè¡¨æ˜å™ªå£°çš„é‡‡æ ·é¢‘ç‡ã€‚$p(w|context)$ æ˜¯ä¸€ä¸ªéå½’ä¸€åŒ–çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™é‡Œé‡‡ç”¨ softmax å½’ä¸€åŒ–å‡½æ•°ä¸­çš„åˆ†å­éƒ¨åˆ†ã€‚$p_n(w)$åˆ™æ˜¯èƒŒæ™¯å™ªå£°çš„è¯åˆ†å¸ƒã€‚**é€šå¸¸é‡‡ç”¨wordçš„unigramåˆ†å¸ƒã€‚**

é€šè¿‡å¯¹å™ªå£°åˆ†å¸ƒçš„ $k$ é‡‡æ ·ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚å…¶ä¸­ï¼Œlabelæ ‡è®°äº†æ•°æ®çš„æ¥æºï¼ˆçœŸå®æ•°æ®åˆ†å¸ƒè¿˜æ˜¯èƒŒæ™¯å™ªå£°åˆ†å¸ƒï¼Ÿï¼‰ã€‚åœ¨è¿™ä¸ªæ–°çš„æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨æœ€å¤§åŒ–ä¸Šå¼ä¸­é€»è¾‘å›å½’çš„ä¼¼ç„¶å‡½æ•°æ¥æ±‚è§£æ¨¡å‹çš„å‚æ•°ã€‚

è€ŒMikolovåœ¨2013å¹´çš„è®ºæ–‡é‡Œæå‡ºçš„**è´Ÿé‡‡æ ·ç®—æ³•ï¼Œ æ˜¯NCEçš„ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ã€‚åœ¨è¿™ä¸ªç®—æ³•é‡Œï¼ŒMikolovæŠ›å¼ƒäº†NCEä¼¼ç„¶å‡½æ•°ä¸­å¯¹å™ªå£°åˆ†å¸ƒçš„ä¾èµ–ï¼Œç›´æ¥ç”¨åŸå§‹softmaxå‡½æ•°é‡Œçš„åˆ†å­å®šä¹‰äº†é€»è¾‘å›å½’çš„å‡½æ•°ï¼Œè¿›ä¸€æ­¥ç®€åŒ–äº†è®¡ç®—ï¼š**
$$
p(D=1|w_o, w_i)=\sigma (U_o \cdot V_i)
$$
æ­¤æ—¶ï¼Œæ¨¡å‹ç›¸åº”çš„ç›®æ ‡å‡½æ•°å˜ä¸ºï¼š
$$
J(\theta) = \log \sigma(U_o \cdot V_i) + \sum_{j=1}^k{E_{w_j \sim p_n(w)}[\log \sigma(- U_j \cdot V_i)]}
$$
é™¤äº†è¿™é‡Œä»‹ç»çš„å±‚æ¬¡Softmaxå’Œè´Ÿé‡‡æ ·çš„ä¼˜åŒ–ç®—æ³•ï¼ŒMikolovåœ¨13å¹´çš„è®ºæ–‡é‡Œè¿˜ä»‹ç»äº†å¦ä¸€ä¸ªtrickï¼š**ä¸‹é‡‡æ ·ï¼ˆsubsamplingï¼‰ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯åœ¨è®­ç»ƒæ—¶ä¾æ¦‚ç‡éšæœºä¸¢å¼ƒæ‰é‚£äº›é«˜é¢‘çš„è¯ï¼š**
$$
p_{discard}(w) = 1 - \sqrt{\frac{t}{f(w)}}
$$
å…¶ä¸­ï¼Œ$t$ æ˜¯ä¸€ä¸ªå…ˆéªŒå‚æ•°ï¼Œä¸€èˆ¬å–ä¸º$10^{âˆ’5}$ã€‚$f(w)$ æ˜¯$w$åœ¨è¯­æ–™ä¸­å‡ºç°çš„é¢‘ç‡ã€‚

å®éªŒè¯æ˜ï¼Œè¿™ç§ä¸‹é‡‡æ ·æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜ä½é¢‘è¯çš„è¯å‘é‡çš„å‡†ç¡®åº¦ã€‚

## Beyond the Word Vector

ä»‹ç»å®Œword2vecæ¨¡å‹çš„ç®—æ³•å’ŒåŸç†ï¼Œæˆ‘ä»¬æ¥è®¨è®ºä¸€äº›è½»æ¾ç‚¹çš„è¯é¢˜â€”â€”æ¨¡å‹çš„åº”ç”¨ã€‚

13å¹´word2vecæ¨¡å‹æ¨ªç©ºå‡ºä¸–åï¼Œäººä»¬æœ€æ´¥æ´¥ä¹é“çš„æ˜¯å®ƒå­¦åˆ°çš„å‘é‡åœ¨è¯­ä¹‰å’Œè¯­æ³•ç›¸ä¼¼æ€§ä¸Šçš„åº”ç”¨â€”â€”å°¤å…¶æ˜¯è¿™ç§ç›¸ä¼¼æ€§å±…ç„¶å¯¹æ•°å­¦ä¸Šçš„åŠ å‡æ“ä½œæœ‰æ„ä¹‰[8]ï¼æœ€ç»å…¸çš„ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œ$v("King")âˆ’v("Man")+v("Woman")=v("Queen")$ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾‹å­ä¼¼ä¹å¹¶æ²¡æœ‰å¤ªå¤šå®é™…çš„ç”¨é€”ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œword2vecæ¨¡å‹è¿˜è¢«åº”ç”¨äº**æœºå™¨ç¿»è¯‘**å’Œ**æ¨èç³»ç»Ÿ**é¢†åŸŸã€‚

### Machine Translation[9]

ä¸åæ¥æå‡ºçš„åœ¨sentence levelä¸Šè¿›è¡Œæœºå™¨ç¿»è¯‘çš„RNNæ¨¡å‹ä¸åŒï¼Œword2vecæ¨¡å‹ä¸»è¦æ˜¯ç”¨äºè¯ç²’åº¦ä¸Šçš„æœºå™¨ç¿»è¯‘ã€‚

å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä»å¤§é‡çš„å•è¯­ç§è¯­æ–™ä¸­å­¦ä¹ åˆ°æ¯ç§è¯­è¨€çš„word2vecè¡¨è¾¾ï¼Œå†å€ŸåŠ©ä¸€ä¸ªå°çš„åŒè¯­è¯­æ–™åº“å­¦ä¹ åˆ°ä¸¤ç§è¯­è¨€word2vecè¡¨è¾¾çš„çº¿æ€§æ˜ å°„å…³ç³»$W$ã€‚æ„é€ çš„æŸå¤±å‡½æ•°ä¸ºï¼š
$$
J(W)=\sum_{i=1}^n{||Wx_i - z_i||^2}
$$
åœ¨ç¿»è¯‘çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬**é¦–å…ˆå°†æºè¯­è¨€çš„word2vecå‘é‡é€šè¿‡çŸ©é˜µ $W$ æ˜ å°„åˆ°ç›®æ ‡è¯­è¨€çš„å‘é‡ç©ºé—´ä¸Šï¼›å†åœ¨ç›®æ ‡è¯­è¨€çš„å‘é‡ç©ºé—´ä¸­æ‰¾å‡ºä¸æŠ•å½±å‘é‡è·ç¦»æœ€è¿‘çš„wordåšä¸ºç¿»è¯‘çš„ç»“æœè¿”å›ã€‚**

å…¶åŸç†æ˜¯ï¼Œä¸åŒè¯­è¨€å­¦ä¹ åˆ°çš„word2vecå‘é‡ç©ºé—´åœ¨å‡ ä½•ä¸Šå…·æœ‰ä¸€å®šçš„åŒæ„æ€§ã€‚æ˜ å°„çŸ©é˜µ $W$ æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç©ºé—´å¯¹é½çš„çº¿æ€§å˜æ¢ã€‚

### Item2Vec[11]

æœ¬è´¨ä¸Šï¼Œword2vecæ¨¡å‹æ˜¯åœ¨word-contextçš„co-occurrenceçŸ©é˜µåŸºç¡€ä¸Šå»ºç«‹èµ·æ¥çš„ã€‚å› æ­¤ï¼Œä»»ä½•åŸºäºco-occurrenceçŸ©é˜µçš„ç®—æ³•æ¨¡å‹ï¼Œéƒ½å¯ä»¥å¥—ç”¨word2vecç®—æ³•çš„æ€è·¯åŠ ä»¥æ”¹è¿›ã€‚

æ¯”å¦‚ï¼Œæ¨èç³»ç»Ÿé¢†åŸŸçš„ååŒè¿‡æ»¤ç®—æ³•ã€‚

ååŒè¿‡æ»¤ç®—æ³•æ˜¯å»ºç«‹åœ¨ä¸€ä¸ªuser-itemçš„co-occurrenceçŸ©é˜µçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡è¡Œå‘é‡æˆ–åˆ—å‘é‡çš„ç›¸ä¼¼æ€§è¿›è¡Œæ¨èã€‚å¦‚æœæˆ‘ä»¬å°†åŒä¸€ä¸ªuserè´­ä¹°çš„itemè§†ä¸ºä¸€ä¸ªcontextï¼Œå°±å¯ä»¥å»ºç«‹ä¸€ä¸ªitem-contextçš„çŸ©é˜µã€‚è¿›ä¸€æ­¥çš„ï¼Œå¯ä»¥åœ¨è¿™ä¸ªçŸ©é˜µä¸Šå€Ÿé‰´CBoWæ¨¡å‹æˆ–Skip-gramæ¨¡å‹è®¡ç®—å‡ºitemçš„å‘é‡è¡¨è¾¾ï¼Œåœ¨æ›´é«˜é˜¶ä¸Šè®¡ç®—itemé—´çš„ç›¸ä¼¼åº¦ã€‚

å…³äºword2vecæ›´å¤šåº”ç”¨çš„ä»‹ç»ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‚è€ƒè¿™ç¯‡æ–‡çŒ®[10]ã€‚

## Word Embedding

æœ€åï¼Œæˆ‘æƒ³ç®€å•é˜è¿°ä¸‹æˆ‘å¯¹Word Embeddingçš„å‡ ç‚¹æ€è€ƒã€‚ä¸ä¸€å®šæ­£ç¡®ï¼Œä¹Ÿæ¬¢è¿å¤§å®¶æå‡ºä¸åŒçš„æ„è§ã€‚

Word embeddingæœ€æ—©å‡ºç°äºBengioåœ¨03å¹´å‘è¡¨çš„å¼€åˆ›æ€§æ–‡ç« ä¸­[3]ã€‚é€šè¿‡åµŒå…¥ä¸€ä¸ªçº¿æ€§çš„æŠ•å½±çŸ©é˜µï¼ˆprojection matrixï¼‰ï¼Œå°†åŸå§‹çš„one-hotå‘é‡æ˜ å°„ä¸ºä¸€ä¸ªç¨ å¯†çš„è¿ç»­å‘é‡ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„ä»»åŠ¡å»å­¦ä¹ è¿™ä¸ªå‘é‡çš„æƒé‡ã€‚è¿™ä¸€æ€æƒ³åæ¥è¢«å¹¿æ³›åº”ç”¨äºåŒ…æ‹¬word2vecåœ¨å†…çš„å„ç§NLPæ¨¡å‹ä¸­ã€‚

#### >>> Important

**Word Embeddingçš„è®­ç»ƒæ–¹æ³•å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç±»æ˜¯æ— ç›‘ç£æˆ–å¼±ç›‘ç£çš„é¢„è®­ç»ƒï¼›ä¸€ç±»æ˜¯ç«¯å¯¹ç«¯ï¼ˆend to endï¼‰çš„æœ‰ç›‘ç£è®­ç»ƒã€‚**

**æ— ç›‘ç£æˆ–å¼±ç›‘ç£çš„é¢„è®­ç»ƒ**ä»¥**word2vecå’Œauto-encoderä¸ºä»£è¡¨**ã€‚è¿™ä¸€ç±»æ¨¡å‹çš„ç‰¹ç‚¹æ˜¯ï¼Œä¸éœ€è¦å¤§é‡çš„äººå·¥æ ‡è®°æ ·æœ¬å°±å¯ä»¥å¾—åˆ°è´¨é‡è¿˜ä¸é”™çš„Embeddingå‘é‡ã€‚ä¸è¿‡å› ä¸ºç¼ºå°‘äº†ä»»åŠ¡å¯¼å‘ï¼Œå¯èƒ½å’Œæˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜è¿˜æœ‰ä¸€å®šçš„è·ç¦»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾€å¾€ä¼šåœ¨å¾—åˆ°é¢„è®­ç»ƒçš„Embeddingå‘é‡åï¼Œç”¨å°‘é‡äººå·¥æ ‡æ³¨çš„æ ·æœ¬å»fine-tuneæ•´ä¸ªæ¨¡å‹ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œç«¯å¯¹ç«¯çš„æœ‰ç›‘ç£æ¨¡å‹åœ¨æœ€è¿‘å‡ å¹´é‡Œè¶Šæ¥è¶Šå—åˆ°äººä»¬çš„å…³æ³¨ã€‚ä¸æ— ç›‘ç£æ¨¡å‹ç›¸æ¯”ï¼Œç«¯å¯¹ç«¯çš„æ¨¡å‹åœ¨ç»“æ„ä¸Šå¾€å¾€æ›´åŠ å¤æ‚ã€‚åŒæ—¶ï¼Œä¹Ÿå› ä¸ºæœ‰ç€æ˜ç¡®çš„ä»»åŠ¡å¯¼å‘ï¼Œç«¯å¯¹ç«¯æ¨¡å‹å­¦ä¹ åˆ°çš„Embeddingå‘é‡ä¹Ÿå¾€å¾€æ›´åŠ å‡†ç¡®ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡ä¸€ä¸ªEmbeddingå±‚å’Œè‹¥å¹²ä¸ªå·ç§¯å±‚è¿æ¥è€Œæˆçš„æ·±åº¦ç¥ç»ç½‘ç»œä»¥å®ç°å¯¹å¥å­çš„æƒ…æ„Ÿåˆ†ç±»ï¼Œå¯ä»¥å­¦ä¹ åˆ°è¯­ä¹‰æ›´ä¸°å¯Œçš„è¯å‘é‡è¡¨è¾¾ã€‚

#### >>> Important

Word Embeddingçš„å¦ä¸€ä¸ªç ”ç©¶æ–¹å‘æ˜¯åœ¨æ›´é«˜å±‚æ¬¡ä¸Šå¯¹Sentenceçš„Embeddingå‘é‡è¿›è¡Œå»ºæ¨¡ã€‚

æˆ‘ä»¬çŸ¥é“ï¼Œwordæ˜¯sentenceçš„åŸºæœ¬ç»„æˆå•ä½ã€‚ä¸€ä¸ªæœ€ç®€å•ä¹Ÿæ˜¯æœ€ç›´æ¥å¾—åˆ°sentence embeddingçš„æ–¹æ³•æ˜¯å°†ç»„æˆsentenceçš„æ‰€æœ‰wordçš„embeddingå‘é‡å…¨éƒ¨åŠ èµ·æ¥â€”â€”ç±»ä¼¼äºCBoWæ¨¡å‹ã€‚

æ˜¾ç„¶ï¼Œè¿™ç§ç®€å•ç²—æš´çš„æ–¹æ³•ä¼šä¸¢å¤±å¾ˆå¤šä¿¡æ¯ã€‚

å¦ä¸€ç§æ–¹æ³•å€Ÿé‰´äº†word2vecçš„æ€æƒ³â€”â€”å°†sentenceæˆ–æ˜¯paragraphè§†ä¸ºä¸€ä¸ªç‰¹æ®Šçš„wordï¼Œç„¶åç”¨CBoWæ¨¡å‹æˆ–æ˜¯Skip-gramè¿›è¡Œè®­ç»ƒ[12]ã€‚è¿™ç§æ–¹æ³•çš„é—®é¢˜åœ¨äºï¼Œå¯¹äºä¸€ç¯‡æ–°æ–‡ç« ï¼Œæ€»æ˜¯éœ€è¦é‡æ–°è®­ç»ƒä¸€ä¸ªæ–°çš„sentence2vecã€‚æ­¤å¤–ï¼ŒåŒword2vecä¸€æ ·ï¼Œè¿™ä¸ªæ¨¡å‹ç¼ºå°‘æœ‰ç›‘ç£çš„è®­ç»ƒå¯¼å‘ã€‚

ä¸ªäººæ„Ÿè§‰æ¯”è¾ƒé è°±çš„æ˜¯ç¬¬ä¸‰ç§æ–¹æ³•â€”â€”åŸºäºword embeddingçš„ç«¯å¯¹ç«¯çš„è®­ç»ƒã€‚Sentenceæœ¬è´¨ä¸Šæ˜¯wordçš„åºåˆ—ã€‚å› æ­¤ï¼Œåœ¨word embeddingçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è¿æ¥å¤šä¸ªRNNæ¨¡å‹æˆ–æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼Œå¯¹word embeddingåºåˆ—è¿›è¡Œç¼–ç ï¼Œä»è€Œå¾—åˆ°sentence embeddingã€‚

è¿™æ–¹é¢çš„å·¥ä½œå·²æœ‰å¾ˆå¤šã€‚æœ‰æœºä¼šï¼Œæˆ‘ä¼šå†å†™ä¸€ç¯‡å…³äºsentence embeddingçš„ç»¼è¿°ã€‚

## References

[1]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013, January 17). Efficient Estimation of Word Representations in Vector Space. arXiv.org.

[2]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013, October 17). Distributed Representations of Words and Phrases and their Compositionality. arXiv.org.

[3]: Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137â€“1155.

[4]: Turney, P. D., & Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1).

[5]: Morin, F., & Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats.

[6]: Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation, 2265â€“2273.

[7]: Mikolov, T., KarafiÃ¡t, M., Burget, L., & CernockÃ½, J. (2010). Recurrent neural network based language model. Interspeech.

[8]: Mikolov, T., Yih, W., & Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Hlt-Naacl.

[9]: Mikolov, T., Le, Q. V., & Sutskever, I. (2013, September 17). Exploiting Similarities among Languages for Machine Translation. arXiv.org.

[10]: Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493â€“2537.

[11]: Barkan, O., & Koenigstein, N. (2016, March 14). Item2Vec: Neural Item Embedding for Collaborative Filtering. arXiv.org.

[12]: Le, Q. V., & Mikolov, T. (2014, May 16). Distributed Representations of Sentences and Documents. arXiv.org.