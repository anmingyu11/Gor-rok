> https://zhuanlan.zhihu.com/p/137847395

# 深入理解Boosting算法-LightGBM

> 导语： 本文主要是关于 LightGBM 的相关原理介绍，对于这样优秀的算法，每次看完 LightGBM 相关论文之后，总是会引发额外的思考，如梯度信息的直接使用，这些东西在元学习(`Learning to Learn with Gradients`)中经常见到，在不平衡样本学习中损失函数的改造也可以看到它的影子(`Gradient Harmonized Single-stage Detector`)，领域迁移学习中(`Domain-Adversarial Training of Neural Networks`)的梯度翻转等，而关于互斥特征(`Exclusive Feature Bundling, EFB`)合并的内容，结合实际的应用场景，如风控系统中挖掘到的很多稀疏规则进行适当的改进似乎可以很容易生成可用的特征等，解决高维稀疏特征的使用问题。

本文的提纲如下：

- LightGBM新特性总结

- GOSS详细分析

- - 保证抽样之后分布不变-小梯度加权补偿
  - 更改LightGBM中信息增益的计算方式

- EFB特征绑定分析

- - 寻找可以进行合并的特征
  - 特征合并

- 直方图优化，直方图加速

- 类别型特征的处理

- 缺失值处理

本文内容主要来源于以下两篇论文, 以及论文作者之一`Taifeng Wang`的公开分享：

- Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. `"LightGBM: A Highly Efficient Gradient Boosting Decision Tree"`. Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.
- Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. `"A Communication-Efficient Parallel Algorithm for Decision Tree"`. Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.
- **开源 | LightGBM：三天内收获GitHub 1000 星[1]**
- **LightGBM文档[2]**

## **LightGBM 新特性总结**

回顾上一篇关于 XGBoost 原理的内容，我们讲到 GBDT 模型优化的方向常见的有两部分内容，在回归树构建时进行优化，用特征直方图代替特征预排序，减少排序带来的性能问题，以及降低特征候选点的个数，可以认为是从特征维度来优化；而从样本角度，即是否对样本抽样等，一般都是在设计数据并行，或者特征并行等，或者更直接的样本权重进行采样等,见 Friedman, J. H. Greedy Function Approximation: A Gradient Boosting Machine. February 1999 论文。从算法层面上，LightGBM引入的新概念，新特性如下：

 

1. 对于XGBoost而言，在构建回归树中最优特征分裂，分裂点查找时需要全量的样本参与增益的计算，而 LightGBM 针对这一点，提出了利用单边梯度采用算法(`Gradient-Based One-Side Sampling, GOSS`)来进行样本采用，不使用全量样本进行信息增益的计算,达到**减少样本个数目的**；
2. 针对特征维度，XGBoost 似乎没有额外的操作，LightGBM 在这方面通过利用互斥特征绑定(`Exclusive Feature Bundling, EFB`)在减少特征的个数。**减少了特征的个数**，就意味着在回归树构建时不需要遍历更多的特征;
3. 在构建树的过程中，LightGBM 利用的 Leaf-wise 的形式(best-first), 也就是在构建回归树的过程中是选择增益最明显的节点往下分裂，而不是按部就班地逐层 Level-wise 方式，相同层的节点都需要同时进行分裂，带来不必要的分裂；
4. 针对类别型特征的优化问题，支持直接使用类别型特征，避免对类别特征进行编码(onehot等)
5. 直方图加速
6. 其他工程优化

本文将对上述的内容做详细的分析，其他更多的特性，可以通过这个页面获取更全的信息,**LightGBM文档[3]**

## **GOSS详细分析**

对于GBDT，下一颗树拟合的是负梯度，梯度的值越大，代表该样本还没有被学习充分，还需要增加树进行迭代拟合，反之对于梯度小的样本，已经无法提供更多的信息，对于损失函数的贡献较低。关于这一点，我们在之前的文章中，分析过梯度下降优化算法，和拟牛顿优化算法，可以进行类比(树模型不能直接利用 SGD 进行参数优化，这里只为方便类比而已),对于常见的可以使用梯度下降进行参数优化的过程，参数更新的过程为：
$$
w_{t+1}=w_{t}-\left.\eta \frac{\partial L(w)}{\partial w}\right|_{w=w_{t}}
$$
对于拟牛顿法：
$$
w_{t+1}=w_{t}-\frac{L^{\prime}\left(w_{t}\right)}{L^{\prime \prime}\left(w_{t}\right)}
$$
从上述公式可知，当梯度很小时，参数是不更新的，或者更新的幅度很小，达到收敛状态。在 GBDT 模型中，各个样本关于损失函数的一阶梯度，二阶梯度是用于进行特征的分裂，以及分裂点查找的，回顾 XGBoost 在迭代第 $t$  颗树时的损失函数：
$$
L^{(t)}=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T
$$
也可以发现对于小梯度的样本，对于损失函数的贡献是很小的（`如果训练集中小梯度占据着主导作用，是不是会掩盖困难样本的`）。

有了上面的分析，我们可以用梯度信息用来衡量样本是否被充分学习的依据，对样本的梯度进行排序，大梯度的样本保留，小梯度的样本随机抽样，减少的训练样本，加速模型训练，但是在抽样之后会产生新的问题，需要额外的应对策略。

### **保证抽样之后分布不变-小梯度加权补偿**

直接根据样本的梯度进行抽样，会改变训练集中的标签的分布，这点让我联想到了神经网络中的`BatchNorm`的作用，如果训练过程中不同层之间的数据分布经常发生变化，对于网络来讲容易造成混乱，如果每层输入的数据都是类似的分布化，那么可以更好的进行参数优化，加快收敛过程。 在 LightGBM 中，为了不改变分布需要做额外的加权补偿。具体分析如下：

1. 在训练过程中，根据损失函数计算得到样本的一阶梯度信息，对其进行降序排列；

2. 选择 Top a 的大梯度样本，同时从中随机选择 b 小梯度样本；假设训练集为 $I$；

3. 为了保持分布不变，可以这样理解，虽然只选择了 其中 $a * len(I) + b * len(I)$  的样本进行梯度统计， 为了达到 $len(I)$ 个样都参与了梯度统计的效果，需要对小梯度进行加权补偿：
   $$
   \begin{aligned}
   \operatorname{len}(I) &=a * \operatorname{len}(I)+b * \operatorname{len}(I) * w \\
   w &=\frac{1-a}{b}
   \end{aligned}
   $$

### **更改 LightGBM 中信息增益的计算方式**

又到了信息增益的计算步骤，关于 XGBoost 的信息增益计算公式具体推导过程可以参考, 这里直接搬运过来，
$$
\text { Gain }=\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]-\gamma
$$
其中，左右子树的一阶梯度之和，二阶梯度之和为：
$$
\begin{aligned}
G_{L} &=\sum_{i \in I_{L}} g_{i}, \quad H_{L}=\sum_{i \in I_{L}} h_{i} \\
G_{R} &=\sum_{i \in I_{R}} g_{i}, \quad H_{R}=\sum_{i \in I_{R}} h_{i}
\end{aligned}
$$
由于 GBDT 中使用的是回归树，也就是在回归树拟合过程中，利用的平方损失函数 $L=\left(y_{i}-\hat{y}_{i}^{(t-1)}\right)^{2} / 2$ ，回顾之前的文章，我们知道平方损失的一阶梯度，二阶梯度（为常数 !）的结果如下：
$$
\begin{aligned}
g_{i} &=-\left(y_{i}-\hat{y}_{i}^{(t-1)}\right) \\
h_{i} &=1
\end{aligned}
$$
直接修改上述增益的计算公式，令 $\lambda =0 , \gamma = 0$,  由于在**同一个分裂节点**时，增益公式中的
$$
\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}
$$
为常数(该节点对应的所有样本)，不同的特征，以及不同的切分点之和仅影响前两项的取值，因此只需要使得前两项之后最大即可：
$$
\begin{aligned}
g a i n &=\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}}+\frac{G_{R}^{2}}{H_{R}}\right] \\
&=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\operatorname{len}\left(I_{L}\right)}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\operatorname{len}\left(I_{L}\right)}\right]
\end{aligned}
$$
对比原始论文中的`variance gain`的公式，与上式基本一致，在前面的系数上有所不同而已，对上述增益公式进行拆分，分成 TOP 部分的，以及采样本部分，利用上述计算得到的加权系数对小梯度进行加权补偿，即可得到新的增益公式，这里不在赘述增益的公式。

## **EFB特征绑定分析**

EFB要解决的问题是，在高维稀疏特征空间中，找到一个几乎无损(nearly lossless)的方法，将多个特征进行合并，减少特征的个数，这样在构建特征的直方图时，可以从时间复杂度为 $O(\# \text { data } * \# \text { feature })$  降低到 $O(\# \text { data } * \text { #bundle })$ , 而 $\text { #feature }$ 远大于 $\text { #bundle }$ 。一个 bundle 里面包含的特征都是互斥的，那么什么是互斥特征呢？

互斥特征，即在高维稀疏的特征空间中，可能存在多个特征的取值 **不会同时为非零值** ，如果取值同时为非零了，那么就说特征之间发生了冲突，而在特征绑定的时候也允许这种冲突，根据数据集冲突比是可以计算的，优先选择冲突比低的特征即可，因此利用这种特性，可以将多个特征合并在一起，合并成一个特征。

在实际的场景中，哪些特征容易互斥呢，假设这样一个场景，在风控场景中，不同客群能够获取的信息种类是不一样的，比如对于高净值的客户，他们可提供的信息可能包含征信信息，公积金，房产信息等金融数据，而对于小镇青年，有可能他们无社保信息，无银行流水，但是他们活跃在各大短视频应用中，留下了丰富的行为数据，如果我们构建了同时包含金融属性特征，与行为特征的训练集，那么可以认为这些特征很可能发生互斥，将这一的特征进行合并，可以减少特征的稀疏程度，特征似乎也更具有信息量（虽然特征的含义发生了变化）；

另外一个例子，也是风控场景中，会设计不同时间切片的特征，如最近 1 天，最近 3 天，最近 7 天被其他家机构查询的次数，一般来说时间越近，特征越稀疏，如果多个不同类型的最近 N 天的信息进行合并，也可以大大提高特征的信息量。

### **寻找可以进行合并的特征**

在论文中，作者将这个问题等价于图着色问题，图着色问题是最著名的 NP-完全问题之一，给定一个无向图 $G=\langle V, E\rangle$, 其中 $V$ 为顶点集合，$E$ 为边集合，图着色问题即将顶点 $V$ 分为 $K$ 个颜色组，即相邻顶点的颜色不同。网上最常见的关于图着色的解法是韦尔奇.鲍威尔法(Welch Powell)，其步骤如下：

1. 将图 G 中的结点按度数递减的次序进行排列(相同度数的结点的排列随意)。
2. 用第一种颜色，对第一点着色，并按排列次序对与前面结点不相邻的每一点着同样的颜色。
3. 用第二种颜色对尚未着色的点重复第2 步, 直到所有的点都着上颜色为止。

下图，举了一个例子：

![](/Users/helloword/Anmingyu/Gor-rok/Algs/GBDT/深入理解Boosting算法-LightGBM/Fig1.png)

相信通过这个例子，大家对这图着色问题有一定的感觉了，回到 LightGBM 中的互斥特征的查找问题，顶点就是数据集中的特征，两个特征发生连接表示特征之间**不互斥**，因此当用了 $K$ 种不同的颜色将所有的顶点染色之后，即把顶点被分成了 $K$ 组互斥的特征，$K$  个bundle，每个 buddle 中的节点颜色是相同的，在图中就是不连接的，即特征之间是互斥的。原始论文中的流程图如下图，以我们上述举例类似：

![](/Users/helloword/Anmingyu/Gor-rok/Algs/GBDT/深入理解Boosting算法-LightGBM/Fig2.png)

注意到上图中的 Algorithm3 的算法复杂度为 $O\left(\# \text { feature }^{2}\right)$ , 当特征量巨大时，相对来说是一个很耗时的过程，如果我们不构建无向图，利用近似算法来操作，可以减小时间复杂度，论文中给了如下的一种的近似算法：统计各个特征的非零个数值, 如果非零个数值少，那么发生碰撞的概率也就小，后面的步骤保持不变。 通过上述方法，找到了可以进行合并的特征，那么下面就是要回到怎么合并的问题

### **特征合并**

由于特征是利用直方图来进行排序，寻找最优分割点，特征被离散化了，多个特征进行合并后，那么算法是无法区分出这个特征是原始的特征，还是经过上述合并的特征，为了使得算法依旧能够对各个特征的取值具有区分度，论文中使用了特征偏移的操作，假定一个 buddle 中合并了两个特征，特征 A 的取值范围为 $[0,10)$  特征 B 的取值为 $[0,20)$ , 那么为特征 B 设置一个偏移项，使得其取值范围变为 $[10,30)$ , 这种在特征直方图中，如果切的 bin 足够多的话，不同特征是在不同的 bin 中的。

## **直方图优化，直方图加速**

在 LightGBM 中，利用特征的直方图进行最优分裂点查找，整个流程如下图所示，由于连续特征和类别特征有所差异，这里以连续特征为例来进行说明，在本文下面的类别型特征处理部分有关类别特征的处理方式。

![](/Users/helloword/Anmingyu/Gor-rok/Algs/GBDT/深入理解Boosting算法-LightGBM/Fig3.png)

第一步是先要将连续特征进行离散化，分成多个桶 bin , bin 的最大取值为 `max_bin` ，默认为 255，这是个超参数，可以进行调整,越小训练速度越快，可以减小过拟合的风险，但可能影响模型的精度。通过这一操作，完成了从特征值到 bin 的映射，在训练过程中这种映射关系保持不变； 完成了上述第一步过程，就需要为各个特征构造直方图，直方图中统计的信息有一阶梯度之和，二阶梯度之和，样本个数，**如果你发现上图中似乎没有统计二阶梯度，那是因为在回归树中使用的平方损失函数，其二阶梯度为 1 ，等价于只需要统计样本个数，但是源码是需要统计一阶，二阶梯度之后的。** 

分析流程图中的 4 个 for 循环，最外层的 for 循环用于当前树结构中的所有叶子节点，对于各个叶子节点中的所有特征 `X.Features` (第二个 for 循环) , 分别为各个特征构造直方图，统计各个特征离散化为 bin 之后各个 bin 中的一阶梯度之和，二阶梯度之和(数值上与样本个数等价)，完成第三个 for 循环，即把训练数据从 $(\# \text { data } * \# \text { feature })$ 变成了 $(\# d a t a * \# b i n)$ ,  因此各个叶子节点上最优特征，分裂点的查找过程只需要在特征直方图上进行即可，极大的减少了搜索空间，特征的增益计算，如上文所述。

**直方图加速**：在最后一个 for 循环中，有一个统计将样本归为左子树的一阶梯度之和 $S_L$ , 样本个数 $n_L$ 统计的过程，而右子树的相关统计量是通过利用父节点总的信息与左子树相减的得到，即一个叶子节点的直方图可以由父节点的直方图与它兄弟的直方图做差得到， 而在实际的实现过程中，LightGBM 利用父节点的直方图减去**数据量比较小**的叶子节点直方图，得到数据量比较大的叶子节点直方图，因为该直方图是做差得到的，时间复杂度仅为 $O(\# \text { bins })$ ，比起不做差得到的兄弟节点的直方图，速度上可以提升一倍。

其实在离散化过程中，还有很多的细节，如

- 当连续特征的 unique 取值个数比 `max_bin` 还小，该怎么处理
- 当连续特征的 unique 取值个数比`max_bin` 还大，多处理的取值该怎么处理，超参数 `min_data_in_bin` 也限定了一个 bin 中最小样本个数；
- 具体的 bin 细节

等等，如果对这方面感兴趣，可以继续深入源码，LightGBM 的源码写得很清晰，掌握基本的C++，STL就可以看懂。

## **类别型特征的处理**

对于类别型的特征，传统的机器学习模型是需要先利用 one-hot 编码，而在 LightGBM 中只需要提前将类别映射到非负整数即可(`integer-encoded categorical features`)，例如，进行如下编码 mapping`{'特朗普': 1, '傻蛋': 2, '其他': 0}`，在官方文档中也建议使用从 0 开始的连续的数值进行编码，当训练集中的某个类别型的特征取值个数超大，可以将其看做是连续特征看待，或者进行 embedding 编码。

关于 LightGBM的 类别型编码，经常和 one-hot 编码相比，one-hot 编码是一种`one-vs-other` 来划分数据，如假设某个类别型特征 $x_n$  的取值只有 $[A, B ,C, D]$, 那么在节点分裂时一共有 4 种分法，每次分裂的时候，依次进行遍历，当取值为 A时，计算增益，取值为 B 时，计算增益...在 LightGBM 中，利用参数 `max_cat_to_onehot` 来设定是否开启 one-hot 编码，当训练集中的某个类别特征取值个数小于等于 4 时，默认利用one-hot对类别特征编码。与之相对的编码方式就是 `many-vs-many`, 还是以上述特征为例，分裂时的判断条件可能为：![[公式]](https://www.zhihu.com/equation?tex=x_n) 取值为 A 或者 B ，取值为 A 或者 C，差异如下：

- one-vs-other： A|[B, C, D], B|[A, C, D], C|[A, B, D], D|[A, B, C]
- many-vs-many: [A, C] | [B, D], [A, B, C] | [D] , ...；

通过比较发现，one-hot 编码需要更深的不平衡(一个取值上样本太少)的树才能得到很好的结果，而 many-vs-many 在一次分裂时可以结合多个取值，树的深度将减少。

以上是关于类别型特征的在操作层面的内容，那么在 LighGBM 中是如何实现 `many-vs-many` 的呢，以一个类别特征为例：

1. 统计该特征中的各取值上的样本数，按照从样本数从大到小排序，去除样本占比小于 1% 的类别值（所以大家遇到特别稀疏的特征时，最好先进行尾部取值合并，要不然会被算法自动抹去）；
2. 对于剩余的特征值（可以理解为一个特征值对应一个桶），统计各个特征值对应的样本的一阶梯度之和，二阶梯度之和，根据正则化系数，算得各个桶的统计量: 一阶梯度之和 / (二阶梯度之和 + 正则化系数）；
3. 根据该统计量对各个桶进行从大到小排序；
4. 在排序好的桶上，进行最佳切点查找，这一步的结果就会得到上述 `many-vs-many` 的结果，而且通过参数 `max_cat_threshold` , 默认一个组(包含多个特征取值，所以叫 `many` )中最多可以存在 32 个不同的特征取值。而且，LightGBM 中用了比较粗的粒度来解决查找，先从左到右，再从右到左，满足特征取值个数不超过 `max_cat_threshold` , 增益最大即可（与连续特征一样的计算公式）；

**缺失值处理**

缺失值的定义，如数据中存在 NaN , Na , None 等都可以被认为是缺失，在LightGBM 中默认是开启对缺失值进行处理的，不需要对缺失做额外操作，如果要将这个特征关闭可以通过参数完成 `use_missing=false` ,而关于 0 是否是缺失值，可以通过参数完成 `zero_as_missing=true` , 默认情况下 `zero_as_missing=false` , 0 跟正常的数一样，在很多稀疏表示如(libsvm,CSR) 未被显式表示的都默认为 0，因此对于这样的数据要仔细对待，决定是否其取值为 0，还是缺失 NAN。 在XGBoost中，在每个节点分裂时，额外增加了一个针对缺失特征的分裂方向的判断，即对于存在取值缺失的样本，应该被分裂到左子树，还是右子树的判断，而LightGBM 也差不多是这样的操作。 关于 LightGBM 对缺失值的讨论可以查看这两个 issues:

- **[Feature] Let data inform node assignment of missing values #122[4]**
- **Support learn the best direction for missing value #516[5]**

按图索骥，查看更多的 issues, 可以看看这个是怎么被处理的。

